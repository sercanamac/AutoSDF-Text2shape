{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21f9292f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28ddad9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: ./launchers/train_new_bert.sh: Permission denied\n"
     ]
    }
   ],
   "source": [
    "rc = subprocess.call(\"./launchers/train_new_bert.sh\", shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f9b0a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autosdf.yaml\t\t      filelists\t\t      README.md\r\n",
      "Compare.ipynb\t\t      info-shapenet.json      results\r\n",
      "configs\t\t\t      launchers\t\t      shape_set_paths.json\r\n",
      "datasets\t\t      logs\t\t      Test-Reproduce.ipynb\r\n",
      "demo_data\t\t      logs2\t\t      test_samples_paper.txt\r\n",
      "demo-lang-conditional.ipynb   models\t\t      text2ShapePP.json\r\n",
      "demo_shape_comp.ipynb\t      New-Bert-Sandbox.ipynb  train.py\r\n",
      "demo_single_view_recon.ipynb  NewBetTrain.ipynb       utils\r\n",
      "extract_code.py\t\t      options\r\n",
      "file.json\t\t      preprocess\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f6cb39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Options -------------\n",
      "alpha: 0.75\n",
      "batch_size: 32\n",
      "bert_cfg: configs/bert2vq_shapeglot.yaml\n",
      "cat: chair\n",
      "checkpoints_dir: ./checkpoints\n",
      "ckpt: None\n",
      "continue_train: False\n",
      "dataset_mode: text2shape-seq\n",
      "debug: 0\n",
      "device: cuda\n",
      "display_freq: 3000\n",
      "gpu_ids: [0]\n",
      "gpu_ids_str: 0\n",
      "input_nc: 3\n",
      "iou_thres: 0.0\n",
      "isTrain: True\n",
      "lambda_L1: 10.0\n",
      "logs_dir: ./logs\n",
      "lr: 0.0001\n",
      "lr_decay_iters: 50\n",
      "lr_policy: lambda\n",
      "max_dataset_size: 100000000000\n",
      "model: bert2vqsc\n",
      "nThreads: 9\n",
      "n_less: 0\n",
      "name: bert2vqsc-text2shape-seq-LR1e-4-new-bert-2-prev_z_shape_input-try-2\n",
      "ndf: 64\n",
      "nepochs: 20\n",
      "nepochs_decay: 5\n",
      "ngf: 64\n",
      "output_nc: 3\n",
      "pix3d_mode: noBG\n",
      "print_freq: 25\n",
      "profiler: 0\n",
      "ratio: 1.0\n",
      "resnet2vq_ckpt: None\n",
      "resnet_arch: resnet18\n",
      "resnet_cfg: configs/resnet2vq_pix3d.yaml\n",
      "resnet_ckpt: None\n",
      "resnet_dset: None\n",
      "resnet_model: None\n",
      "resnet_norm: gn\n",
      "save_epoch_freq: 3\n",
      "save_latest_freq: 5000\n",
      "seed: 111\n",
      "serial_batches: False\n",
      "snet_mode: noBG\n",
      "tf_cfg: configs/rand_tf_snet_code.yaml\n",
      "topk: 30\n",
      "trunc_thres: 0.2\n",
      "use_bin_sdf: 0\n",
      "use_marginal: 0\n",
      "vq_cat: chair\n",
      "vq_cfg: configs/pvqvae_snet.yaml\n",
      "vq_ckpt: ../raw_dataset/checkpoints/vqvae.pth\n",
      "vq_dset: snet\n",
      "vq_model: pvqvae\n",
      "vq_note: default\n",
      "which_epoch: latest\n",
      "-------------- End ----------------\n",
      "[*] Dataset has been created: Text2Shape\n",
      "[*] # training images = 140707\n",
      "[*] # testing images = 16000\n",
      "---------- Networks initialized -------------\n",
      "-----------------------------------------------\n",
      "[*] Model has been created: BERT2VQSC-Model\n",
      "[*] \"bert2vqsc\" initialized.\n",
      "[*] create image directory:\n",
      "/cluster/54/streakfull/ADL4CV/Project/src/plz-autosdf/logs/bert2vqsc-text2shape-seq-LR1e-4-new-bert-2-prev_z_shape_input-try-2/images...\n",
      "[*] saving model and dataset files: /cluster/54/streakfull/ADL4CV/Project/src/plz-autosdf/models/bert2vq_scmodel.py, /cluster/54/streakfull/ADL4CV/Project/src/plz-autosdf/datasets/text2shape.py\n",
      "140707 Length train dataset\n",
      "16000 Length test dataset\n",
      "4397 Length train_dl\n",
      "500 Length test_dl\n",
      "[*] Start training. name: bert2vqsc-text2shape-seq-LR1e-4-new-bert-2-prev_z_shape_input-try-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/4397 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 0, iters: 32, time: 0.028) nll: 15848.143555 \n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 3049/4397 [14:05<05:47,  3.88it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 0, iters: 32, time: 0.028) nll: 15490.901367 \n",
      "(GPU: 0, epoch: 0, iters: 800, time: 0.007) nll: 8148.311035 \n",
      "(GPU: 0, epoch: 0, iters: 1600, time: 0.008) nll: 7647.347656 \n",
      "(GPU: 0, epoch: 0, iters: 2400, time: 0.007) nll: 7844.635742 \n",
      "(GPU: 0, epoch: 0, iters: 3200, time: 0.008) nll: 6662.813477 \n",
      "(GPU: 0, epoch: 0, iters: 4000, time: 0.008) nll: 5155.349609 \n",
      "(GPU: 0, epoch: 0, iters: 4800, time: 0.008) nll: 5698.162598 \n",
      "(GPU: 0, epoch: 0, iters: 5600, time: 0.008) nll: 4423.576172 \n",
      "(GPU: 0, epoch: 0, iters: 6400, time: 0.008) nll: 3772.024658 \n",
      "(GPU: 0, epoch: 0, iters: 7200, time: 0.008) nll: 4052.045410 \n",
      "(GPU: 0, epoch: 0, iters: 8000, time: 0.008) nll: 5178.372559 \n",
      "(GPU: 0, epoch: 0, iters: 8800, time: 0.008) nll: 4521.852539 \n",
      "(GPU: 0, epoch: 0, iters: 9600, time: 0.008) nll: 5160.625000 \n",
      "(GPU: 0, epoch: 0, iters: 10400, time: 0.008) nll: 3321.322754 \n",
      "(GPU: 0, epoch: 0, iters: 11200, time: 0.008) nll: 4972.231445 \n",
      "(GPU: 0, epoch: 0, iters: 12000, time: 0.008) nll: 2599.836914 \n",
      "(GPU: 0, epoch: 0, iters: 12800, time: 0.008) nll: 4285.183594 \n",
      "(GPU: 0, epoch: 0, iters: 13600, time: 0.008) nll: 4518.578613 \n",
      "(GPU: 0, epoch: 0, iters: 14400, time: 0.008) nll: 3554.991211 \n",
      "(GPU: 0, epoch: 0, iters: 15200, time: 0.008) nll: 3945.466064 \n",
      "(GPU: 0, epoch: 0, iters: 16000, time: 0.008) nll: 2850.765137 \n",
      "(GPU: 0, epoch: 0, iters: 16800, time: 0.007) nll: 3728.300781 \n",
      "(GPU: 0, epoch: 0, iters: 17600, time: 0.008) nll: 4066.760254 \n",
      "(GPU: 0, epoch: 0, iters: 18400, time: 0.008) nll: 5536.725586 \n",
      "(GPU: 0, epoch: 0, iters: 19200, time: 0.008) nll: 3602.025879 \n",
      "(GPU: 0, epoch: 0, iters: 20000, time: 0.007) nll: 2413.235840 \n",
      "saving the latest model (epoch 0, total_steps 20000)\n",
      "(GPU: 0, epoch: 0, iters: 20800, time: 0.008) nll: 3836.522461 \n",
      "(GPU: 0, epoch: 0, iters: 21600, time: 0.008) nll: 3877.543701 \n",
      "(GPU: 0, epoch: 0, iters: 22400, time: 0.008) nll: 4978.807617 \n",
      "(GPU: 0, epoch: 0, iters: 23200, time: 0.007) nll: 3742.285156 \n",
      "(GPU: 0, epoch: 0, iters: 24000, time: 0.008) nll: 3824.958252 \n",
      "(GPU: 0, epoch: 0, iters: 24800, time: 0.008) nll: 3626.182129 \n",
      "(GPU: 0, epoch: 0, iters: 25600, time: 0.008) nll: 4954.915039 \n",
      "(GPU: 0, epoch: 0, iters: 26400, time: 0.008) nll: 3668.003174 \n",
      "(GPU: 0, epoch: 0, iters: 27200, time: 0.008) nll: 5035.751953 \n",
      "(GPU: 0, epoch: 0, iters: 28000, time: 0.008) nll: 5597.249023 \n",
      "(GPU: 0, epoch: 0, iters: 28800, time: 0.008) nll: 4046.093262 \n",
      "(GPU: 0, epoch: 0, iters: 29600, time: 0.008) nll: 5247.792969 \n",
      "(GPU: 0, epoch: 0, iters: 30400, time: 0.008) nll: 2994.489746 \n",
      "(GPU: 0, epoch: 0, iters: 31200, time: 0.008) nll: 3924.032715 \n",
      "(GPU: 0, epoch: 0, iters: 32000, time: 0.008) nll: 2285.245117 \n",
      "(GPU: 0, epoch: 0, iters: 32800, time: 0.008) nll: 5254.487305 \n",
      "(GPU: 0, epoch: 0, iters: 33600, time: 0.008) nll: 3529.734863 \n",
      "(GPU: 0, epoch: 0, iters: 34400, time: 0.008) nll: 4352.279785 \n",
      "(GPU: 0, epoch: 0, iters: 35200, time: 0.008) nll: 4759.771484 \n",
      "(GPU: 0, epoch: 0, iters: 36000, time: 0.008) nll: 4095.372070 \n",
      "(GPU: 0, epoch: 0, iters: 36800, time: 0.008) nll: 3676.468994 \n",
      "(GPU: 0, epoch: 0, iters: 37600, time: 0.008) nll: 3945.248047 \n",
      "(GPU: 0, epoch: 0, iters: 38400, time: 0.008) nll: 4034.497070 \n",
      "(GPU: 0, epoch: 0, iters: 39200, time: 0.008) nll: 4811.155762 \n",
      "(GPU: 0, epoch: 0, iters: 40000, time: 0.008) nll: 4511.214355 \n",
      "saving the latest model (epoch 0, total_steps 40000)\n",
      "(GPU: 0, epoch: 0, iters: 40800, time: 0.008) nll: 4412.095215 \n",
      "(GPU: 0, epoch: 0, iters: 41600, time: 0.008) nll: 4343.130371 \n",
      "(GPU: 0, epoch: 0, iters: 42400, time: 0.008) nll: 5090.686523 \n",
      "(GPU: 0, epoch: 0, iters: 43200, time: 0.008) nll: 4280.363770 \n",
      "(GPU: 0, epoch: 0, iters: 44000, time: 0.008) nll: 5185.073242 \n",
      "(GPU: 0, epoch: 0, iters: 44800, time: 0.008) nll: 3645.526367 \n",
      "(GPU: 0, epoch: 0, iters: 45600, time: 0.008) nll: 3998.693848 \n",
      "(GPU: 0, epoch: 0, iters: 46400, time: 0.008) nll: 4991.626953 \n",
      "(GPU: 0, epoch: 0, iters: 47200, time: 0.008) nll: 4057.759766 \n",
      "(GPU: 0, epoch: 0, iters: 48000, time: 0.008) nll: 4188.536133 \n",
      "(GPU: 0, epoch: 0, iters: 48800, time: 0.008) nll: 4027.378418 \n",
      "(GPU: 0, epoch: 0, iters: 49600, time: 0.008) nll: 3572.313721 \n",
      "(GPU: 0, epoch: 0, iters: 50400, time: 0.008) nll: 4241.441406 \n",
      "(GPU: 0, epoch: 0, iters: 51200, time: 0.008) nll: 2776.266602 \n",
      "(GPU: 0, epoch: 0, iters: 52000, time: 0.008) nll: 3266.341064 \n",
      "(GPU: 0, epoch: 0, iters: 52800, time: 0.008) nll: 4296.143066 \n",
      "(GPU: 0, epoch: 0, iters: 53600, time: 0.008) nll: 2903.846191 \n",
      "(GPU: 0, epoch: 0, iters: 54400, time: 0.008) nll: 2649.047363 \n",
      "(GPU: 0, epoch: 0, iters: 55200, time: 0.008) nll: 4137.546875 \n",
      "(GPU: 0, epoch: 0, iters: 56000, time: 0.008) nll: 4514.342773 \n",
      "(GPU: 0, epoch: 0, iters: 56800, time: 0.008) nll: 4901.062012 \n",
      "(GPU: 0, epoch: 0, iters: 57600, time: 0.008) nll: 3004.388184 \n",
      "(GPU: 0, epoch: 0, iters: 58400, time: 0.008) nll: 3221.466309 \n",
      "(GPU: 0, epoch: 0, iters: 59200, time: 0.008) nll: 5256.780273 \n",
      "(GPU: 0, epoch: 0, iters: 60000, time: 0.007) nll: 3994.701904 \n",
      "saving the latest model (epoch 0, total_steps 60000)\n",
      "(GPU: 0, epoch: 0, iters: 60800, time: 0.008) nll: 3521.071777 \n",
      "(GPU: 0, epoch: 0, iters: 61600, time: 0.008) nll: 4010.051025 \n",
      "(GPU: 0, epoch: 0, iters: 62400, time: 0.008) nll: 4140.401855 \n",
      "(GPU: 0, epoch: 0, iters: 63200, time: 0.008) nll: 4766.511719 \n",
      "(GPU: 0, epoch: 0, iters: 64000, time: 0.008) nll: 3393.871826 \n",
      "(GPU: 0, epoch: 0, iters: 64800, time: 0.008) nll: 3005.948730 \n",
      "(GPU: 0, epoch: 0, iters: 65600, time: 0.008) nll: 4112.183594 \n",
      "(GPU: 0, epoch: 0, iters: 66400, time: 0.008) nll: 4896.051758 \n",
      "(GPU: 0, epoch: 0, iters: 67200, time: 0.008) nll: 4342.568848 \n",
      "(GPU: 0, epoch: 0, iters: 68000, time: 0.007) nll: 3277.050781 \n",
      "(GPU: 0, epoch: 0, iters: 68800, time: 0.008) nll: 2993.988525 \n",
      "(GPU: 0, epoch: 0, iters: 69600, time: 0.008) nll: 4172.791504 \n",
      "(GPU: 0, epoch: 0, iters: 70400, time: 0.008) nll: 3753.437500 \n",
      "(GPU: 0, epoch: 0, iters: 71200, time: 0.008) nll: 4286.571289 \n",
      "(GPU: 0, epoch: 0, iters: 72000, time: 0.008) nll: 3109.938232 \n",
      "(GPU: 0, epoch: 0, iters: 72800, time: 0.008) nll: 3201.094238 \n",
      "(GPU: 0, epoch: 0, iters: 73600, time: 0.008) nll: 3364.369629 \n",
      "(GPU: 0, epoch: 0, iters: 74400, time: 0.008) nll: 3266.680176 \n",
      "(GPU: 0, epoch: 0, iters: 75200, time: 0.008) nll: 2871.347900 \n",
      "(GPU: 0, epoch: 0, iters: 76000, time: 0.008) nll: 4521.599121 \n",
      "(GPU: 0, epoch: 0, iters: 76800, time: 0.008) nll: 4856.285156 \n",
      "(GPU: 0, epoch: 0, iters: 77600, time: 0.008) nll: 3108.491699 \n",
      "(GPU: 0, epoch: 0, iters: 78400, time: 0.008) nll: 4433.584961 \n",
      "(GPU: 0, epoch: 0, iters: 79200, time: 0.007) nll: 3687.224121 \n",
      "(GPU: 0, epoch: 0, iters: 80000, time: 0.008) nll: 4841.655273 \n",
      "saving the latest model (epoch 0, total_steps 80000)\n",
      "(GPU: 0, epoch: 0, iters: 80800, time: 0.008) nll: 3161.692139 \n",
      "(GPU: 0, epoch: 0, iters: 81600, time: 0.008) nll: 5166.857910 \n",
      "(GPU: 0, epoch: 0, iters: 82400, time: 0.008) nll: 3420.404785 \n",
      "(GPU: 0, epoch: 0, iters: 83200, time: 0.008) nll: 3836.227783 \n",
      "(GPU: 0, epoch: 0, iters: 84000, time: 0.008) nll: 3470.128906 \n",
      "(GPU: 0, epoch: 0, iters: 84800, time: 0.008) nll: 5017.956543 \n",
      "(GPU: 0, epoch: 0, iters: 85600, time: 0.008) nll: 5868.110352 \n",
      "(GPU: 0, epoch: 0, iters: 86400, time: 0.008) nll: 3305.880615 \n",
      "(GPU: 0, epoch: 0, iters: 87200, time: 0.008) nll: 4207.489746 \n",
      "(GPU: 0, epoch: 0, iters: 88000, time: 0.008) nll: 5052.393066 \n",
      "(GPU: 0, epoch: 0, iters: 88800, time: 0.008) nll: 6836.283203 \n",
      "(GPU: 0, epoch: 0, iters: 89600, time: 0.008) nll: 5009.959961 \n",
      "(GPU: 0, epoch: 0, iters: 90400, time: 0.008) nll: 4722.685059 \n",
      "(GPU: 0, epoch: 0, iters: 91200, time: 0.008) nll: 5222.325195 \n",
      "(GPU: 0, epoch: 0, iters: 92000, time: 0.008) nll: 4754.869141 \n",
      "(GPU: 0, epoch: 0, iters: 92800, time: 0.008) nll: 2925.618408 \n",
      "(GPU: 0, epoch: 0, iters: 93600, time: 0.008) nll: 4173.899414 \n",
      "(GPU: 0, epoch: 0, iters: 94400, time: 0.008) nll: 3319.585938 \n",
      "(GPU: 0, epoch: 0, iters: 95200, time: 0.008) nll: 4207.376953 \n",
      "(GPU: 0, epoch: 0, iters: 96000, time: 0.008) nll: 3447.263672 \n",
      "(GPU: 0, epoch: 0, iters: 96000, time: 0.013) nll: 3443.125977 \n",
      "(GPU: 0, epoch: 0, iters: 96000, time: 0.013) nll: 4390.620117 \n",
      "(GPU: 0, epoch: 0, iters: 96800, time: 0.008) nll: 4113.336426 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4397/4397 [20:35<00:00,  3.56it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 0, iters: 97600, time: 0.008) nll: 4220.783203 \n",
      "(GPU: 0, epoch: 0, iters: 98400, time: 0.008) nll: 3465.052002 \n",
      "(GPU: 0, epoch: 0, iters: 99200, time: 0.008) nll: 5067.193848 \n",
      "(GPU: 0, epoch: 0, iters: 100000, time: 0.008) nll: 2730.850098 \n",
      "saving the latest model (epoch 0, total_steps 100000)\n",
      "(GPU: 0, epoch: 0, iters: 100800, time: 0.008) nll: 2983.576416 \n",
      "(GPU: 0, epoch: 0, iters: 101600, time: 0.008) nll: 2558.388672 \n",
      "(GPU: 0, epoch: 0, iters: 102400, time: 0.008) nll: 4435.000977 \n",
      "(GPU: 0, epoch: 0, iters: 103200, time: 0.008) nll: 4219.874023 \n",
      "(GPU: 0, epoch: 0, iters: 104000, time: 0.008) nll: 2973.973633 \n",
      "(GPU: 0, epoch: 0, iters: 104800, time: 0.008) nll: 3381.169189 \n",
      "(GPU: 0, epoch: 0, iters: 105600, time: 0.008) nll: 3710.260010 \n",
      "(GPU: 0, epoch: 0, iters: 106400, time: 0.007) nll: 4542.693359 \n",
      "(GPU: 0, epoch: 0, iters: 107200, time: 0.008) nll: 3706.917480 \n",
      "(GPU: 0, epoch: 0, iters: 108000, time: 0.008) nll: 4325.765625 \n",
      "(GPU: 0, epoch: 0, iters: 108800, time: 0.008) nll: 4360.181152 \n",
      "(GPU: 0, epoch: 0, iters: 109600, time: 0.008) nll: 3634.378174 \n",
      "(GPU: 0, epoch: 0, iters: 110400, time: 0.008) nll: 3921.846680 \n",
      "(GPU: 0, epoch: 0, iters: 111200, time: 0.008) nll: 4642.587891 \n",
      "(GPU: 0, epoch: 0, iters: 112000, time: 0.008) nll: 4557.717773 \n",
      "(GPU: 0, epoch: 0, iters: 112800, time: 0.008) nll: 3689.414551 \n",
      "(GPU: 0, epoch: 0, iters: 113600, time: 0.008) nll: 3250.705078 \n",
      "(GPU: 0, epoch: 0, iters: 114400, time: 0.008) nll: 5378.149414 \n",
      "(GPU: 0, epoch: 0, iters: 115200, time: 0.008) nll: 4948.630371 \n",
      "(GPU: 0, epoch: 0, iters: 116000, time: 0.008) nll: 4458.374023 \n",
      "(GPU: 0, epoch: 0, iters: 116800, time: 0.008) nll: 3678.001709 \n",
      "(GPU: 0, epoch: 0, iters: 117600, time: 0.008) nll: 4185.390625 \n",
      "(GPU: 0, epoch: 0, iters: 118400, time: 0.008) nll: 2567.399658 \n",
      "(GPU: 0, epoch: 0, iters: 119200, time: 0.008) nll: 3581.552979 \n",
      "(GPU: 0, epoch: 0, iters: 120000, time: 0.008) nll: 5076.766602 \n",
      "saving the latest model (epoch 0, total_steps 120000)\n",
      "(GPU: 0, epoch: 0, iters: 120800, time: 0.008) nll: 3514.806396 \n",
      "(GPU: 0, epoch: 0, iters: 121600, time: 0.008) nll: 4161.519531 \n",
      "(GPU: 0, epoch: 0, iters: 122400, time: 0.007) nll: 4120.119141 \n",
      "(GPU: 0, epoch: 0, iters: 123200, time: 0.008) nll: 2437.050537 \n",
      "(GPU: 0, epoch: 0, iters: 124000, time: 0.008) nll: 4027.773926 \n",
      "(GPU: 0, epoch: 0, iters: 124800, time: 0.008) nll: 4834.426270 \n",
      "(GPU: 0, epoch: 0, iters: 125600, time: 0.007) nll: 5555.873047 \n",
      "(GPU: 0, epoch: 0, iters: 126400, time: 0.008) nll: 4674.442383 \n",
      "(GPU: 0, epoch: 0, iters: 127200, time: 0.008) nll: 5404.315918 \n",
      "(GPU: 0, epoch: 0, iters: 128000, time: 0.008) nll: 4608.403320 \n",
      "(GPU: 0, epoch: 0, iters: 128800, time: 0.007) nll: 5904.165039 \n",
      "(GPU: 0, epoch: 0, iters: 129600, time: 0.008) nll: 2270.311768 \n",
      "(GPU: 0, epoch: 0, iters: 130400, time: 0.008) nll: 3825.554199 \n",
      "(GPU: 0, epoch: 0, iters: 131200, time: 0.008) nll: 4473.165039 \n",
      "(GPU: 0, epoch: 0, iters: 132000, time: 0.008) nll: 3530.017822 \n",
      "(GPU: 0, epoch: 0, iters: 132800, time: 0.008) nll: 4274.479980 \n",
      "(GPU: 0, epoch: 0, iters: 133600, time: 0.007) nll: 3518.192871 \n",
      "(GPU: 0, epoch: 0, iters: 134400, time: 0.008) nll: 3865.467529 \n",
      "(GPU: 0, epoch: 0, iters: 135200, time: 0.007) nll: 4936.180664 \n",
      "(GPU: 0, epoch: 0, iters: 136000, time: 0.008) nll: 5252.451660 \n",
      "(GPU: 0, epoch: 0, iters: 136800, time: 0.007) nll: 3917.824707 \n",
      "(GPU: 0, epoch: 0, iters: 137600, time: 0.008) nll: 3286.947266 \n",
      "(GPU: 0, epoch: 0, iters: 138400, time: 0.008) nll: 3566.498535 \n",
      "(GPU: 0, epoch: 0, iters: 139200, time: 0.008) nll: 3243.941895 \n",
      "(GPU: 0, epoch: 0, iters: 140000, time: 0.008) nll: 4779.338867 \n",
      "saving the latest model (epoch 0, total_steps 140000)\n",
      "saving the model at the end of epoch 0, iters 140704\n",
      "([test] GPU: 0, epoch: 0) \n",
      "OrderedDict()\n",
      "[*] End of epoch 0 / 25 \t Time Taken: 1263 sec \n",
      "/cluster/54/streakfull/ADL4CV/Project/src/plz-autosdf/logs/bert2vqsc-text2shape-seq-LR1e-4-new-bert-2-prev_z_shape_input-try-2\n",
      "[*] learning rate = 0.0000100\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 3002/4397 [13:51<06:00,  3.87it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 1, iters: 32, time: 0.004) nll: 4444.364258 \n",
      "(GPU: 0, epoch: 1, iters: 32, time: 0.004) nll: 4545.028320 \n",
      "(GPU: 0, epoch: 1, iters: 96, time: 0.007) nll: 2023.062988 \n",
      "(GPU: 0, epoch: 1, iters: 896, time: 0.008) nll: 3679.690186 \n",
      "(GPU: 0, epoch: 1, iters: 1696, time: 0.008) nll: 4536.358398 \n",
      "(GPU: 0, epoch: 1, iters: 2496, time: 0.008) nll: 3962.486084 \n",
      "(GPU: 0, epoch: 1, iters: 3296, time: 0.008) nll: 4070.291504 \n",
      "(GPU: 0, epoch: 1, iters: 4096, time: 0.008) nll: 4340.926758 \n",
      "(GPU: 0, epoch: 1, iters: 4896, time: 0.008) nll: 3167.177734 \n",
      "(GPU: 0, epoch: 1, iters: 5696, time: 0.008) nll: 3068.155273 \n",
      "(GPU: 0, epoch: 1, iters: 6496, time: 0.007) nll: 3237.228516 \n",
      "(GPU: 0, epoch: 1, iters: 7296, time: 0.008) nll: 4058.937256 \n",
      "(GPU: 0, epoch: 1, iters: 8096, time: 0.008) nll: 2741.515625 \n",
      "(GPU: 0, epoch: 1, iters: 8896, time: 0.008) nll: 3981.718750 \n",
      "(GPU: 0, epoch: 1, iters: 9696, time: 0.008) nll: 4252.559570 \n",
      "(GPU: 0, epoch: 1, iters: 10496, time: 0.008) nll: 5013.581055 \n",
      "(GPU: 0, epoch: 1, iters: 11296, time: 0.008) nll: 4304.722168 \n",
      "(GPU: 0, epoch: 1, iters: 12096, time: 0.008) nll: 2969.018311 \n",
      "(GPU: 0, epoch: 1, iters: 12896, time: 0.008) nll: 4868.315430 \n",
      "(GPU: 0, epoch: 1, iters: 13696, time: 0.008) nll: 3929.594727 \n",
      "(GPU: 0, epoch: 1, iters: 14496, time: 0.008) nll: 3969.895020 \n",
      "(GPU: 0, epoch: 1, iters: 15296, time: 0.008) nll: 4838.401367 \n",
      "(GPU: 0, epoch: 1, iters: 16096, time: 0.008) nll: 2859.865234 \n",
      "(GPU: 0, epoch: 1, iters: 16896, time: 0.008) nll: 4121.431152 \n",
      "(GPU: 0, epoch: 1, iters: 17696, time: 0.008) nll: 3966.714355 \n",
      "(GPU: 0, epoch: 1, iters: 18496, time: 0.008) nll: 3525.481934 \n",
      "(GPU: 0, epoch: 1, iters: 19296, time: 0.008) nll: 4984.755859 \n",
      "saving the latest model (epoch 1, total_steps 160000)\n",
      "(GPU: 0, epoch: 1, iters: 20096, time: 0.008) nll: 4312.285156 \n",
      "(GPU: 0, epoch: 1, iters: 20896, time: 0.007) nll: 3671.592773 \n",
      "(GPU: 0, epoch: 1, iters: 21696, time: 0.008) nll: 5963.369141 \n",
      "(GPU: 0, epoch: 1, iters: 22496, time: 0.007) nll: 4294.131836 \n",
      "(GPU: 0, epoch: 1, iters: 23296, time: 0.008) nll: 3112.576660 \n",
      "(GPU: 0, epoch: 1, iters: 24096, time: 0.008) nll: 2712.386719 \n",
      "(GPU: 0, epoch: 1, iters: 24896, time: 0.008) nll: 2940.221191 \n",
      "(GPU: 0, epoch: 1, iters: 25696, time: 0.008) nll: 4006.482422 \n",
      "(GPU: 0, epoch: 1, iters: 26496, time: 0.008) nll: 4427.786621 \n",
      "(GPU: 0, epoch: 1, iters: 27296, time: 0.008) nll: 2466.686523 \n",
      "(GPU: 0, epoch: 1, iters: 28096, time: 0.008) nll: 2777.870605 \n",
      "(GPU: 0, epoch: 1, iters: 28896, time: 0.008) nll: 3615.632324 \n",
      "(GPU: 0, epoch: 1, iters: 29696, time: 0.008) nll: 2948.875488 \n",
      "(GPU: 0, epoch: 1, iters: 30496, time: 0.008) nll: 2976.739258 \n",
      "(GPU: 0, epoch: 1, iters: 31296, time: 0.008) nll: 2790.613770 \n",
      "(GPU: 0, epoch: 1, iters: 32096, time: 0.007) nll: 4282.412109 \n",
      "(GPU: 0, epoch: 1, iters: 32896, time: 0.008) nll: 3552.845703 \n",
      "(GPU: 0, epoch: 1, iters: 33696, time: 0.008) nll: 3584.286865 \n",
      "(GPU: 0, epoch: 1, iters: 34496, time: 0.008) nll: 5515.481445 \n",
      "(GPU: 0, epoch: 1, iters: 35296, time: 0.008) nll: 3525.051270 \n",
      "(GPU: 0, epoch: 1, iters: 36096, time: 0.008) nll: 3390.209961 \n",
      "(GPU: 0, epoch: 1, iters: 36896, time: 0.008) nll: 4841.202148 \n",
      "(GPU: 0, epoch: 1, iters: 37696, time: 0.008) nll: 3677.508789 \n",
      "(GPU: 0, epoch: 1, iters: 38496, time: 0.008) nll: 4278.457520 \n",
      "(GPU: 0, epoch: 1, iters: 39296, time: 0.008) nll: 4293.758301 \n",
      "saving the latest model (epoch 1, total_steps 180000)\n",
      "(GPU: 0, epoch: 1, iters: 40096, time: 0.008) nll: 3368.183105 \n",
      "(GPU: 0, epoch: 1, iters: 40896, time: 0.008) nll: 3766.729492 \n",
      "(GPU: 0, epoch: 1, iters: 41696, time: 0.008) nll: 3064.487061 \n",
      "(GPU: 0, epoch: 1, iters: 42496, time: 0.008) nll: 3980.291748 \n",
      "(GPU: 0, epoch: 1, iters: 43296, time: 0.007) nll: 2239.057129 \n",
      "(GPU: 0, epoch: 1, iters: 44096, time: 0.008) nll: 4074.235107 \n",
      "(GPU: 0, epoch: 1, iters: 44896, time: 0.008) nll: 3703.798340 \n",
      "(GPU: 0, epoch: 1, iters: 45696, time: 0.008) nll: 4310.541992 \n",
      "(GPU: 0, epoch: 1, iters: 46496, time: 0.008) nll: 3536.672852 \n",
      "(GPU: 0, epoch: 1, iters: 47296, time: 0.008) nll: 2597.114746 \n",
      "(GPU: 0, epoch: 1, iters: 48096, time: 0.008) nll: 3967.091553 \n",
      "(GPU: 0, epoch: 1, iters: 48896, time: 0.008) nll: 3951.748047 \n",
      "(GPU: 0, epoch: 1, iters: 49696, time: 0.008) nll: 5002.778320 \n",
      "(GPU: 0, epoch: 1, iters: 50496, time: 0.008) nll: 5004.977539 \n",
      "(GPU: 0, epoch: 1, iters: 51296, time: 0.007) nll: 3797.966553 \n",
      "(GPU: 0, epoch: 1, iters: 51296, time: 0.012) nll: 3781.133301 \n",
      "(GPU: 0, epoch: 1, iters: 51296, time: 0.012) nll: 3038.790527 \n",
      "(GPU: 0, epoch: 1, iters: 52096, time: 0.008) nll: 3207.574707 \n",
      "(GPU: 0, epoch: 1, iters: 52896, time: 0.008) nll: 3168.858398 \n",
      "(GPU: 0, epoch: 1, iters: 53696, time: 0.008) nll: 5092.088379 \n",
      "(GPU: 0, epoch: 1, iters: 54496, time: 0.007) nll: 4195.685059 \n",
      "(GPU: 0, epoch: 1, iters: 55296, time: 0.008) nll: 4284.904297 \n",
      "(GPU: 0, epoch: 1, iters: 56096, time: 0.007) nll: 4001.238770 \n",
      "(GPU: 0, epoch: 1, iters: 56896, time: 0.008) nll: 4306.694824 \n",
      "(GPU: 0, epoch: 1, iters: 57696, time: 0.007) nll: 3767.238281 \n",
      "(GPU: 0, epoch: 1, iters: 58496, time: 0.008) nll: 3054.815186 \n",
      "(GPU: 0, epoch: 1, iters: 59296, time: 0.008) nll: 5163.874512 \n",
      "saving the latest model (epoch 1, total_steps 200000)\n",
      "(GPU: 0, epoch: 1, iters: 60096, time: 0.008) nll: 3937.694092 \n",
      "(GPU: 0, epoch: 1, iters: 60896, time: 0.008) nll: 3661.800537 \n",
      "(GPU: 0, epoch: 1, iters: 61696, time: 0.008) nll: 4205.963867 \n",
      "(GPU: 0, epoch: 1, iters: 62496, time: 0.008) nll: 3999.252441 \n",
      "(GPU: 0, epoch: 1, iters: 63296, time: 0.008) nll: 4361.895020 \n",
      "(GPU: 0, epoch: 1, iters: 64096, time: 0.007) nll: 4579.901367 \n",
      "(GPU: 0, epoch: 1, iters: 64896, time: 0.008) nll: 3794.196533 \n",
      "(GPU: 0, epoch: 1, iters: 65696, time: 0.007) nll: 4164.600098 \n",
      "(GPU: 0, epoch: 1, iters: 66496, time: 0.008) nll: 3575.229980 \n",
      "(GPU: 0, epoch: 1, iters: 67296, time: 0.007) nll: 4897.272949 \n",
      "(GPU: 0, epoch: 1, iters: 68096, time: 0.008) nll: 4449.375488 \n",
      "(GPU: 0, epoch: 1, iters: 68896, time: 0.008) nll: 3067.361572 \n",
      "(GPU: 0, epoch: 1, iters: 69696, time: 0.008) nll: 3960.454102 \n",
      "(GPU: 0, epoch: 1, iters: 70496, time: 0.008) nll: 4482.308594 \n",
      "(GPU: 0, epoch: 1, iters: 71296, time: 0.008) nll: 4022.029541 \n",
      "(GPU: 0, epoch: 1, iters: 72096, time: 0.008) nll: 4239.684570 \n",
      "(GPU: 0, epoch: 1, iters: 72896, time: 0.008) nll: 4112.764648 \n",
      "(GPU: 0, epoch: 1, iters: 73696, time: 0.008) nll: 3447.199219 \n",
      "(GPU: 0, epoch: 1, iters: 74496, time: 0.008) nll: 2143.210449 \n",
      "(GPU: 0, epoch: 1, iters: 75296, time: 0.008) nll: 3469.059570 \n",
      "(GPU: 0, epoch: 1, iters: 76096, time: 0.008) nll: 4680.281738 \n",
      "(GPU: 0, epoch: 1, iters: 76896, time: 0.008) nll: 5209.627930 \n",
      "(GPU: 0, epoch: 1, iters: 77696, time: 0.008) nll: 4535.552246 \n",
      "(GPU: 0, epoch: 1, iters: 78496, time: 0.008) nll: 4196.750000 \n",
      "(GPU: 0, epoch: 1, iters: 79296, time: 0.008) nll: 4887.586914 \n",
      "saving the latest model (epoch 1, total_steps 220000)\n",
      "(GPU: 0, epoch: 1, iters: 80096, time: 0.007) nll: 4271.434570 \n",
      "(GPU: 0, epoch: 1, iters: 80896, time: 0.008) nll: 4123.418945 \n",
      "(GPU: 0, epoch: 1, iters: 81696, time: 0.008) nll: 4361.328613 \n",
      "(GPU: 0, epoch: 1, iters: 82496, time: 0.008) nll: 2748.572998 \n",
      "(GPU: 0, epoch: 1, iters: 83296, time: 0.008) nll: 4479.292969 \n",
      "(GPU: 0, epoch: 1, iters: 84096, time: 0.008) nll: 4225.632324 \n",
      "(GPU: 0, epoch: 1, iters: 84896, time: 0.007) nll: 3636.044189 \n",
      "(GPU: 0, epoch: 1, iters: 85696, time: 0.008) nll: 3635.501709 \n",
      "(GPU: 0, epoch: 1, iters: 86496, time: 0.008) nll: 4981.030273 \n",
      "(GPU: 0, epoch: 1, iters: 87296, time: 0.008) nll: 3934.419922 \n",
      "(GPU: 0, epoch: 1, iters: 88096, time: 0.008) nll: 2654.014160 \n",
      "(GPU: 0, epoch: 1, iters: 88896, time: 0.008) nll: 4657.682617 \n",
      "(GPU: 0, epoch: 1, iters: 89696, time: 0.008) nll: 4306.171875 \n",
      "(GPU: 0, epoch: 1, iters: 90496, time: 0.008) nll: 3769.658691 \n",
      "(GPU: 0, epoch: 1, iters: 91296, time: 0.008) nll: 5687.787109 \n",
      "(GPU: 0, epoch: 1, iters: 92096, time: 0.008) nll: 4268.472168 \n",
      "(GPU: 0, epoch: 1, iters: 92896, time: 0.008) nll: 4813.227539 \n",
      "(GPU: 0, epoch: 1, iters: 93696, time: 0.008) nll: 4252.547852 \n",
      "(GPU: 0, epoch: 1, iters: 94496, time: 0.008) nll: 3622.972412 \n",
      "(GPU: 0, epoch: 1, iters: 95296, time: 0.008) nll: 3162.581055 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4397/4397 [20:33<00:00,  3.56it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 1, iters: 96096, time: 0.008) nll: 4295.518555 \n",
      "(GPU: 0, epoch: 1, iters: 96896, time: 0.008) nll: 4169.386230 \n",
      "(GPU: 0, epoch: 1, iters: 97696, time: 0.008) nll: 4239.116211 \n",
      "(GPU: 0, epoch: 1, iters: 98496, time: 0.008) nll: 3690.214111 \n",
      "(GPU: 0, epoch: 1, iters: 99296, time: 0.008) nll: 5480.861328 \n",
      "saving the latest model (epoch 1, total_steps 240000)\n",
      "(GPU: 0, epoch: 1, iters: 100096, time: 0.008) nll: 4712.179199 \n",
      "(GPU: 0, epoch: 1, iters: 100896, time: 0.008) nll: 3568.208984 \n",
      "(GPU: 0, epoch: 1, iters: 101696, time: 0.008) nll: 3564.466553 \n",
      "(GPU: 0, epoch: 1, iters: 102496, time: 0.007) nll: 4645.772461 \n",
      "(GPU: 0, epoch: 1, iters: 103296, time: 0.008) nll: 4718.063477 \n",
      "(GPU: 0, epoch: 1, iters: 104096, time: 0.007) nll: 3880.110352 \n",
      "(GPU: 0, epoch: 1, iters: 104896, time: 0.008) nll: 4193.549316 \n",
      "(GPU: 0, epoch: 1, iters: 105696, time: 0.008) nll: 4196.076172 \n",
      "(GPU: 0, epoch: 1, iters: 106496, time: 0.008) nll: 6342.206543 \n",
      "(GPU: 0, epoch: 1, iters: 107296, time: 0.008) nll: 3108.885010 \n",
      "(GPU: 0, epoch: 1, iters: 108096, time: 0.008) nll: 4399.923828 \n",
      "(GPU: 0, epoch: 1, iters: 108896, time: 0.008) nll: 3481.452637 \n",
      "(GPU: 0, epoch: 1, iters: 109696, time: 0.008) nll: 3405.979492 \n",
      "(GPU: 0, epoch: 1, iters: 110496, time: 0.008) nll: 4491.400391 \n",
      "(GPU: 0, epoch: 1, iters: 111296, time: 0.008) nll: 5657.756348 \n",
      "(GPU: 0, epoch: 1, iters: 112096, time: 0.008) nll: 3414.733398 \n",
      "(GPU: 0, epoch: 1, iters: 112896, time: 0.008) nll: 4274.639160 \n",
      "(GPU: 0, epoch: 1, iters: 113696, time: 0.008) nll: 4536.999023 \n",
      "(GPU: 0, epoch: 1, iters: 114496, time: 0.008) nll: 6407.614258 \n",
      "(GPU: 0, epoch: 1, iters: 115296, time: 0.007) nll: 5052.893555 \n",
      "(GPU: 0, epoch: 1, iters: 116096, time: 0.008) nll: 2199.923828 \n",
      "(GPU: 0, epoch: 1, iters: 116896, time: 0.008) nll: 4428.334961 \n",
      "(GPU: 0, epoch: 1, iters: 117696, time: 0.008) nll: 3595.083008 \n",
      "(GPU: 0, epoch: 1, iters: 118496, time: 0.008) nll: 3272.420654 \n",
      "(GPU: 0, epoch: 1, iters: 119296, time: 0.008) nll: 4947.742188 \n",
      "saving the latest model (epoch 1, total_steps 260000)\n",
      "(GPU: 0, epoch: 1, iters: 120096, time: 0.008) nll: 4661.604980 \n",
      "(GPU: 0, epoch: 1, iters: 120896, time: 0.008) nll: 4065.452637 \n",
      "(GPU: 0, epoch: 1, iters: 121696, time: 0.007) nll: 4755.930176 \n",
      "(GPU: 0, epoch: 1, iters: 122496, time: 0.008) nll: 5496.234375 \n",
      "(GPU: 0, epoch: 1, iters: 123296, time: 0.008) nll: 2641.156494 \n",
      "(GPU: 0, epoch: 1, iters: 124096, time: 0.008) nll: 4683.355469 \n",
      "(GPU: 0, epoch: 1, iters: 124896, time: 0.007) nll: 4110.753906 \n",
      "(GPU: 0, epoch: 1, iters: 125696, time: 0.008) nll: 4489.367188 \n",
      "(GPU: 0, epoch: 1, iters: 126496, time: 0.008) nll: 4882.560547 \n",
      "(GPU: 0, epoch: 1, iters: 127296, time: 0.008) nll: 4675.388672 \n",
      "(GPU: 0, epoch: 1, iters: 128096, time: 0.007) nll: 5048.800781 \n",
      "(GPU: 0, epoch: 1, iters: 128896, time: 0.008) nll: 4560.987793 \n",
      "(GPU: 0, epoch: 1, iters: 129696, time: 0.008) nll: 3679.112305 \n",
      "(GPU: 0, epoch: 1, iters: 130496, time: 0.008) nll: 2302.311523 \n",
      "(GPU: 0, epoch: 1, iters: 131296, time: 0.008) nll: 3484.143066 \n",
      "(GPU: 0, epoch: 1, iters: 132096, time: 0.008) nll: 5663.843750 \n",
      "(GPU: 0, epoch: 1, iters: 132896, time: 0.008) nll: 3823.846191 \n",
      "(GPU: 0, epoch: 1, iters: 133696, time: 0.008) nll: 3555.433105 \n",
      "(GPU: 0, epoch: 1, iters: 134496, time: 0.007) nll: 3213.814941 \n",
      "(GPU: 0, epoch: 1, iters: 135296, time: 0.008) nll: 2177.648193 \n",
      "(GPU: 0, epoch: 1, iters: 136096, time: 0.008) nll: 5030.901855 \n",
      "(GPU: 0, epoch: 1, iters: 136896, time: 0.008) nll: 3105.930908 \n",
      "(GPU: 0, epoch: 1, iters: 137696, time: 0.008) nll: 3915.571289 \n",
      "(GPU: 0, epoch: 1, iters: 138496, time: 0.008) nll: 2703.803711 \n",
      "(GPU: 0, epoch: 1, iters: 139296, time: 0.008) nll: 3768.775635 \n",
      "saving the latest model (epoch 1, total_steps 280000)\n",
      "(GPU: 0, epoch: 1, iters: 140096, time: 0.008) nll: 3875.747314 \n",
      "[*] End of epoch 1 / 25 \t Time Taken: 1234 sec \n",
      "/cluster/54/streakfull/ADL4CV/Project/src/plz-autosdf/logs/bert2vqsc-text2shape-seq-LR1e-4-new-bert-2-prev_z_shape_input-try-2\n",
      "[*] learning rate = 0.0000200\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 3005/4397 [13:51<06:01,  3.85it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 2, iters: 32, time: 0.004) nll: 3087.766602 \n",
      "(GPU: 0, epoch: 2, iters: 32, time: 0.004) nll: 2383.420410 \n",
      "(GPU: 0, epoch: 2, iters: 192, time: 0.008) nll: 3458.197754 \n",
      "(GPU: 0, epoch: 2, iters: 992, time: 0.008) nll: 4890.766113 \n",
      "(GPU: 0, epoch: 2, iters: 1792, time: 0.008) nll: 3598.987061 \n",
      "(GPU: 0, epoch: 2, iters: 2592, time: 0.008) nll: 3434.906250 \n",
      "(GPU: 0, epoch: 2, iters: 3392, time: 0.008) nll: 4556.833008 \n",
      "(GPU: 0, epoch: 2, iters: 4192, time: 0.007) nll: 2861.690918 \n",
      "(GPU: 0, epoch: 2, iters: 4992, time: 0.008) nll: 3862.111084 \n",
      "(GPU: 0, epoch: 2, iters: 5792, time: 0.008) nll: 4555.005859 \n",
      "(GPU: 0, epoch: 2, iters: 6592, time: 0.008) nll: 3769.271973 \n",
      "(GPU: 0, epoch: 2, iters: 6592, time: 0.013) nll: 3741.162354 \n",
      "(GPU: 0, epoch: 2, iters: 6592, time: 0.013) nll: 3163.869629 \n",
      "(GPU: 0, epoch: 2, iters: 7392, time: 0.007) nll: 3813.049805 \n",
      "(GPU: 0, epoch: 2, iters: 8192, time: 0.008) nll: 5030.416992 \n",
      "(GPU: 0, epoch: 2, iters: 8992, time: 0.008) nll: 3793.115234 \n",
      "(GPU: 0, epoch: 2, iters: 9792, time: 0.008) nll: 3539.064209 \n",
      "(GPU: 0, epoch: 2, iters: 10592, time: 0.008) nll: 4255.898438 \n",
      "(GPU: 0, epoch: 2, iters: 11392, time: 0.008) nll: 4230.121582 \n",
      "(GPU: 0, epoch: 2, iters: 12192, time: 0.007) nll: 5354.277344 \n",
      "(GPU: 0, epoch: 2, iters: 12992, time: 0.008) nll: 3370.130371 \n",
      "(GPU: 0, epoch: 2, iters: 13792, time: 0.008) nll: 4699.577148 \n",
      "(GPU: 0, epoch: 2, iters: 14592, time: 0.008) nll: 4640.185059 \n",
      "(GPU: 0, epoch: 2, iters: 15392, time: 0.008) nll: 3852.995117 \n",
      "(GPU: 0, epoch: 2, iters: 16192, time: 0.008) nll: 4654.121094 \n",
      "(GPU: 0, epoch: 2, iters: 16992, time: 0.008) nll: 4302.457031 \n",
      "(GPU: 0, epoch: 2, iters: 17792, time: 0.008) nll: 4639.367188 \n",
      "(GPU: 0, epoch: 2, iters: 18592, time: 0.008) nll: 3564.366699 \n",
      "saving the latest model (epoch 2, total_steps 300000)\n",
      "(GPU: 0, epoch: 2, iters: 19392, time: 0.008) nll: 3775.513672 \n",
      "(GPU: 0, epoch: 2, iters: 20192, time: 0.008) nll: 4519.625488 \n",
      "(GPU: 0, epoch: 2, iters: 20992, time: 0.008) nll: 3647.008301 \n",
      "(GPU: 0, epoch: 2, iters: 21792, time: 0.008) nll: 4170.165039 \n",
      "(GPU: 0, epoch: 2, iters: 22592, time: 0.008) nll: 3761.654297 \n",
      "(GPU: 0, epoch: 2, iters: 23392, time: 0.007) nll: 3596.248047 \n",
      "(GPU: 0, epoch: 2, iters: 24192, time: 0.008) nll: 5838.856445 \n",
      "(GPU: 0, epoch: 2, iters: 24992, time: 0.007) nll: 3679.171387 \n",
      "(GPU: 0, epoch: 2, iters: 25792, time: 0.008) nll: 3555.621826 \n",
      "(GPU: 0, epoch: 2, iters: 26592, time: 0.008) nll: 4305.490723 \n",
      "(GPU: 0, epoch: 2, iters: 27392, time: 0.008) nll: 4843.455078 \n",
      "(GPU: 0, epoch: 2, iters: 28192, time: 0.008) nll: 3804.023438 \n",
      "(GPU: 0, epoch: 2, iters: 28992, time: 0.008) nll: 4829.647461 \n",
      "(GPU: 0, epoch: 2, iters: 29792, time: 0.007) nll: 2877.105469 \n",
      "(GPU: 0, epoch: 2, iters: 30592, time: 0.008) nll: 4881.220703 \n",
      "(GPU: 0, epoch: 2, iters: 31392, time: 0.008) nll: 5570.424805 \n",
      "(GPU: 0, epoch: 2, iters: 32192, time: 0.008) nll: 4769.569336 \n",
      "(GPU: 0, epoch: 2, iters: 32992, time: 0.008) nll: 4789.785645 \n",
      "(GPU: 0, epoch: 2, iters: 33792, time: 0.008) nll: 3895.769775 \n",
      "(GPU: 0, epoch: 2, iters: 34592, time: 0.008) nll: 3712.494141 \n",
      "(GPU: 0, epoch: 2, iters: 35392, time: 0.008) nll: 4975.494141 \n",
      "(GPU: 0, epoch: 2, iters: 36192, time: 0.008) nll: 3855.091797 \n",
      "(GPU: 0, epoch: 2, iters: 36992, time: 0.008) nll: 2948.033203 \n",
      "(GPU: 0, epoch: 2, iters: 37792, time: 0.008) nll: 4857.151855 \n",
      "(GPU: 0, epoch: 2, iters: 38592, time: 0.008) nll: 4899.034180 \n",
      "saving the latest model (epoch 2, total_steps 320000)\n",
      "(GPU: 0, epoch: 2, iters: 39392, time: 0.008) nll: 4451.367188 \n",
      "(GPU: 0, epoch: 2, iters: 40192, time: 0.008) nll: 3436.959961 \n",
      "(GPU: 0, epoch: 2, iters: 40992, time: 0.008) nll: 5097.981445 \n",
      "(GPU: 0, epoch: 2, iters: 41792, time: 0.008) nll: 2784.472900 \n",
      "(GPU: 0, epoch: 2, iters: 42592, time: 0.008) nll: 4456.955078 \n",
      "(GPU: 0, epoch: 2, iters: 43392, time: 0.008) nll: 3907.481934 \n",
      "(GPU: 0, epoch: 2, iters: 44192, time: 0.008) nll: 4000.549805 \n",
      "(GPU: 0, epoch: 2, iters: 44992, time: 0.008) nll: 4577.250488 \n",
      "(GPU: 0, epoch: 2, iters: 45792, time: 0.008) nll: 4436.618164 \n",
      "(GPU: 0, epoch: 2, iters: 46592, time: 0.008) nll: 4487.647461 \n",
      "(GPU: 0, epoch: 2, iters: 47392, time: 0.008) nll: 2660.667236 \n",
      "(GPU: 0, epoch: 2, iters: 48192, time: 0.008) nll: 3848.321289 \n",
      "(GPU: 0, epoch: 2, iters: 48992, time: 0.008) nll: 4204.091797 \n",
      "(GPU: 0, epoch: 2, iters: 49792, time: 0.008) nll: 4166.362305 \n",
      "(GPU: 0, epoch: 2, iters: 50592, time: 0.008) nll: 3990.492188 \n",
      "(GPU: 0, epoch: 2, iters: 51392, time: 0.008) nll: 2993.690430 \n",
      "(GPU: 0, epoch: 2, iters: 52192, time: 0.008) nll: 5322.192871 \n",
      "(GPU: 0, epoch: 2, iters: 52992, time: 0.008) nll: 4669.773926 \n",
      "(GPU: 0, epoch: 2, iters: 53792, time: 0.008) nll: 3115.102051 \n",
      "(GPU: 0, epoch: 2, iters: 54592, time: 0.008) nll: 4036.037598 \n",
      "(GPU: 0, epoch: 2, iters: 55392, time: 0.007) nll: 3924.087402 \n",
      "(GPU: 0, epoch: 2, iters: 56192, time: 0.008) nll: 4063.051025 \n",
      "(GPU: 0, epoch: 2, iters: 56992, time: 0.008) nll: 3803.197266 \n",
      "(GPU: 0, epoch: 2, iters: 57792, time: 0.008) nll: 3514.040039 \n",
      "(GPU: 0, epoch: 2, iters: 58592, time: 0.008) nll: 3197.609863 \n",
      "saving the latest model (epoch 2, total_steps 340000)\n",
      "(GPU: 0, epoch: 2, iters: 59392, time: 0.008) nll: 4171.061035 \n",
      "(GPU: 0, epoch: 2, iters: 60192, time: 0.008) nll: 4377.470215 \n",
      "(GPU: 0, epoch: 2, iters: 60992, time: 0.008) nll: 3802.341553 \n",
      "(GPU: 0, epoch: 2, iters: 61792, time: 0.007) nll: 3364.452637 \n",
      "(GPU: 0, epoch: 2, iters: 62592, time: 0.008) nll: 4387.990234 \n",
      "(GPU: 0, epoch: 2, iters: 63392, time: 0.007) nll: 3286.560791 \n",
      "(GPU: 0, epoch: 2, iters: 64192, time: 0.008) nll: 3847.054199 \n",
      "(GPU: 0, epoch: 2, iters: 64992, time: 0.008) nll: 4495.190430 \n",
      "(GPU: 0, epoch: 2, iters: 65792, time: 0.008) nll: 3302.452393 \n",
      "(GPU: 0, epoch: 2, iters: 66592, time: 0.008) nll: 4266.777344 \n",
      "(GPU: 0, epoch: 2, iters: 67392, time: 0.008) nll: 2582.029053 \n",
      "(GPU: 0, epoch: 2, iters: 68192, time: 0.008) nll: 3134.799316 \n",
      "(GPU: 0, epoch: 2, iters: 68992, time: 0.008) nll: 3693.423096 \n",
      "(GPU: 0, epoch: 2, iters: 69792, time: 0.008) nll: 3120.910645 \n",
      "(GPU: 0, epoch: 2, iters: 70592, time: 0.008) nll: 4472.940430 \n",
      "(GPU: 0, epoch: 2, iters: 71392, time: 0.008) nll: 4257.566895 \n",
      "(GPU: 0, epoch: 2, iters: 72192, time: 0.008) nll: 4074.506348 \n",
      "(GPU: 0, epoch: 2, iters: 72992, time: 0.008) nll: 4471.140625 \n",
      "(GPU: 0, epoch: 2, iters: 73792, time: 0.008) nll: 2758.321777 \n",
      "(GPU: 0, epoch: 2, iters: 74592, time: 0.008) nll: 4752.228516 \n",
      "(GPU: 0, epoch: 2, iters: 75392, time: 0.008) nll: 4033.756104 \n",
      "(GPU: 0, epoch: 2, iters: 76192, time: 0.008) nll: 3039.051758 \n",
      "(GPU: 0, epoch: 2, iters: 76992, time: 0.008) nll: 3906.633301 \n",
      "(GPU: 0, epoch: 2, iters: 77792, time: 0.008) nll: 4958.210938 \n",
      "(GPU: 0, epoch: 2, iters: 78592, time: 0.008) nll: 4482.120117 \n",
      "saving the latest model (epoch 2, total_steps 360000)\n",
      "(GPU: 0, epoch: 2, iters: 79392, time: 0.008) nll: 2797.192871 \n",
      "(GPU: 0, epoch: 2, iters: 80192, time: 0.008) nll: 4432.574707 \n",
      "(GPU: 0, epoch: 2, iters: 80992, time: 0.008) nll: 4099.147461 \n",
      "(GPU: 0, epoch: 2, iters: 81792, time: 0.008) nll: 3874.803711 \n",
      "(GPU: 0, epoch: 2, iters: 82592, time: 0.008) nll: 4526.474121 \n",
      "(GPU: 0, epoch: 2, iters: 83392, time: 0.008) nll: 3893.550781 \n",
      "(GPU: 0, epoch: 2, iters: 84192, time: 0.008) nll: 3143.483398 \n",
      "(GPU: 0, epoch: 2, iters: 84992, time: 0.008) nll: 4449.699219 \n",
      "(GPU: 0, epoch: 2, iters: 85792, time: 0.008) nll: 3283.057373 \n",
      "(GPU: 0, epoch: 2, iters: 86592, time: 0.008) nll: 2662.965820 \n",
      "(GPU: 0, epoch: 2, iters: 87392, time: 0.008) nll: 4708.927734 \n",
      "(GPU: 0, epoch: 2, iters: 88192, time: 0.008) nll: 3397.881104 \n",
      "(GPU: 0, epoch: 2, iters: 88992, time: 0.008) nll: 4164.988770 \n",
      "(GPU: 0, epoch: 2, iters: 89792, time: 0.008) nll: 3843.284180 \n",
      "(GPU: 0, epoch: 2, iters: 90592, time: 0.007) nll: 3332.836914 \n",
      "(GPU: 0, epoch: 2, iters: 91392, time: 0.008) nll: 2860.013916 \n",
      "(GPU: 0, epoch: 2, iters: 92192, time: 0.008) nll: 3984.675049 \n",
      "(GPU: 0, epoch: 2, iters: 92992, time: 0.008) nll: 4008.626465 \n",
      "(GPU: 0, epoch: 2, iters: 93792, time: 0.008) nll: 4476.258789 \n",
      "(GPU: 0, epoch: 2, iters: 94592, time: 0.008) nll: 3582.768311 \n",
      "(GPU: 0, epoch: 2, iters: 95392, time: 0.008) nll: 2177.487793 \n",
      "(GPU: 0, epoch: 2, iters: 96192, time: 0.008) nll: 5241.937012 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4397/4397 [20:32<00:00,  3.57it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(GPU: 0, epoch: 2, iters: 96992, time: 0.008) nll: 4733.043945 \n",
      "(GPU: 0, epoch: 2, iters: 97792, time: 0.008) nll: 4357.597168 \n",
      "(GPU: 0, epoch: 2, iters: 98592, time: 0.008) nll: 5568.921875 \n",
      "saving the latest model (epoch 2, total_steps 380000)\n",
      "(GPU: 0, epoch: 2, iters: 99392, time: 0.008) nll: 3801.025635 \n",
      "(GPU: 0, epoch: 2, iters: 100192, time: 0.008) nll: 3014.718262 \n",
      "(GPU: 0, epoch: 2, iters: 100992, time: 0.008) nll: 3484.334961 \n",
      "(GPU: 0, epoch: 2, iters: 101792, time: 0.007) nll: 4771.982910 \n",
      "(GPU: 0, epoch: 2, iters: 102592, time: 0.008) nll: 4983.834961 \n",
      "(GPU: 0, epoch: 2, iters: 102592, time: 0.013) nll: 4953.141602 \n",
      "(GPU: 0, epoch: 2, iters: 102592, time: 0.013) nll: 4951.579590 \n",
      "(GPU: 0, epoch: 2, iters: 103392, time: 0.007) nll: 4266.812500 \n",
      "(GPU: 0, epoch: 2, iters: 104192, time: 0.008) nll: 3938.950684 \n",
      "(GPU: 0, epoch: 2, iters: 104992, time: 0.008) nll: 4416.411133 \n",
      "(GPU: 0, epoch: 2, iters: 105792, time: 0.008) nll: 3678.081543 \n",
      "(GPU: 0, epoch: 2, iters: 106592, time: 0.008) nll: 3343.426514 \n",
      "(GPU: 0, epoch: 2, iters: 107392, time: 0.008) nll: 3298.909424 \n",
      "(GPU: 0, epoch: 2, iters: 108192, time: 0.008) nll: 4131.571289 \n",
      "(GPU: 0, epoch: 2, iters: 108992, time: 0.008) nll: 2461.624268 \n",
      "(GPU: 0, epoch: 2, iters: 109792, time: 0.008) nll: 5172.386719 \n",
      "(GPU: 0, epoch: 2, iters: 110592, time: 0.008) nll: 3870.229004 \n",
      "(GPU: 0, epoch: 2, iters: 111392, time: 0.008) nll: 5455.935059 \n",
      "(GPU: 0, epoch: 2, iters: 112192, time: 0.008) nll: 4860.343750 \n",
      "(GPU: 0, epoch: 2, iters: 112992, time: 0.008) nll: 3316.134521 \n",
      "(GPU: 0, epoch: 2, iters: 113792, time: 0.008) nll: 4809.429688 \n",
      "(GPU: 0, epoch: 2, iters: 114592, time: 0.008) nll: 4555.285156 \n",
      "(GPU: 0, epoch: 2, iters: 115392, time: 0.008) nll: 4530.694336 \n",
      "(GPU: 0, epoch: 2, iters: 116192, time: 0.008) nll: 4454.814453 \n",
      "(GPU: 0, epoch: 2, iters: 116992, time: 0.008) nll: 3299.817627 \n",
      "(GPU: 0, epoch: 2, iters: 117792, time: 0.008) nll: 6165.367188 \n",
      "(GPU: 0, epoch: 2, iters: 118592, time: 0.008) nll: 4688.984375 \n",
      "saving the latest model (epoch 2, total_steps 400000)\n",
      "(GPU: 0, epoch: 2, iters: 119392, time: 0.007) nll: 3437.067871 \n",
      "(GPU: 0, epoch: 2, iters: 120192, time: 0.008) nll: 3901.746094 \n",
      "(GPU: 0, epoch: 2, iters: 120992, time: 0.008) nll: 4473.308594 \n",
      "(GPU: 0, epoch: 2, iters: 121792, time: 0.008) nll: 3894.351807 \n",
      "(GPU: 0, epoch: 2, iters: 122592, time: 0.008) nll: 5537.647461 \n",
      "(GPU: 0, epoch: 2, iters: 123392, time: 0.008) nll: 5475.060059 \n",
      "(GPU: 0, epoch: 2, iters: 124192, time: 0.008) nll: 4675.694336 \n",
      "(GPU: 0, epoch: 2, iters: 124992, time: 0.008) nll: 4538.627441 \n",
      "(GPU: 0, epoch: 2, iters: 125792, time: 0.008) nll: 3462.607910 \n",
      "(GPU: 0, epoch: 2, iters: 126592, time: 0.008) nll: 4942.152832 \n",
      "(GPU: 0, epoch: 2, iters: 127392, time: 0.007) nll: 4490.738281 \n",
      "(GPU: 0, epoch: 2, iters: 128192, time: 0.008) nll: 3489.966553 \n",
      "(GPU: 0, epoch: 2, iters: 128992, time: 0.008) nll: 3788.791992 \n",
      "(GPU: 0, epoch: 2, iters: 129792, time: 0.008) nll: 3542.236328 \n",
      "(GPU: 0, epoch: 2, iters: 130592, time: 0.008) nll: 3991.238525 \n",
      "(GPU: 0, epoch: 2, iters: 131392, time: 0.008) nll: 4482.334961 \n",
      "(GPU: 0, epoch: 2, iters: 132192, time: 0.008) nll: 4207.094238 \n",
      "(GPU: 0, epoch: 2, iters: 132992, time: 0.008) nll: 4491.299805 \n",
      "(GPU: 0, epoch: 2, iters: 133792, time: 0.008) nll: 3105.804688 \n",
      "(GPU: 0, epoch: 2, iters: 134592, time: 0.008) nll: 3115.884766 \n",
      "(GPU: 0, epoch: 2, iters: 135392, time: 0.008) nll: 5534.736328 \n",
      "(GPU: 0, epoch: 2, iters: 136192, time: 0.008) nll: 4971.556641 \n",
      "(GPU: 0, epoch: 2, iters: 136992, time: 0.008) nll: 3319.524414 \n",
      "(GPU: 0, epoch: 2, iters: 137792, time: 0.008) nll: 4037.669189 \n",
      "(GPU: 0, epoch: 2, iters: 138592, time: 0.007) nll: 4279.876465 \n",
      "saving the latest model (epoch 2, total_steps 420000)\n",
      "(GPU: 0, epoch: 2, iters: 139392, time: 0.008) nll: 4245.603027 \n",
      "(GPU: 0, epoch: 2, iters: 140192, time: 0.008) nll: 3391.756104 \n",
      "[*] End of epoch 2 / 25 \t Time Taken: 1233 sec \n",
      "/cluster/54/streakfull/ADL4CV/Project/src/plz-autosdf/logs/bert2vqsc-text2shape-seq-LR1e-4-new-bert-2-prev_z_shape_input-try-2\n",
      "[*] learning rate = 0.0000300\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 3008/4397 [13:52<05:58,  3.88it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 3, iters: 32, time: 0.004) nll: 5575.455078 \n",
      "(GPU: 0, epoch: 3, iters: 32, time: 0.004) nll: 4656.235352 \n",
      "(GPU: 0, epoch: 3, iters: 288, time: 0.008) nll: 3211.646240 \n",
      "(GPU: 0, epoch: 3, iters: 1088, time: 0.008) nll: 3267.287109 \n",
      "(GPU: 0, epoch: 3, iters: 1888, time: 0.008) nll: 3287.510254 \n",
      "(GPU: 0, epoch: 3, iters: 2688, time: 0.008) nll: 2924.624512 \n",
      "(GPU: 0, epoch: 3, iters: 3488, time: 0.008) nll: 4263.362305 \n",
      "(GPU: 0, epoch: 3, iters: 4288, time: 0.008) nll: 3685.710938 \n",
      "(GPU: 0, epoch: 3, iters: 5088, time: 0.007) nll: 5024.543945 \n",
      "(GPU: 0, epoch: 3, iters: 5888, time: 0.008) nll: 4199.166016 \n",
      "(GPU: 0, epoch: 3, iters: 6688, time: 0.007) nll: 5501.299805 \n",
      "(GPU: 0, epoch: 3, iters: 7488, time: 0.008) nll: 4039.209961 \n",
      "(GPU: 0, epoch: 3, iters: 8288, time: 0.008) nll: 3072.452637 \n",
      "(GPU: 0, epoch: 3, iters: 9088, time: 0.008) nll: 2097.652100 \n",
      "(GPU: 0, epoch: 3, iters: 9888, time: 0.008) nll: 3022.492432 \n",
      "(GPU: 0, epoch: 3, iters: 10688, time: 0.008) nll: 4420.255859 \n",
      "(GPU: 0, epoch: 3, iters: 11488, time: 0.008) nll: 3490.344727 \n",
      "(GPU: 0, epoch: 3, iters: 12288, time: 0.008) nll: 4647.034180 \n",
      "(GPU: 0, epoch: 3, iters: 13088, time: 0.008) nll: 4955.221191 \n",
      "(GPU: 0, epoch: 3, iters: 13888, time: 0.008) nll: 3713.001953 \n",
      "(GPU: 0, epoch: 3, iters: 14688, time: 0.008) nll: 3422.047363 \n",
      "(GPU: 0, epoch: 3, iters: 15488, time: 0.008) nll: 6021.231445 \n",
      "(GPU: 0, epoch: 3, iters: 16288, time: 0.008) nll: 3672.952148 \n",
      "(GPU: 0, epoch: 3, iters: 17088, time: 0.008) nll: 4077.497070 \n",
      "(GPU: 0, epoch: 3, iters: 17888, time: 0.008) nll: 3494.429199 \n",
      "saving the latest model (epoch 3, total_steps 440000)\n",
      "(GPU: 0, epoch: 3, iters: 18688, time: 0.008) nll: 3726.878906 \n",
      "(GPU: 0, epoch: 3, iters: 19488, time: 0.007) nll: 4152.585449 \n",
      "(GPU: 0, epoch: 3, iters: 20288, time: 0.008) nll: 3931.443359 \n",
      "(GPU: 0, epoch: 3, iters: 21088, time: 0.007) nll: 4257.691406 \n",
      "(GPU: 0, epoch: 3, iters: 21888, time: 0.008) nll: 3554.868652 \n",
      "(GPU: 0, epoch: 3, iters: 22688, time: 0.008) nll: 3432.465820 \n",
      "(GPU: 0, epoch: 3, iters: 23488, time: 0.008) nll: 3554.402344 \n",
      "(GPU: 0, epoch: 3, iters: 24288, time: 0.008) nll: 5013.390625 \n",
      "(GPU: 0, epoch: 3, iters: 25088, time: 0.008) nll: 5018.754883 \n",
      "(GPU: 0, epoch: 3, iters: 25888, time: 0.008) nll: 3432.172363 \n",
      "(GPU: 0, epoch: 3, iters: 26688, time: 0.008) nll: 4415.920410 \n",
      "(GPU: 0, epoch: 3, iters: 27488, time: 0.008) nll: 3656.106445 \n",
      "(GPU: 0, epoch: 3, iters: 28288, time: 0.008) nll: 4254.202637 \n",
      "(GPU: 0, epoch: 3, iters: 29088, time: 0.008) nll: 2417.349854 \n",
      "(GPU: 0, epoch: 3, iters: 29888, time: 0.008) nll: 3689.272949 \n",
      "(GPU: 0, epoch: 3, iters: 30688, time: 0.008) nll: 3873.623047 \n",
      "(GPU: 0, epoch: 3, iters: 31488, time: 0.008) nll: 4439.551758 \n",
      "(GPU: 0, epoch: 3, iters: 32288, time: 0.008) nll: 5326.724609 \n",
      "(GPU: 0, epoch: 3, iters: 33088, time: 0.008) nll: 4216.307129 \n",
      "(GPU: 0, epoch: 3, iters: 33888, time: 0.008) nll: 3305.042480 \n",
      "(GPU: 0, epoch: 3, iters: 34688, time: 0.008) nll: 4095.052490 \n",
      "(GPU: 0, epoch: 3, iters: 35488, time: 0.008) nll: 3683.795898 \n",
      "(GPU: 0, epoch: 3, iters: 36288, time: 0.008) nll: 2478.000000 \n",
      "(GPU: 0, epoch: 3, iters: 37088, time: 0.008) nll: 5559.508789 \n",
      "(GPU: 0, epoch: 3, iters: 37888, time: 0.008) nll: 4154.496094 \n",
      "saving the latest model (epoch 3, total_steps 460000)\n",
      "(GPU: 0, epoch: 3, iters: 38688, time: 0.008) nll: 5076.906738 \n",
      "(GPU: 0, epoch: 3, iters: 39488, time: 0.008) nll: 4514.161133 \n",
      "(GPU: 0, epoch: 3, iters: 40288, time: 0.008) nll: 3865.169434 \n",
      "(GPU: 0, epoch: 3, iters: 41088, time: 0.008) nll: 5219.359375 \n",
      "(GPU: 0, epoch: 3, iters: 41888, time: 0.008) nll: 4131.867188 \n",
      "(GPU: 0, epoch: 3, iters: 42688, time: 0.008) nll: 4382.274414 \n",
      "(GPU: 0, epoch: 3, iters: 43488, time: 0.008) nll: 4518.455566 \n",
      "(GPU: 0, epoch: 3, iters: 44288, time: 0.008) nll: 5085.641602 \n",
      "(GPU: 0, epoch: 3, iters: 45088, time: 0.008) nll: 3004.672363 \n",
      "(GPU: 0, epoch: 3, iters: 45888, time: 0.008) nll: 4890.562500 \n",
      "(GPU: 0, epoch: 3, iters: 46688, time: 0.008) nll: 3525.810791 \n",
      "(GPU: 0, epoch: 3, iters: 47488, time: 0.008) nll: 4213.274414 \n",
      "(GPU: 0, epoch: 3, iters: 48288, time: 0.008) nll: 4845.512695 \n",
      "(GPU: 0, epoch: 3, iters: 49088, time: 0.008) nll: 3773.357910 \n",
      "(GPU: 0, epoch: 3, iters: 49888, time: 0.008) nll: 3564.902588 \n",
      "(GPU: 0, epoch: 3, iters: 50688, time: 0.008) nll: 5260.789062 \n",
      "(GPU: 0, epoch: 3, iters: 51488, time: 0.008) nll: 4721.014648 \n",
      "(GPU: 0, epoch: 3, iters: 52288, time: 0.008) nll: 3506.057129 \n",
      "(GPU: 0, epoch: 3, iters: 53088, time: 0.008) nll: 2882.356689 \n",
      "(GPU: 0, epoch: 3, iters: 53888, time: 0.008) nll: 2881.551514 \n",
      "(GPU: 0, epoch: 3, iters: 54688, time: 0.008) nll: 3491.701172 \n",
      "(GPU: 0, epoch: 3, iters: 55488, time: 0.008) nll: 4472.038086 \n",
      "(GPU: 0, epoch: 3, iters: 56288, time: 0.008) nll: 3262.473145 \n",
      "(GPU: 0, epoch: 3, iters: 57088, time: 0.008) nll: 4232.169922 \n",
      "(GPU: 0, epoch: 3, iters: 57888, time: 0.008) nll: 5260.345703 \n",
      "(GPU: 0, epoch: 3, iters: 57888, time: 0.013) nll: 5223.250977 \n",
      "(GPU: 0, epoch: 3, iters: 57888, time: 0.013) nll: 4089.031738 \n",
      "saving the latest model (epoch 3, total_steps 480000)\n",
      "(GPU: 0, epoch: 3, iters: 58688, time: 0.008) nll: 5255.844727 \n",
      "(GPU: 0, epoch: 3, iters: 59488, time: 0.007) nll: 4594.301758 \n",
      "(GPU: 0, epoch: 3, iters: 60288, time: 0.008) nll: 3336.866699 \n",
      "(GPU: 0, epoch: 3, iters: 61088, time: 0.008) nll: 3728.162109 \n",
      "(GPU: 0, epoch: 3, iters: 61888, time: 0.008) nll: 3321.894287 \n",
      "(GPU: 0, epoch: 3, iters: 62688, time: 0.008) nll: 4347.249023 \n",
      "(GPU: 0, epoch: 3, iters: 63488, time: 0.008) nll: 2645.158691 \n",
      "(GPU: 0, epoch: 3, iters: 64288, time: 0.008) nll: 3215.107910 \n",
      "(GPU: 0, epoch: 3, iters: 65088, time: 0.008) nll: 3091.896729 \n",
      "(GPU: 0, epoch: 3, iters: 65888, time: 0.008) nll: 3676.625244 \n",
      "(GPU: 0, epoch: 3, iters: 66688, time: 0.008) nll: 4430.582520 \n",
      "(GPU: 0, epoch: 3, iters: 67488, time: 0.008) nll: 5228.201660 \n",
      "(GPU: 0, epoch: 3, iters: 68288, time: 0.008) nll: 5017.708008 \n",
      "(GPU: 0, epoch: 3, iters: 69088, time: 0.008) nll: 3398.102051 \n",
      "(GPU: 0, epoch: 3, iters: 69888, time: 0.008) nll: 5366.526855 \n",
      "(GPU: 0, epoch: 3, iters: 70688, time: 0.008) nll: 4704.272461 \n",
      "(GPU: 0, epoch: 3, iters: 71488, time: 0.008) nll: 3352.174316 \n",
      "(GPU: 0, epoch: 3, iters: 72288, time: 0.008) nll: 2340.419434 \n",
      "(GPU: 0, epoch: 3, iters: 73088, time: 0.008) nll: 4459.475586 \n",
      "(GPU: 0, epoch: 3, iters: 73888, time: 0.007) nll: 4915.249512 \n",
      "(GPU: 0, epoch: 3, iters: 74688, time: 0.008) nll: 3987.553223 \n",
      "(GPU: 0, epoch: 3, iters: 75488, time: 0.008) nll: 3637.247070 \n",
      "(GPU: 0, epoch: 3, iters: 76288, time: 0.008) nll: 3835.820068 \n",
      "(GPU: 0, epoch: 3, iters: 77088, time: 0.008) nll: 4633.288086 \n",
      "(GPU: 0, epoch: 3, iters: 77888, time: 0.008) nll: 4358.190430 \n",
      "saving the latest model (epoch 3, total_steps 500000)\n",
      "(GPU: 0, epoch: 3, iters: 78688, time: 0.008) nll: 4143.489258 \n",
      "(GPU: 0, epoch: 3, iters: 79488, time: 0.008) nll: 4765.668457 \n",
      "(GPU: 0, epoch: 3, iters: 80288, time: 0.007) nll: 4088.076416 \n",
      "(GPU: 0, epoch: 3, iters: 81088, time: 0.008) nll: 4788.617676 \n",
      "(GPU: 0, epoch: 3, iters: 81888, time: 0.008) nll: 4751.320312 \n",
      "(GPU: 0, epoch: 3, iters: 82688, time: 0.008) nll: 5607.293945 \n",
      "(GPU: 0, epoch: 3, iters: 83488, time: 0.007) nll: 3659.622803 \n",
      "(GPU: 0, epoch: 3, iters: 84288, time: 0.008) nll: 4059.160156 \n",
      "(GPU: 0, epoch: 3, iters: 85088, time: 0.008) nll: 3358.522949 \n",
      "(GPU: 0, epoch: 3, iters: 85888, time: 0.008) nll: 4023.248047 \n",
      "(GPU: 0, epoch: 3, iters: 86688, time: 0.008) nll: 3627.818359 \n",
      "(GPU: 0, epoch: 3, iters: 87488, time: 0.008) nll: 4096.971680 \n",
      "(GPU: 0, epoch: 3, iters: 88288, time: 0.008) nll: 4409.862305 \n",
      "(GPU: 0, epoch: 3, iters: 89088, time: 0.008) nll: 4897.225586 \n",
      "(GPU: 0, epoch: 3, iters: 89888, time: 0.008) nll: 5255.466797 \n",
      "(GPU: 0, epoch: 3, iters: 90688, time: 0.008) nll: 5150.872070 \n",
      "(GPU: 0, epoch: 3, iters: 91488, time: 0.008) nll: 1884.321655 \n",
      "(GPU: 0, epoch: 3, iters: 92288, time: 0.008) nll: 5343.196289 \n",
      "(GPU: 0, epoch: 3, iters: 93088, time: 0.008) nll: 4028.032227 \n",
      "(GPU: 0, epoch: 3, iters: 93888, time: 0.008) nll: 3107.609863 \n",
      "(GPU: 0, epoch: 3, iters: 94688, time: 0.008) nll: 3750.781250 \n",
      "(GPU: 0, epoch: 3, iters: 95488, time: 0.008) nll: 4712.090332 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4397/4397 [20:31<00:00,  3.57it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 3, iters: 96288, time: 0.007) nll: 4142.551758 \n",
      "(GPU: 0, epoch: 3, iters: 97088, time: 0.008) nll: 4480.836914 \n",
      "(GPU: 0, epoch: 3, iters: 97888, time: 0.007) nll: 4281.569336 \n",
      "saving the latest model (epoch 3, total_steps 520000)\n",
      "(GPU: 0, epoch: 3, iters: 98688, time: 0.008) nll: 4238.911621 \n",
      "(GPU: 0, epoch: 3, iters: 99488, time: 0.008) nll: 5235.021484 \n",
      "(GPU: 0, epoch: 3, iters: 100288, time: 0.008) nll: 3422.239502 \n",
      "(GPU: 0, epoch: 3, iters: 101088, time: 0.008) nll: 3627.467285 \n",
      "(GPU: 0, epoch: 3, iters: 101888, time: 0.008) nll: 2683.572266 \n",
      "(GPU: 0, epoch: 3, iters: 102688, time: 0.008) nll: 4432.629883 \n",
      "(GPU: 0, epoch: 3, iters: 103488, time: 0.008) nll: 3553.083008 \n",
      "(GPU: 0, epoch: 3, iters: 104288, time: 0.007) nll: 5270.259766 \n",
      "(GPU: 0, epoch: 3, iters: 105088, time: 0.008) nll: 5393.273926 \n",
      "(GPU: 0, epoch: 3, iters: 105888, time: 0.008) nll: 3786.799072 \n",
      "(GPU: 0, epoch: 3, iters: 106688, time: 0.008) nll: 3506.509277 \n",
      "(GPU: 0, epoch: 3, iters: 107488, time: 0.008) nll: 4284.620605 \n",
      "(GPU: 0, epoch: 3, iters: 108288, time: 0.008) nll: 6576.500488 \n",
      "(GPU: 0, epoch: 3, iters: 109088, time: 0.008) nll: 4260.554688 \n",
      "(GPU: 0, epoch: 3, iters: 109888, time: 0.008) nll: 3747.696045 \n",
      "(GPU: 0, epoch: 3, iters: 110688, time: 0.008) nll: 3355.240723 \n",
      "(GPU: 0, epoch: 3, iters: 111488, time: 0.008) nll: 3675.978516 \n",
      "(GPU: 0, epoch: 3, iters: 112288, time: 0.007) nll: 4256.230469 \n",
      "(GPU: 0, epoch: 3, iters: 113088, time: 0.008) nll: 4617.726074 \n",
      "(GPU: 0, epoch: 3, iters: 113888, time: 0.008) nll: 3339.480469 \n",
      "(GPU: 0, epoch: 3, iters: 114688, time: 0.008) nll: 4228.585449 \n",
      "(GPU: 0, epoch: 3, iters: 115488, time: 0.008) nll: 3612.821777 \n",
      "(GPU: 0, epoch: 3, iters: 116288, time: 0.008) nll: 4038.520020 \n",
      "(GPU: 0, epoch: 3, iters: 117088, time: 0.008) nll: 3053.728027 \n",
      "(GPU: 0, epoch: 3, iters: 117888, time: 0.008) nll: 3887.514160 \n",
      "saving the latest model (epoch 3, total_steps 540000)\n",
      "(GPU: 0, epoch: 3, iters: 118688, time: 0.007) nll: 2734.440430 \n",
      "(GPU: 0, epoch: 3, iters: 119488, time: 0.008) nll: 5182.458008 \n",
      "(GPU: 0, epoch: 3, iters: 120288, time: 0.008) nll: 4146.145508 \n",
      "(GPU: 0, epoch: 3, iters: 121088, time: 0.008) nll: 4889.851074 \n",
      "(GPU: 0, epoch: 3, iters: 121888, time: 0.007) nll: 4847.624023 \n",
      "(GPU: 0, epoch: 3, iters: 122688, time: 0.008) nll: 3562.410400 \n",
      "(GPU: 0, epoch: 3, iters: 123488, time: 0.008) nll: 3942.752197 \n",
      "(GPU: 0, epoch: 3, iters: 124288, time: 0.008) nll: 4404.106445 \n",
      "(GPU: 0, epoch: 3, iters: 125088, time: 0.007) nll: 3955.124023 \n",
      "(GPU: 0, epoch: 3, iters: 125888, time: 0.008) nll: 3146.420410 \n",
      "(GPU: 0, epoch: 3, iters: 126688, time: 0.008) nll: 4773.804688 \n",
      "(GPU: 0, epoch: 3, iters: 127488, time: 0.008) nll: 5262.836914 \n",
      "(GPU: 0, epoch: 3, iters: 128288, time: 0.008) nll: 4895.895020 \n",
      "(GPU: 0, epoch: 3, iters: 129088, time: 0.008) nll: 5382.574707 \n",
      "(GPU: 0, epoch: 3, iters: 129888, time: 0.008) nll: 3132.120361 \n",
      "(GPU: 0, epoch: 3, iters: 130688, time: 0.008) nll: 3909.932861 \n",
      "(GPU: 0, epoch: 3, iters: 131488, time: 0.008) nll: 4321.842773 \n",
      "(GPU: 0, epoch: 3, iters: 132288, time: 0.008) nll: 4981.922852 \n",
      "(GPU: 0, epoch: 3, iters: 133088, time: 0.008) nll: 3149.940430 \n",
      "(GPU: 0, epoch: 3, iters: 133888, time: 0.008) nll: 4611.542969 \n",
      "(GPU: 0, epoch: 3, iters: 134688, time: 0.008) nll: 2209.008301 \n",
      "(GPU: 0, epoch: 3, iters: 135488, time: 0.008) nll: 3233.018799 \n",
      "(GPU: 0, epoch: 3, iters: 136288, time: 0.008) nll: 3901.687012 \n",
      "(GPU: 0, epoch: 3, iters: 137088, time: 0.008) nll: 5150.350098 \n",
      "(GPU: 0, epoch: 3, iters: 137888, time: 0.008) nll: 4000.651855 \n",
      "saving the latest model (epoch 3, total_steps 560000)\n",
      "(GPU: 0, epoch: 3, iters: 138688, time: 0.008) nll: 3662.697021 \n",
      "(GPU: 0, epoch: 3, iters: 139488, time: 0.008) nll: 4124.719238 \n",
      "(GPU: 0, epoch: 3, iters: 140288, time: 0.008) nll: 5243.631348 \n",
      "saving the model at the end of epoch 3, iters 562816\n",
      "([test] GPU: 0, epoch: 3) \n",
      "OrderedDict()\n",
      "[*] End of epoch 3 / 25 \t Time Taken: 1259 sec \n",
      "/cluster/54/streakfull/ADL4CV/Project/src/plz-autosdf/logs/bert2vqsc-text2shape-seq-LR1e-4-new-bert-2-prev_z_shape_input-try-2\n",
      "[*] learning rate = 0.0000400\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 3011/4397 [13:50<05:58,  3.87it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 4, iters: 32, time: 0.004) nll: 3838.023682 \n",
      "(GPU: 0, epoch: 4, iters: 32, time: 0.004) nll: 4606.803711 \n",
      "(GPU: 0, epoch: 4, iters: 384, time: 0.008) nll: 2913.819580 \n",
      "(GPU: 0, epoch: 4, iters: 1184, time: 0.008) nll: 3742.105225 \n",
      "(GPU: 0, epoch: 4, iters: 1984, time: 0.008) nll: 3484.109863 \n",
      "(GPU: 0, epoch: 4, iters: 2784, time: 0.008) nll: 5276.911133 \n",
      "(GPU: 0, epoch: 4, iters: 3584, time: 0.008) nll: 2371.410645 \n",
      "(GPU: 0, epoch: 4, iters: 4384, time: 0.007) nll: 2286.815430 \n",
      "(GPU: 0, epoch: 4, iters: 5184, time: 0.008) nll: 3035.573730 \n",
      "(GPU: 0, epoch: 4, iters: 5984, time: 0.008) nll: 3521.335449 \n",
      "(GPU: 0, epoch: 4, iters: 6784, time: 0.008) nll: 3215.163574 \n",
      "(GPU: 0, epoch: 4, iters: 7584, time: 0.008) nll: 4896.657227 \n",
      "(GPU: 0, epoch: 4, iters: 8384, time: 0.008) nll: 3724.843018 \n",
      "(GPU: 0, epoch: 4, iters: 9184, time: 0.008) nll: 3210.219238 \n",
      "(GPU: 0, epoch: 4, iters: 9984, time: 0.008) nll: 4349.501465 \n",
      "(GPU: 0, epoch: 4, iters: 10784, time: 0.008) nll: 3579.093506 \n",
      "(GPU: 0, epoch: 4, iters: 11584, time: 0.008) nll: 4796.252441 \n",
      "(GPU: 0, epoch: 4, iters: 12384, time: 0.008) nll: 3107.927246 \n",
      "(GPU: 0, epoch: 4, iters: 13184, time: 0.008) nll: 4580.952148 \n",
      "(GPU: 0, epoch: 4, iters: 13184, time: 0.013) nll: 4548.938477 \n",
      "(GPU: 0, epoch: 4, iters: 13184, time: 0.013) nll: 3371.485596 \n",
      "(GPU: 0, epoch: 4, iters: 13984, time: 0.008) nll: 3930.376953 \n",
      "(GPU: 0, epoch: 4, iters: 14784, time: 0.008) nll: 3257.771973 \n",
      "(GPU: 0, epoch: 4, iters: 15584, time: 0.008) nll: 4426.506836 \n",
      "(GPU: 0, epoch: 4, iters: 16384, time: 0.008) nll: 4247.251953 \n",
      "(GPU: 0, epoch: 4, iters: 17184, time: 0.008) nll: 4639.975586 \n",
      "saving the latest model (epoch 4, total_steps 580000)\n",
      "(GPU: 0, epoch: 4, iters: 17984, time: 0.008) nll: 3781.252930 \n",
      "(GPU: 0, epoch: 4, iters: 18784, time: 0.008) nll: 4429.527344 \n",
      "(GPU: 0, epoch: 4, iters: 19584, time: 0.008) nll: 4534.295898 \n",
      "(GPU: 0, epoch: 4, iters: 20384, time: 0.008) nll: 3860.307129 \n",
      "(GPU: 0, epoch: 4, iters: 21184, time: 0.008) nll: 4645.539551 \n",
      "(GPU: 0, epoch: 4, iters: 21984, time: 0.008) nll: 4040.473389 \n",
      "(GPU: 0, epoch: 4, iters: 22784, time: 0.008) nll: 4302.170410 \n",
      "(GPU: 0, epoch: 4, iters: 23584, time: 0.007) nll: 3534.177246 \n",
      "(GPU: 0, epoch: 4, iters: 24384, time: 0.008) nll: 3775.385010 \n",
      "(GPU: 0, epoch: 4, iters: 25184, time: 0.008) nll: 4682.147949 \n",
      "(GPU: 0, epoch: 4, iters: 25984, time: 0.008) nll: 4640.474609 \n",
      "(GPU: 0, epoch: 4, iters: 26784, time: 0.008) nll: 4276.945312 \n",
      "(GPU: 0, epoch: 4, iters: 27584, time: 0.008) nll: 4747.991211 \n",
      "(GPU: 0, epoch: 4, iters: 28384, time: 0.008) nll: 3192.560547 \n",
      "(GPU: 0, epoch: 4, iters: 29184, time: 0.008) nll: 4122.582520 \n",
      "(GPU: 0, epoch: 4, iters: 29984, time: 0.007) nll: 5170.804688 \n",
      "(GPU: 0, epoch: 4, iters: 30784, time: 0.008) nll: 2983.999023 \n",
      "(GPU: 0, epoch: 4, iters: 31584, time: 0.008) nll: 3764.998291 \n",
      "(GPU: 0, epoch: 4, iters: 32384, time: 0.008) nll: 5610.453125 \n",
      "(GPU: 0, epoch: 4, iters: 33184, time: 0.008) nll: 3365.893066 \n",
      "(GPU: 0, epoch: 4, iters: 33984, time: 0.008) nll: 4807.898438 \n",
      "(GPU: 0, epoch: 4, iters: 34784, time: 0.008) nll: 2263.932617 \n",
      "(GPU: 0, epoch: 4, iters: 35584, time: 0.008) nll: 3156.798828 \n",
      "(GPU: 0, epoch: 4, iters: 36384, time: 0.008) nll: 4390.475586 \n",
      "(GPU: 0, epoch: 4, iters: 37184, time: 0.008) nll: 4503.466797 \n",
      "saving the latest model (epoch 4, total_steps 600000)\n",
      "(GPU: 0, epoch: 4, iters: 37984, time: 0.007) nll: 3780.465820 \n",
      "(GPU: 0, epoch: 4, iters: 38784, time: 0.008) nll: 3649.354004 \n",
      "(GPU: 0, epoch: 4, iters: 39584, time: 0.008) nll: 2407.548340 \n",
      "(GPU: 0, epoch: 4, iters: 40384, time: 0.008) nll: 3728.443359 \n",
      "(GPU: 0, epoch: 4, iters: 41184, time: 0.008) nll: 4572.243164 \n",
      "(GPU: 0, epoch: 4, iters: 41984, time: 0.008) nll: 4100.664062 \n",
      "(GPU: 0, epoch: 4, iters: 42784, time: 0.007) nll: 4373.283691 \n",
      "(GPU: 0, epoch: 4, iters: 43584, time: 0.008) nll: 5726.646973 \n",
      "(GPU: 0, epoch: 4, iters: 44384, time: 0.008) nll: 3299.270264 \n",
      "(GPU: 0, epoch: 4, iters: 45184, time: 0.008) nll: 3400.503418 \n",
      "(GPU: 0, epoch: 4, iters: 45984, time: 0.007) nll: 3013.288086 \n",
      "(GPU: 0, epoch: 4, iters: 46784, time: 0.008) nll: 3395.498535 \n",
      "(GPU: 0, epoch: 4, iters: 47584, time: 0.008) nll: 5766.789062 \n",
      "(GPU: 0, epoch: 4, iters: 48384, time: 0.008) nll: 3302.712891 \n",
      "(GPU: 0, epoch: 4, iters: 49184, time: 0.008) nll: 3857.682617 \n",
      "(GPU: 0, epoch: 4, iters: 49984, time: 0.008) nll: 4096.868164 \n",
      "(GPU: 0, epoch: 4, iters: 50784, time: 0.008) nll: 3173.292236 \n",
      "(GPU: 0, epoch: 4, iters: 51584, time: 0.008) nll: 5144.332520 \n",
      "(GPU: 0, epoch: 4, iters: 52384, time: 0.008) nll: 4520.523438 \n",
      "(GPU: 0, epoch: 4, iters: 53184, time: 0.008) nll: 5423.773438 \n",
      "(GPU: 0, epoch: 4, iters: 53984, time: 0.008) nll: 2777.739014 \n",
      "(GPU: 0, epoch: 4, iters: 54784, time: 0.008) nll: 4444.997070 \n",
      "(GPU: 0, epoch: 4, iters: 55584, time: 0.008) nll: 3459.883545 \n",
      "(GPU: 0, epoch: 4, iters: 56384, time: 0.008) nll: 4109.709961 \n",
      "(GPU: 0, epoch: 4, iters: 57184, time: 0.008) nll: 3182.594727 \n",
      "saving the latest model (epoch 4, total_steps 620000)\n",
      "(GPU: 0, epoch: 4, iters: 57984, time: 0.008) nll: 3174.306885 \n",
      "(GPU: 0, epoch: 4, iters: 58784, time: 0.007) nll: 3838.083496 \n",
      "(GPU: 0, epoch: 4, iters: 59584, time: 0.008) nll: 3028.218750 \n",
      "(GPU: 0, epoch: 4, iters: 60384, time: 0.008) nll: 4501.383301 \n",
      "(GPU: 0, epoch: 4, iters: 61184, time: 0.008) nll: 5851.604492 \n",
      "(GPU: 0, epoch: 4, iters: 61984, time: 0.008) nll: 3775.219727 \n",
      "(GPU: 0, epoch: 4, iters: 62784, time: 0.008) nll: 3221.424805 \n",
      "(GPU: 0, epoch: 4, iters: 63584, time: 0.008) nll: 3716.206055 \n",
      "(GPU: 0, epoch: 4, iters: 64384, time: 0.008) nll: 4278.681641 \n",
      "(GPU: 0, epoch: 4, iters: 65184, time: 0.007) nll: 4276.858398 \n",
      "(GPU: 0, epoch: 4, iters: 65984, time: 0.008) nll: 4114.115723 \n",
      "(GPU: 0, epoch: 4, iters: 66784, time: 0.008) nll: 3448.230957 \n",
      "(GPU: 0, epoch: 4, iters: 67584, time: 0.008) nll: 3267.363525 \n",
      "(GPU: 0, epoch: 4, iters: 68384, time: 0.008) nll: 4665.727539 \n",
      "(GPU: 0, epoch: 4, iters: 69184, time: 0.008) nll: 4224.623047 \n",
      "(GPU: 0, epoch: 4, iters: 69984, time: 0.007) nll: 3847.356201 \n",
      "(GPU: 0, epoch: 4, iters: 70784, time: 0.008) nll: 4234.121094 \n",
      "(GPU: 0, epoch: 4, iters: 71584, time: 0.008) nll: 2943.682617 \n",
      "(GPU: 0, epoch: 4, iters: 72384, time: 0.008) nll: 3744.423340 \n",
      "(GPU: 0, epoch: 4, iters: 73184, time: 0.008) nll: 2531.459473 \n",
      "(GPU: 0, epoch: 4, iters: 73984, time: 0.008) nll: 3716.283691 \n",
      "(GPU: 0, epoch: 4, iters: 74784, time: 0.008) nll: 3163.474609 \n",
      "(GPU: 0, epoch: 4, iters: 75584, time: 0.008) nll: 4881.847656 \n",
      "(GPU: 0, epoch: 4, iters: 76384, time: 0.007) nll: 4689.929688 \n",
      "(GPU: 0, epoch: 4, iters: 77184, time: 0.008) nll: 3523.219238 \n",
      "saving the latest model (epoch 4, total_steps 640000)\n",
      "(GPU: 0, epoch: 4, iters: 77984, time: 0.008) nll: 4726.061523 \n",
      "(GPU: 0, epoch: 4, iters: 78784, time: 0.008) nll: 3376.009766 \n",
      "(GPU: 0, epoch: 4, iters: 79584, time: 0.008) nll: 2575.898438 \n",
      "(GPU: 0, epoch: 4, iters: 80384, time: 0.008) nll: 5250.234375 \n",
      "(GPU: 0, epoch: 4, iters: 81184, time: 0.007) nll: 4049.176758 \n",
      "(GPU: 0, epoch: 4, iters: 81984, time: 0.008) nll: 3137.633301 \n",
      "(GPU: 0, epoch: 4, iters: 82784, time: 0.007) nll: 4112.166016 \n",
      "(GPU: 0, epoch: 4, iters: 83584, time: 0.008) nll: 3481.875000 \n",
      "(GPU: 0, epoch: 4, iters: 84384, time: 0.008) nll: 3848.896484 \n",
      "(GPU: 0, epoch: 4, iters: 85184, time: 0.008) nll: 3836.408203 \n",
      "(GPU: 0, epoch: 4, iters: 85984, time: 0.008) nll: 3694.256836 \n",
      "(GPU: 0, epoch: 4, iters: 86784, time: 0.008) nll: 3547.613770 \n",
      "(GPU: 0, epoch: 4, iters: 87584, time: 0.007) nll: 4452.679688 \n",
      "(GPU: 0, epoch: 4, iters: 88384, time: 0.008) nll: 2986.633301 \n",
      "(GPU: 0, epoch: 4, iters: 89184, time: 0.008) nll: 2602.132324 \n",
      "(GPU: 0, epoch: 4, iters: 89984, time: 0.008) nll: 5045.431641 \n",
      "(GPU: 0, epoch: 4, iters: 90784, time: 0.007) nll: 4388.190430 \n",
      "(GPU: 0, epoch: 4, iters: 91584, time: 0.008) nll: 4815.576172 \n",
      "(GPU: 0, epoch: 4, iters: 92384, time: 0.008) nll: 4428.265137 \n",
      "(GPU: 0, epoch: 4, iters: 93184, time: 0.008) nll: 4450.623047 \n",
      "(GPU: 0, epoch: 4, iters: 93984, time: 0.008) nll: 3310.493652 \n",
      "(GPU: 0, epoch: 4, iters: 94784, time: 0.008) nll: 3740.479492 \n",
      "(GPU: 0, epoch: 4, iters: 95584, time: 0.008) nll: 3171.015625 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4397/4397 [20:28<00:00,  3.58it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 4, iters: 96384, time: 0.008) nll: 4941.643066 \n",
      "(GPU: 0, epoch: 4, iters: 97184, time: 0.008) nll: 5060.648438 \n",
      "saving the latest model (epoch 4, total_steps 660000)\n",
      "(GPU: 0, epoch: 4, iters: 97984, time: 0.008) nll: 3205.001953 \n",
      "(GPU: 0, epoch: 4, iters: 98784, time: 0.008) nll: 6462.788574 \n",
      "(GPU: 0, epoch: 4, iters: 99584, time: 0.008) nll: 3693.386230 \n",
      "(GPU: 0, epoch: 4, iters: 100384, time: 0.008) nll: 4510.927734 \n",
      "(GPU: 0, epoch: 4, iters: 101184, time: 0.008) nll: 4066.540527 \n",
      "(GPU: 0, epoch: 4, iters: 101984, time: 0.008) nll: 3842.532227 \n",
      "(GPU: 0, epoch: 4, iters: 102784, time: 0.008) nll: 3387.076172 \n",
      "(GPU: 0, epoch: 4, iters: 103584, time: 0.008) nll: 4402.369141 \n",
      "(GPU: 0, epoch: 4, iters: 104384, time: 0.008) nll: 3206.807617 \n",
      "(GPU: 0, epoch: 4, iters: 105184, time: 0.008) nll: 3290.025146 \n",
      "(GPU: 0, epoch: 4, iters: 105984, time: 0.008) nll: 2664.216309 \n",
      "(GPU: 0, epoch: 4, iters: 106784, time: 0.007) nll: 3447.466553 \n",
      "(GPU: 0, epoch: 4, iters: 107584, time: 0.008) nll: 3527.201904 \n",
      "(GPU: 0, epoch: 4, iters: 108384, time: 0.008) nll: 3144.342041 \n",
      "(GPU: 0, epoch: 4, iters: 109184, time: 0.008) nll: 5791.008789 \n",
      "(GPU: 0, epoch: 4, iters: 109184, time: 0.013) nll: 5742.176758 \n",
      "(GPU: 0, epoch: 4, iters: 109184, time: 0.013) nll: 3398.895508 \n",
      "(GPU: 0, epoch: 4, iters: 109984, time: 0.008) nll: 5301.758301 \n",
      "(GPU: 0, epoch: 4, iters: 110784, time: 0.008) nll: 3899.192871 \n",
      "(GPU: 0, epoch: 4, iters: 111584, time: 0.008) nll: 2728.435059 \n",
      "(GPU: 0, epoch: 4, iters: 112384, time: 0.008) nll: 3407.391602 \n",
      "(GPU: 0, epoch: 4, iters: 113184, time: 0.008) nll: 1931.556152 \n",
      "(GPU: 0, epoch: 4, iters: 113984, time: 0.008) nll: 5040.552246 \n",
      "(GPU: 0, epoch: 4, iters: 114784, time: 0.007) nll: 4827.840332 \n",
      "(GPU: 0, epoch: 4, iters: 115584, time: 0.008) nll: 4295.541992 \n",
      "(GPU: 0, epoch: 4, iters: 116384, time: 0.008) nll: 3729.084473 \n",
      "(GPU: 0, epoch: 4, iters: 117184, time: 0.008) nll: 4908.582520 \n",
      "saving the latest model (epoch 4, total_steps 680000)\n",
      "(GPU: 0, epoch: 4, iters: 117984, time: 0.008) nll: 3087.008301 \n",
      "(GPU: 0, epoch: 4, iters: 118784, time: 0.008) nll: 3916.899902 \n",
      "(GPU: 0, epoch: 4, iters: 119584, time: 0.007) nll: 3826.144531 \n",
      "(GPU: 0, epoch: 4, iters: 120384, time: 0.008) nll: 4329.984375 \n",
      "(GPU: 0, epoch: 4, iters: 121184, time: 0.008) nll: 3406.114258 \n",
      "(GPU: 0, epoch: 4, iters: 121984, time: 0.008) nll: 3853.095703 \n",
      "(GPU: 0, epoch: 4, iters: 122784, time: 0.007) nll: 5351.764648 \n",
      "(GPU: 0, epoch: 4, iters: 123584, time: 0.008) nll: 4125.499023 \n",
      "(GPU: 0, epoch: 4, iters: 124384, time: 0.007) nll: 4144.833008 \n",
      "(GPU: 0, epoch: 4, iters: 125184, time: 0.008) nll: 4698.077637 \n",
      "(GPU: 0, epoch: 4, iters: 125984, time: 0.008) nll: 4679.623047 \n",
      "(GPU: 0, epoch: 4, iters: 126784, time: 0.008) nll: 3092.328369 \n",
      "(GPU: 0, epoch: 4, iters: 127584, time: 0.008) nll: 3820.235840 \n",
      "(GPU: 0, epoch: 4, iters: 128384, time: 0.008) nll: 4038.969971 \n",
      "(GPU: 0, epoch: 4, iters: 129184, time: 0.007) nll: 4475.952148 \n",
      "(GPU: 0, epoch: 4, iters: 129984, time: 0.008) nll: 2778.776367 \n",
      "(GPU: 0, epoch: 4, iters: 130784, time: 0.008) nll: 5487.562500 \n",
      "(GPU: 0, epoch: 4, iters: 131584, time: 0.008) nll: 3683.479492 \n",
      "(GPU: 0, epoch: 4, iters: 132384, time: 0.008) nll: 2981.284180 \n",
      "(GPU: 0, epoch: 4, iters: 133184, time: 0.008) nll: 3892.576172 \n",
      "(GPU: 0, epoch: 4, iters: 133984, time: 0.008) nll: 3553.424072 \n",
      "(GPU: 0, epoch: 4, iters: 134784, time: 0.008) nll: 3085.777344 \n",
      "(GPU: 0, epoch: 4, iters: 135584, time: 0.008) nll: 4041.177490 \n",
      "(GPU: 0, epoch: 4, iters: 136384, time: 0.008) nll: 4772.526367 \n",
      "(GPU: 0, epoch: 4, iters: 137184, time: 0.008) nll: 4047.424072 \n",
      "saving the latest model (epoch 4, total_steps 700000)\n",
      "(GPU: 0, epoch: 4, iters: 137984, time: 0.008) nll: 4530.202148 \n",
      "(GPU: 0, epoch: 4, iters: 138784, time: 0.007) nll: 1701.132935 \n",
      "(GPU: 0, epoch: 4, iters: 139584, time: 0.008) nll: 4242.341797 \n",
      "(GPU: 0, epoch: 4, iters: 140384, time: 0.008) nll: 3306.318359 \n",
      "[*] End of epoch 4 / 25 \t Time Taken: 1229 sec \n",
      "/cluster/54/streakfull/ADL4CV/Project/src/plz-autosdf/logs/bert2vqsc-text2shape-seq-LR1e-4-new-bert-2-prev_z_shape_input-try-2\n",
      "[*] learning rate = 0.0000500\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▊   | 3014/4397 [13:53<05:58,  3.86it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 5, iters: 32, time: 0.004) nll: 3144.783447 \n",
      "(GPU: 0, epoch: 5, iters: 32, time: 0.004) nll: 3157.396973 \n",
      "(GPU: 0, epoch: 5, iters: 480, time: 0.008) nll: 4020.372070 \n",
      "(GPU: 0, epoch: 5, iters: 1280, time: 0.008) nll: 3678.586426 \n",
      "(GPU: 0, epoch: 5, iters: 2080, time: 0.008) nll: 4744.536133 \n",
      "(GPU: 0, epoch: 5, iters: 2880, time: 0.008) nll: 3205.185791 \n",
      "(GPU: 0, epoch: 5, iters: 3680, time: 0.008) nll: 5619.012695 \n",
      "(GPU: 0, epoch: 5, iters: 4480, time: 0.008) nll: 3372.065674 \n",
      "(GPU: 0, epoch: 5, iters: 5280, time: 0.007) nll: 4699.205566 \n",
      "(GPU: 0, epoch: 5, iters: 6080, time: 0.008) nll: 4864.879883 \n",
      "(GPU: 0, epoch: 5, iters: 6880, time: 0.008) nll: 3935.713379 \n",
      "(GPU: 0, epoch: 5, iters: 7680, time: 0.008) nll: 3878.333984 \n",
      "(GPU: 0, epoch: 5, iters: 8480, time: 0.008) nll: 2484.267090 \n",
      "(GPU: 0, epoch: 5, iters: 9280, time: 0.008) nll: 3849.626709 \n",
      "(GPU: 0, epoch: 5, iters: 10080, time: 0.008) nll: 1833.927979 \n",
      "(GPU: 0, epoch: 5, iters: 10880, time: 0.008) nll: 4649.300781 \n",
      "(GPU: 0, epoch: 5, iters: 11680, time: 0.007) nll: 3486.528809 \n",
      "(GPU: 0, epoch: 5, iters: 12480, time: 0.008) nll: 6750.976074 \n",
      "(GPU: 0, epoch: 5, iters: 13280, time: 0.008) nll: 4501.902344 \n",
      "(GPU: 0, epoch: 5, iters: 14080, time: 0.008) nll: 4223.578125 \n",
      "(GPU: 0, epoch: 5, iters: 14880, time: 0.008) nll: 3901.376221 \n",
      "(GPU: 0, epoch: 5, iters: 15680, time: 0.008) nll: 3862.896484 \n",
      "(GPU: 0, epoch: 5, iters: 16480, time: 0.008) nll: 5737.179688 \n",
      "saving the latest model (epoch 5, total_steps 720000)\n",
      "(GPU: 0, epoch: 5, iters: 17280, time: 0.008) nll: 4450.873047 \n",
      "(GPU: 0, epoch: 5, iters: 18080, time: 0.007) nll: 3530.604004 \n",
      "(GPU: 0, epoch: 5, iters: 18880, time: 0.008) nll: 4243.359375 \n",
      "(GPU: 0, epoch: 5, iters: 19680, time: 0.008) nll: 3474.766113 \n",
      "(GPU: 0, epoch: 5, iters: 20480, time: 0.008) nll: 2256.049072 \n",
      "(GPU: 0, epoch: 5, iters: 21280, time: 0.007) nll: 3565.133301 \n",
      "(GPU: 0, epoch: 5, iters: 22080, time: 0.008) nll: 4593.085938 \n",
      "(GPU: 0, epoch: 5, iters: 22880, time: 0.008) nll: 2802.156250 \n",
      "(GPU: 0, epoch: 5, iters: 23680, time: 0.008) nll: 2890.934570 \n",
      "(GPU: 0, epoch: 5, iters: 24480, time: 0.008) nll: 4131.301270 \n",
      "(GPU: 0, epoch: 5, iters: 25280, time: 0.008) nll: 4430.742676 \n",
      "(GPU: 0, epoch: 5, iters: 26080, time: 0.008) nll: 4592.108398 \n",
      "(GPU: 0, epoch: 5, iters: 26880, time: 0.008) nll: 3437.826172 \n",
      "(GPU: 0, epoch: 5, iters: 27680, time: 0.008) nll: 4791.364746 \n",
      "(GPU: 0, epoch: 5, iters: 28480, time: 0.008) nll: 2995.118896 \n",
      "(GPU: 0, epoch: 5, iters: 29280, time: 0.008) nll: 3711.951904 \n",
      "(GPU: 0, epoch: 5, iters: 30080, time: 0.008) nll: 3927.636719 \n",
      "(GPU: 0, epoch: 5, iters: 30880, time: 0.008) nll: 3436.362305 \n",
      "(GPU: 0, epoch: 5, iters: 31680, time: 0.008) nll: 3791.431885 \n",
      "(GPU: 0, epoch: 5, iters: 32480, time: 0.008) nll: 3216.596191 \n",
      "(GPU: 0, epoch: 5, iters: 33280, time: 0.008) nll: 4070.617676 \n",
      "(GPU: 0, epoch: 5, iters: 34080, time: 0.008) nll: 3845.582031 \n",
      "(GPU: 0, epoch: 5, iters: 34880, time: 0.008) nll: 2750.306396 \n",
      "(GPU: 0, epoch: 5, iters: 35680, time: 0.008) nll: 4505.687500 \n",
      "(GPU: 0, epoch: 5, iters: 36480, time: 0.008) nll: 3805.796875 \n",
      "saving the latest model (epoch 5, total_steps 740000)\n",
      "(GPU: 0, epoch: 5, iters: 37280, time: 0.007) nll: 3489.610840 \n",
      "(GPU: 0, epoch: 5, iters: 38080, time: 0.008) nll: 4891.311523 \n",
      "(GPU: 0, epoch: 5, iters: 38880, time: 0.008) nll: 4315.848633 \n",
      "(GPU: 0, epoch: 5, iters: 39680, time: 0.008) nll: 2977.444336 \n",
      "(GPU: 0, epoch: 5, iters: 40480, time: 0.008) nll: 3605.395264 \n",
      "(GPU: 0, epoch: 5, iters: 41280, time: 0.008) nll: 3999.790039 \n",
      "(GPU: 0, epoch: 5, iters: 42080, time: 0.008) nll: 3293.894531 \n",
      "(GPU: 0, epoch: 5, iters: 42880, time: 0.008) nll: 3314.915771 \n",
      "(GPU: 0, epoch: 5, iters: 43680, time: 0.008) nll: 3335.302002 \n",
      "(GPU: 0, epoch: 5, iters: 44480, time: 0.008) nll: 3833.302002 \n",
      "(GPU: 0, epoch: 5, iters: 45280, time: 0.008) nll: 3835.597168 \n",
      "(GPU: 0, epoch: 5, iters: 46080, time: 0.008) nll: 3331.507324 \n",
      "(GPU: 0, epoch: 5, iters: 46880, time: 0.008) nll: 5231.858887 \n",
      "(GPU: 0, epoch: 5, iters: 47680, time: 0.008) nll: 4444.046875 \n",
      "(GPU: 0, epoch: 5, iters: 48480, time: 0.008) nll: 4032.306641 \n",
      "(GPU: 0, epoch: 5, iters: 49280, time: 0.008) nll: 3834.462402 \n",
      "(GPU: 0, epoch: 5, iters: 50080, time: 0.008) nll: 4430.854004 \n",
      "(GPU: 0, epoch: 5, iters: 50880, time: 0.008) nll: 4700.396484 \n",
      "(GPU: 0, epoch: 5, iters: 51680, time: 0.008) nll: 4301.819824 \n",
      "(GPU: 0, epoch: 5, iters: 52480, time: 0.008) nll: 3860.587891 \n",
      "(GPU: 0, epoch: 5, iters: 53280, time: 0.008) nll: 2838.739990 \n",
      "(GPU: 0, epoch: 5, iters: 54080, time: 0.008) nll: 3134.113525 \n",
      "(GPU: 0, epoch: 5, iters: 54880, time: 0.007) nll: 4066.920166 \n",
      "(GPU: 0, epoch: 5, iters: 55680, time: 0.008) nll: 3550.736816 \n",
      "(GPU: 0, epoch: 5, iters: 56480, time: 0.007) nll: 4442.220703 \n",
      "saving the latest model (epoch 5, total_steps 760000)\n",
      "(GPU: 0, epoch: 5, iters: 57280, time: 0.008) nll: 4809.770996 \n",
      "(GPU: 0, epoch: 5, iters: 58080, time: 0.007) nll: 3855.810059 \n",
      "(GPU: 0, epoch: 5, iters: 58880, time: 0.008) nll: 3833.872803 \n",
      "(GPU: 0, epoch: 5, iters: 59680, time: 0.008) nll: 4187.292480 \n",
      "(GPU: 0, epoch: 5, iters: 60480, time: 0.008) nll: 3297.260742 \n",
      "(GPU: 0, epoch: 5, iters: 61280, time: 0.008) nll: 4149.810547 \n",
      "(GPU: 0, epoch: 5, iters: 62080, time: 0.008) nll: 2939.852051 \n",
      "(GPU: 0, epoch: 5, iters: 62880, time: 0.008) nll: 3641.668945 \n",
      "(GPU: 0, epoch: 5, iters: 63680, time: 0.008) nll: 4372.734863 \n",
      "(GPU: 0, epoch: 5, iters: 64480, time: 0.008) nll: 3178.925293 \n",
      "(GPU: 0, epoch: 5, iters: 64480, time: 0.012) nll: 3133.836670 \n",
      "(GPU: 0, epoch: 5, iters: 64480, time: 0.012) nll: 4113.083496 \n",
      "(GPU: 0, epoch: 5, iters: 65280, time: 0.008) nll: 2846.198730 \n",
      "(GPU: 0, epoch: 5, iters: 66080, time: 0.007) nll: 4786.947266 \n",
      "(GPU: 0, epoch: 5, iters: 66880, time: 0.008) nll: 4384.833984 \n",
      "(GPU: 0, epoch: 5, iters: 67680, time: 0.008) nll: 2775.328857 \n",
      "(GPU: 0, epoch: 5, iters: 68480, time: 0.008) nll: 4654.552734 \n",
      "(GPU: 0, epoch: 5, iters: 69280, time: 0.008) nll: 3561.242188 \n",
      "(GPU: 0, epoch: 5, iters: 70080, time: 0.008) nll: 5197.832031 \n",
      "(GPU: 0, epoch: 5, iters: 70880, time: 0.008) nll: 3687.552979 \n",
      "(GPU: 0, epoch: 5, iters: 71680, time: 0.008) nll: 5200.988281 \n",
      "(GPU: 0, epoch: 5, iters: 72480, time: 0.008) nll: 3235.095459 \n",
      "(GPU: 0, epoch: 5, iters: 73280, time: 0.008) nll: 4617.763672 \n",
      "(GPU: 0, epoch: 5, iters: 74080, time: 0.008) nll: 5037.847168 \n",
      "(GPU: 0, epoch: 5, iters: 74880, time: 0.008) nll: 4055.237305 \n",
      "(GPU: 0, epoch: 5, iters: 75680, time: 0.008) nll: 3203.557129 \n",
      "(GPU: 0, epoch: 5, iters: 76480, time: 0.008) nll: 4307.147461 \n",
      "saving the latest model (epoch 5, total_steps 780000)\n",
      "(GPU: 0, epoch: 5, iters: 77280, time: 0.008) nll: 3676.216064 \n",
      "(GPU: 0, epoch: 5, iters: 78080, time: 0.008) nll: 5072.867676 \n",
      "(GPU: 0, epoch: 5, iters: 78880, time: 0.008) nll: 2688.591797 \n",
      "(GPU: 0, epoch: 5, iters: 79680, time: 0.008) nll: 4629.028320 \n",
      "(GPU: 0, epoch: 5, iters: 80480, time: 0.008) nll: 4669.688477 \n",
      "(GPU: 0, epoch: 5, iters: 81280, time: 0.008) nll: 2834.603271 \n",
      "(GPU: 0, epoch: 5, iters: 82080, time: 0.008) nll: 4532.750977 \n",
      "(GPU: 0, epoch: 5, iters: 82880, time: 0.008) nll: 5626.891602 \n",
      "(GPU: 0, epoch: 5, iters: 83680, time: 0.008) nll: 3298.065430 \n",
      "(GPU: 0, epoch: 5, iters: 84480, time: 0.008) nll: 3362.727051 \n",
      "(GPU: 0, epoch: 5, iters: 85280, time: 0.008) nll: 3198.098633 \n",
      "(GPU: 0, epoch: 5, iters: 86080, time: 0.008) nll: 4041.377197 \n",
      "(GPU: 0, epoch: 5, iters: 86880, time: 0.008) nll: 4023.205566 \n",
      "(GPU: 0, epoch: 5, iters: 87680, time: 0.008) nll: 2903.475098 \n",
      "(GPU: 0, epoch: 5, iters: 88480, time: 0.008) nll: 3654.037598 \n",
      "(GPU: 0, epoch: 5, iters: 89280, time: 0.008) nll: 4604.098145 \n",
      "(GPU: 0, epoch: 5, iters: 90080, time: 0.008) nll: 3982.385254 \n",
      "(GPU: 0, epoch: 5, iters: 90880, time: 0.008) nll: 4367.359375 \n",
      "(GPU: 0, epoch: 5, iters: 91680, time: 0.008) nll: 2085.690918 \n",
      "(GPU: 0, epoch: 5, iters: 92480, time: 0.008) nll: 2195.355225 \n",
      "(GPU: 0, epoch: 5, iters: 93280, time: 0.008) nll: 4485.051758 \n",
      "(GPU: 0, epoch: 5, iters: 94080, time: 0.008) nll: 4311.526855 \n",
      "(GPU: 0, epoch: 5, iters: 94880, time: 0.007) nll: 5025.532715 \n",
      "(GPU: 0, epoch: 5, iters: 95680, time: 0.008) nll: 5635.619629 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4397/4397 [20:31<00:00,  3.57it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 5, iters: 96480, time: 0.008) nll: 4877.663086 \n",
      "saving the latest model (epoch 5, total_steps 800000)\n",
      "(GPU: 0, epoch: 5, iters: 97280, time: 0.008) nll: 4783.083984 \n",
      "(GPU: 0, epoch: 5, iters: 98080, time: 0.008) nll: 3030.072266 \n",
      "(GPU: 0, epoch: 5, iters: 98880, time: 0.008) nll: 4722.750000 \n",
      "(GPU: 0, epoch: 5, iters: 99680, time: 0.008) nll: 3613.783691 \n",
      "(GPU: 0, epoch: 5, iters: 100480, time: 0.008) nll: 3907.286133 \n",
      "(GPU: 0, epoch: 5, iters: 101280, time: 0.008) nll: 3020.087891 \n",
      "(GPU: 0, epoch: 5, iters: 102080, time: 0.008) nll: 3479.666016 \n",
      "(GPU: 0, epoch: 5, iters: 102880, time: 0.008) nll: 4175.166016 \n",
      "(GPU: 0, epoch: 5, iters: 103680, time: 0.008) nll: 3537.628418 \n",
      "(GPU: 0, epoch: 5, iters: 104480, time: 0.007) nll: 4371.053223 \n",
      "(GPU: 0, epoch: 5, iters: 105280, time: 0.008) nll: 5133.657715 \n",
      "(GPU: 0, epoch: 5, iters: 106080, time: 0.007) nll: 4000.595947 \n",
      "(GPU: 0, epoch: 5, iters: 106880, time: 0.008) nll: 4062.053223 \n",
      "(GPU: 0, epoch: 5, iters: 107680, time: 0.008) nll: 4660.518555 \n",
      "(GPU: 0, epoch: 5, iters: 108480, time: 0.008) nll: 3668.870117 \n",
      "(GPU: 0, epoch: 5, iters: 109280, time: 0.008) nll: 4643.919922 \n",
      "(GPU: 0, epoch: 5, iters: 110080, time: 0.008) nll: 6364.918945 \n",
      "(GPU: 0, epoch: 5, iters: 110880, time: 0.007) nll: 4133.762695 \n",
      "(GPU: 0, epoch: 5, iters: 111680, time: 0.008) nll: 3288.273682 \n",
      "(GPU: 0, epoch: 5, iters: 112480, time: 0.008) nll: 5578.023438 \n",
      "(GPU: 0, epoch: 5, iters: 113280, time: 0.008) nll: 4530.001953 \n",
      "(GPU: 0, epoch: 5, iters: 114080, time: 0.008) nll: 4275.449219 \n",
      "(GPU: 0, epoch: 5, iters: 114880, time: 0.008) nll: 6505.525391 \n",
      "(GPU: 0, epoch: 5, iters: 115680, time: 0.008) nll: 3013.099121 \n",
      "(GPU: 0, epoch: 5, iters: 116480, time: 0.008) nll: 3261.094238 \n",
      "saving the latest model (epoch 5, total_steps 820000)\n",
      "(GPU: 0, epoch: 5, iters: 117280, time: 0.007) nll: 2710.500488 \n",
      "(GPU: 0, epoch: 5, iters: 118080, time: 0.008) nll: 4562.004883 \n",
      "(GPU: 0, epoch: 5, iters: 118880, time: 0.007) nll: 4529.351074 \n",
      "(GPU: 0, epoch: 5, iters: 119680, time: 0.008) nll: 3003.773682 \n",
      "(GPU: 0, epoch: 5, iters: 120480, time: 0.008) nll: 3095.584229 \n",
      "(GPU: 0, epoch: 5, iters: 121280, time: 0.008) nll: 4263.313477 \n",
      "(GPU: 0, epoch: 5, iters: 122080, time: 0.008) nll: 3242.024902 \n",
      "(GPU: 0, epoch: 5, iters: 122880, time: 0.008) nll: 3750.187012 \n",
      "(GPU: 0, epoch: 5, iters: 123680, time: 0.008) nll: 4042.742676 \n",
      "(GPU: 0, epoch: 5, iters: 124480, time: 0.008) nll: 2845.721680 \n",
      "(GPU: 0, epoch: 5, iters: 125280, time: 0.008) nll: 5203.092285 \n",
      "(GPU: 0, epoch: 5, iters: 126080, time: 0.008) nll: 3520.293945 \n",
      "(GPU: 0, epoch: 5, iters: 126880, time: 0.008) nll: 3440.223633 \n",
      "(GPU: 0, epoch: 5, iters: 127680, time: 0.008) nll: 4645.161133 \n",
      "(GPU: 0, epoch: 5, iters: 128480, time: 0.007) nll: 4182.837891 \n",
      "(GPU: 0, epoch: 5, iters: 129280, time: 0.008) nll: 4278.065918 \n",
      "(GPU: 0, epoch: 5, iters: 130080, time: 0.008) nll: 2751.718994 \n",
      "(GPU: 0, epoch: 5, iters: 130880, time: 0.008) nll: 3381.213867 \n",
      "(GPU: 0, epoch: 5, iters: 131680, time: 0.008) nll: 4626.185547 \n",
      "(GPU: 0, epoch: 5, iters: 132480, time: 0.008) nll: 3887.871094 \n",
      "(GPU: 0, epoch: 5, iters: 133280, time: 0.008) nll: 3003.065430 \n",
      "(GPU: 0, epoch: 5, iters: 134080, time: 0.008) nll: 4956.683594 \n",
      "(GPU: 0, epoch: 5, iters: 134880, time: 0.008) nll: 3328.449707 \n",
      "(GPU: 0, epoch: 5, iters: 135680, time: 0.008) nll: 3621.562012 \n",
      "(GPU: 0, epoch: 5, iters: 136480, time: 0.008) nll: 3911.269531 \n",
      "saving the latest model (epoch 5, total_steps 840000)\n",
      "(GPU: 0, epoch: 5, iters: 137280, time: 0.008) nll: 3753.381592 \n",
      "(GPU: 0, epoch: 5, iters: 138080, time: 0.008) nll: 3024.666748 \n",
      "(GPU: 0, epoch: 5, iters: 138880, time: 0.008) nll: 4888.673828 \n",
      "(GPU: 0, epoch: 5, iters: 139680, time: 0.007) nll: 3893.734863 \n",
      "(GPU: 0, epoch: 5, iters: 140480, time: 0.008) nll: 4220.999023 \n",
      "[*] End of epoch 5 / 25 \t Time Taken: 1231 sec \n",
      "/cluster/54/streakfull/ADL4CV/Project/src/plz-autosdf/logs/bert2vqsc-text2shape-seq-LR1e-4-new-bert-2-prev_z_shape_input-try-2\n",
      "[*] learning rate = 0.0000600\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▊   | 3017/4397 [14:06<05:54,  3.89it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 6, iters: 32, time: 0.004) nll: 4761.575195 \n",
      "(GPU: 0, epoch: 6, iters: 32, time: 0.004) nll: 3572.397461 \n",
      "(GPU: 0, epoch: 6, iters: 576, time: 0.008) nll: 3614.758789 \n",
      "(GPU: 0, epoch: 6, iters: 1376, time: 0.008) nll: 2863.960449 \n",
      "(GPU: 0, epoch: 6, iters: 2176, time: 0.008) nll: 5497.981445 \n",
      "(GPU: 0, epoch: 6, iters: 2976, time: 0.008) nll: 4528.825195 \n",
      "(GPU: 0, epoch: 6, iters: 3776, time: 0.008) nll: 3136.143066 \n",
      "(GPU: 0, epoch: 6, iters: 4576, time: 0.007) nll: 3832.957520 \n",
      "(GPU: 0, epoch: 6, iters: 5376, time: 0.008) nll: 3794.967529 \n",
      "(GPU: 0, epoch: 6, iters: 6176, time: 0.008) nll: 4032.549805 \n",
      "(GPU: 0, epoch: 6, iters: 6976, time: 0.008) nll: 4048.242188 \n",
      "(GPU: 0, epoch: 6, iters: 7776, time: 0.008) nll: 3787.084473 \n",
      "(GPU: 0, epoch: 6, iters: 8576, time: 0.008) nll: 2629.278564 \n",
      "(GPU: 0, epoch: 6, iters: 9376, time: 0.008) nll: 3731.342529 \n",
      "(GPU: 0, epoch: 6, iters: 10176, time: 0.008) nll: 3893.244141 \n",
      "(GPU: 0, epoch: 6, iters: 10976, time: 0.008) nll: 4419.002930 \n",
      "(GPU: 0, epoch: 6, iters: 11776, time: 0.008) nll: 4069.610840 \n",
      "(GPU: 0, epoch: 6, iters: 12576, time: 0.008) nll: 3993.627197 \n",
      "(GPU: 0, epoch: 6, iters: 13376, time: 0.008) nll: 4623.057617 \n",
      "(GPU: 0, epoch: 6, iters: 14176, time: 0.008) nll: 3893.415527 \n",
      "(GPU: 0, epoch: 6, iters: 14976, time: 0.008) nll: 4101.188477 \n",
      "(GPU: 0, epoch: 6, iters: 15776, time: 0.008) nll: 3598.279053 \n",
      "saving the latest model (epoch 6, total_steps 860000)\n",
      "(GPU: 0, epoch: 6, iters: 16576, time: 0.008) nll: 5576.186523 \n",
      "(GPU: 0, epoch: 6, iters: 17376, time: 0.007) nll: 4125.051758 \n",
      "(GPU: 0, epoch: 6, iters: 18176, time: 0.008) nll: 3857.950928 \n",
      "(GPU: 0, epoch: 6, iters: 18976, time: 0.008) nll: 3953.879395 \n",
      "(GPU: 0, epoch: 6, iters: 19776, time: 0.008) nll: 4726.251953 \n",
      "(GPU: 0, epoch: 6, iters: 19776, time: 0.013) nll: 4635.616211 \n",
      "(GPU: 0, epoch: 6, iters: 19776, time: 0.013) nll: 4596.665527 \n",
      "(GPU: 0, epoch: 6, iters: 20576, time: 0.008) nll: 3381.205078 \n",
      "(GPU: 0, epoch: 6, iters: 21376, time: 0.008) nll: 3933.886719 \n",
      "(GPU: 0, epoch: 6, iters: 22176, time: 0.008) nll: 3495.568115 \n",
      "(GPU: 0, epoch: 6, iters: 22976, time: 0.008) nll: 4375.045410 \n",
      "(GPU: 0, epoch: 6, iters: 23776, time: 0.008) nll: 5235.339844 \n",
      "(GPU: 0, epoch: 6, iters: 24576, time: 0.008) nll: 3246.346924 \n",
      "(GPU: 0, epoch: 6, iters: 25376, time: 0.008) nll: 5766.072266 \n",
      "(GPU: 0, epoch: 6, iters: 26176, time: 0.008) nll: 3152.772217 \n",
      "(GPU: 0, epoch: 6, iters: 26976, time: 0.008) nll: 4971.804688 \n",
      "(GPU: 0, epoch: 6, iters: 27776, time: 0.008) nll: 3362.354736 \n",
      "(GPU: 0, epoch: 6, iters: 28576, time: 0.008) nll: 4910.559570 \n",
      "(GPU: 0, epoch: 6, iters: 29376, time: 0.008) nll: 3829.398682 \n",
      "(GPU: 0, epoch: 6, iters: 30176, time: 0.008) nll: 4712.651367 \n",
      "(GPU: 0, epoch: 6, iters: 30976, time: 0.008) nll: 4325.528320 \n",
      "(GPU: 0, epoch: 6, iters: 31776, time: 0.008) nll: 4182.658691 \n",
      "(GPU: 0, epoch: 6, iters: 32576, time: 0.008) nll: 4242.011719 \n",
      "(GPU: 0, epoch: 6, iters: 33376, time: 0.008) nll: 4485.550781 \n",
      "(GPU: 0, epoch: 6, iters: 34176, time: 0.008) nll: 4320.018066 \n",
      "(GPU: 0, epoch: 6, iters: 34976, time: 0.008) nll: 3820.475830 \n",
      "(GPU: 0, epoch: 6, iters: 35776, time: 0.008) nll: 3755.217773 \n",
      "saving the latest model (epoch 6, total_steps 880000)\n",
      "(GPU: 0, epoch: 6, iters: 36576, time: 0.008) nll: 4820.356445 \n",
      "(GPU: 0, epoch: 6, iters: 37376, time: 0.008) nll: 3532.223145 \n",
      "(GPU: 0, epoch: 6, iters: 38176, time: 0.008) nll: 2759.259766 \n",
      "(GPU: 0, epoch: 6, iters: 38976, time: 0.008) nll: 4226.945312 \n",
      "(GPU: 0, epoch: 6, iters: 39776, time: 0.008) nll: 3942.650879 \n",
      "(GPU: 0, epoch: 6, iters: 40576, time: 0.008) nll: 3102.765625 \n",
      "(GPU: 0, epoch: 6, iters: 41376, time: 0.008) nll: 4083.033203 \n",
      "(GPU: 0, epoch: 6, iters: 42176, time: 0.008) nll: 4513.839355 \n",
      "(GPU: 0, epoch: 6, iters: 42976, time: 0.008) nll: 4433.356445 \n",
      "(GPU: 0, epoch: 6, iters: 43776, time: 0.008) nll: 3474.589600 \n",
      "(GPU: 0, epoch: 6, iters: 44576, time: 0.008) nll: 4003.663086 \n",
      "(GPU: 0, epoch: 6, iters: 45376, time: 0.008) nll: 2921.942383 \n",
      "(GPU: 0, epoch: 6, iters: 46176, time: 0.008) nll: 4277.613770 \n",
      "(GPU: 0, epoch: 6, iters: 46976, time: 0.008) nll: 5085.581055 \n",
      "(GPU: 0, epoch: 6, iters: 47776, time: 0.008) nll: 4424.844727 \n",
      "(GPU: 0, epoch: 6, iters: 48576, time: 0.008) nll: 3135.818359 \n",
      "(GPU: 0, epoch: 6, iters: 49376, time: 0.007) nll: 4531.473145 \n",
      "(GPU: 0, epoch: 6, iters: 50176, time: 0.008) nll: 3837.234131 \n",
      "(GPU: 0, epoch: 6, iters: 50976, time: 0.008) nll: 3509.960938 \n",
      "(GPU: 0, epoch: 6, iters: 51776, time: 0.008) nll: 4448.436523 \n",
      "(GPU: 0, epoch: 6, iters: 52576, time: 0.008) nll: 3936.574951 \n",
      "(GPU: 0, epoch: 6, iters: 53376, time: 0.008) nll: 3855.279541 \n",
      "(GPU: 0, epoch: 6, iters: 54176, time: 0.007) nll: 4916.325195 \n",
      "(GPU: 0, epoch: 6, iters: 54976, time: 0.008) nll: 4362.359375 \n",
      "(GPU: 0, epoch: 6, iters: 55776, time: 0.008) nll: 4117.933594 \n",
      "saving the latest model (epoch 6, total_steps 900000)\n",
      "(GPU: 0, epoch: 6, iters: 56576, time: 0.008) nll: 6113.870117 \n",
      "(GPU: 0, epoch: 6, iters: 57376, time: 0.007) nll: 4606.963867 \n",
      "(GPU: 0, epoch: 6, iters: 58176, time: 0.008) nll: 4160.058594 \n",
      "(GPU: 0, epoch: 6, iters: 58976, time: 0.007) nll: 2417.523438 \n",
      "(GPU: 0, epoch: 6, iters: 59776, time: 0.008) nll: 4146.299805 \n",
      "(GPU: 0, epoch: 6, iters: 60576, time: 0.007) nll: 5124.323242 \n",
      "(GPU: 0, epoch: 6, iters: 61376, time: 0.008) nll: 4075.433105 \n",
      "(GPU: 0, epoch: 6, iters: 62176, time: 0.007) nll: 4425.000977 \n",
      "(GPU: 0, epoch: 6, iters: 62976, time: 0.008) nll: 3015.112793 \n",
      "(GPU: 0, epoch: 6, iters: 63776, time: 0.008) nll: 4431.889648 \n",
      "(GPU: 0, epoch: 6, iters: 64576, time: 0.008) nll: 3853.839600 \n",
      "(GPU: 0, epoch: 6, iters: 65376, time: 0.008) nll: 4023.250488 \n",
      "(GPU: 0, epoch: 6, iters: 66176, time: 0.008) nll: 4573.755859 \n",
      "(GPU: 0, epoch: 6, iters: 66976, time: 0.008) nll: 3880.342773 \n",
      "(GPU: 0, epoch: 6, iters: 67776, time: 0.008) nll: 4499.205078 \n",
      "(GPU: 0, epoch: 6, iters: 68576, time: 0.008) nll: 2672.580566 \n",
      "(GPU: 0, epoch: 6, iters: 69376, time: 0.008) nll: 4844.390137 \n",
      "(GPU: 0, epoch: 6, iters: 70176, time: 0.008) nll: 4529.837891 \n",
      "(GPU: 0, epoch: 6, iters: 70976, time: 0.008) nll: 3964.332275 \n",
      "(GPU: 0, epoch: 6, iters: 71776, time: 0.008) nll: 3183.827637 \n",
      "(GPU: 0, epoch: 6, iters: 72576, time: 0.008) nll: 3102.407715 \n",
      "(GPU: 0, epoch: 6, iters: 73376, time: 0.008) nll: 2424.903320 \n",
      "(GPU: 0, epoch: 6, iters: 74176, time: 0.008) nll: 3502.493164 \n",
      "(GPU: 0, epoch: 6, iters: 74976, time: 0.007) nll: 4896.621094 \n",
      "(GPU: 0, epoch: 6, iters: 75776, time: 0.008) nll: 4824.614258 \n",
      "saving the latest model (epoch 6, total_steps 920000)\n",
      "(GPU: 0, epoch: 6, iters: 76576, time: 0.008) nll: 4377.987305 \n",
      "(GPU: 0, epoch: 6, iters: 77376, time: 0.008) nll: 4824.337891 \n",
      "(GPU: 0, epoch: 6, iters: 78176, time: 0.007) nll: 2851.066406 \n",
      "(GPU: 0, epoch: 6, iters: 78976, time: 0.008) nll: 3675.982422 \n",
      "(GPU: 0, epoch: 6, iters: 79776, time: 0.008) nll: 4953.401367 \n",
      "(GPU: 0, epoch: 6, iters: 80576, time: 0.008) nll: 3395.611572 \n",
      "(GPU: 0, epoch: 6, iters: 81376, time: 0.008) nll: 3244.439697 \n",
      "(GPU: 0, epoch: 6, iters: 82176, time: 0.008) nll: 5676.562500 \n",
      "(GPU: 0, epoch: 6, iters: 82976, time: 0.008) nll: 3645.813232 \n",
      "(GPU: 0, epoch: 6, iters: 83776, time: 0.008) nll: 3204.788086 \n",
      "(GPU: 0, epoch: 6, iters: 84576, time: 0.008) nll: 4740.879395 \n",
      "(GPU: 0, epoch: 6, iters: 85376, time: 0.008) nll: 4352.389160 \n",
      "(GPU: 0, epoch: 6, iters: 86176, time: 0.007) nll: 3332.027344 \n",
      "(GPU: 0, epoch: 6, iters: 86976, time: 0.008) nll: 3076.140381 \n",
      "(GPU: 0, epoch: 6, iters: 87776, time: 0.008) nll: 2953.527344 \n",
      "(GPU: 0, epoch: 6, iters: 88576, time: 0.008) nll: 4756.546875 \n",
      "(GPU: 0, epoch: 6, iters: 89376, time: 0.008) nll: 3728.218262 \n",
      "(GPU: 0, epoch: 6, iters: 90176, time: 0.008) nll: 4070.595215 \n",
      "(GPU: 0, epoch: 6, iters: 90976, time: 0.008) nll: 2739.598633 \n",
      "(GPU: 0, epoch: 6, iters: 91776, time: 0.008) nll: 4771.172852 \n",
      "(GPU: 0, epoch: 6, iters: 92576, time: 0.008) nll: 4314.996094 \n",
      "(GPU: 0, epoch: 6, iters: 93376, time: 0.008) nll: 2945.539062 \n",
      "(GPU: 0, epoch: 6, iters: 94176, time: 0.008) nll: 3539.349121 \n",
      "(GPU: 0, epoch: 6, iters: 94976, time: 0.008) nll: 3959.223145 \n",
      "(GPU: 0, epoch: 6, iters: 95776, time: 0.007) nll: 4640.678711 \n",
      "saving the latest model (epoch 6, total_steps 940000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4397/4397 [20:30<00:00,  3.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 6, iters: 96576, time: 0.008) nll: 5035.159180 \n",
      "(GPU: 0, epoch: 6, iters: 97376, time: 0.007) nll: 3920.773193 \n",
      "(GPU: 0, epoch: 6, iters: 98176, time: 0.008) nll: 4965.658203 \n",
      "(GPU: 0, epoch: 6, iters: 98976, time: 0.008) nll: 3310.492432 \n",
      "(GPU: 0, epoch: 6, iters: 99776, time: 0.008) nll: 4372.854492 \n",
      "(GPU: 0, epoch: 6, iters: 100576, time: 0.007) nll: 3627.800781 \n",
      "(GPU: 0, epoch: 6, iters: 101376, time: 0.008) nll: 3579.057373 \n",
      "(GPU: 0, epoch: 6, iters: 102176, time: 0.008) nll: 3908.722168 \n",
      "(GPU: 0, epoch: 6, iters: 102976, time: 0.008) nll: 4606.631836 \n",
      "(GPU: 0, epoch: 6, iters: 103776, time: 0.008) nll: 3674.708496 \n",
      "(GPU: 0, epoch: 6, iters: 104576, time: 0.008) nll: 4582.008301 \n",
      "(GPU: 0, epoch: 6, iters: 105376, time: 0.008) nll: 4039.712402 \n",
      "(GPU: 0, epoch: 6, iters: 106176, time: 0.008) nll: 4837.333008 \n",
      "(GPU: 0, epoch: 6, iters: 106976, time: 0.008) nll: 3921.738770 \n",
      "(GPU: 0, epoch: 6, iters: 107776, time: 0.008) nll: 4153.740234 \n",
      "(GPU: 0, epoch: 6, iters: 108576, time: 0.008) nll: 4291.208008 \n",
      "(GPU: 0, epoch: 6, iters: 109376, time: 0.008) nll: 4523.998047 \n",
      "(GPU: 0, epoch: 6, iters: 110176, time: 0.007) nll: 4502.019531 \n",
      "(GPU: 0, epoch: 6, iters: 110976, time: 0.008) nll: 3500.673828 \n",
      "(GPU: 0, epoch: 6, iters: 111776, time: 0.008) nll: 3238.488770 \n",
      "(GPU: 0, epoch: 6, iters: 112576, time: 0.008) nll: 4070.801270 \n",
      "(GPU: 0, epoch: 6, iters: 113376, time: 0.008) nll: 3693.274902 \n",
      "(GPU: 0, epoch: 6, iters: 114176, time: 0.008) nll: 4442.888184 \n",
      "(GPU: 0, epoch: 6, iters: 114976, time: 0.008) nll: 5356.292969 \n",
      "(GPU: 0, epoch: 6, iters: 115776, time: 0.008) nll: 5586.057617 \n",
      "(GPU: 0, epoch: 6, iters: 115776, time: 0.013) nll: 5536.563477 \n",
      "(GPU: 0, epoch: 6, iters: 115776, time: 0.013) nll: 3749.770996 \n",
      "saving the latest model (epoch 6, total_steps 960000)\n",
      "(GPU: 0, epoch: 6, iters: 116576, time: 0.008) nll: 5214.557617 \n",
      "(GPU: 0, epoch: 6, iters: 117376, time: 0.008) nll: 5744.999512 \n",
      "(GPU: 0, epoch: 6, iters: 118176, time: 0.008) nll: 3374.280273 \n",
      "(GPU: 0, epoch: 6, iters: 118976, time: 0.008) nll: 3392.880859 \n",
      "(GPU: 0, epoch: 6, iters: 119776, time: 0.007) nll: 3362.818359 \n",
      "(GPU: 0, epoch: 6, iters: 120576, time: 0.008) nll: 4334.235352 \n",
      "(GPU: 0, epoch: 6, iters: 121376, time: 0.008) nll: 2992.153564 \n",
      "(GPU: 0, epoch: 6, iters: 122176, time: 0.008) nll: 3206.809082 \n",
      "(GPU: 0, epoch: 6, iters: 122976, time: 0.007) nll: 4043.391846 \n",
      "(GPU: 0, epoch: 6, iters: 123776, time: 0.008) nll: 3667.603027 \n",
      "(GPU: 0, epoch: 6, iters: 124576, time: 0.008) nll: 5760.685059 \n",
      "(GPU: 0, epoch: 6, iters: 125376, time: 0.008) nll: 5033.198242 \n",
      "(GPU: 0, epoch: 6, iters: 126176, time: 0.008) nll: 5102.891602 \n",
      "(GPU: 0, epoch: 6, iters: 126976, time: 0.008) nll: 3915.099121 \n",
      "(GPU: 0, epoch: 6, iters: 127776, time: 0.008) nll: 2707.853027 \n",
      "(GPU: 0, epoch: 6, iters: 128576, time: 0.008) nll: 4328.737793 \n",
      "(GPU: 0, epoch: 6, iters: 129376, time: 0.007) nll: 3019.974121 \n",
      "(GPU: 0, epoch: 6, iters: 130176, time: 0.008) nll: 4670.264648 \n",
      "(GPU: 0, epoch: 6, iters: 130976, time: 0.008) nll: 3921.711182 \n",
      "(GPU: 0, epoch: 6, iters: 131776, time: 0.008) nll: 4326.039062 \n",
      "(GPU: 0, epoch: 6, iters: 132576, time: 0.007) nll: 4280.221680 \n",
      "(GPU: 0, epoch: 6, iters: 133376, time: 0.008) nll: 4666.420410 \n",
      "(GPU: 0, epoch: 6, iters: 134176, time: 0.008) nll: 3401.640137 \n",
      "(GPU: 0, epoch: 6, iters: 134976, time: 0.008) nll: 3177.061279 \n",
      "(GPU: 0, epoch: 6, iters: 135776, time: 0.008) nll: 4740.537109 \n",
      "saving the latest model (epoch 6, total_steps 980000)\n",
      "(GPU: 0, epoch: 6, iters: 136576, time: 0.008) nll: 3492.905518 \n",
      "(GPU: 0, epoch: 6, iters: 137376, time: 0.007) nll: 3761.892334 \n",
      "(GPU: 0, epoch: 6, iters: 138176, time: 0.008) nll: 3040.397217 \n",
      "(GPU: 0, epoch: 6, iters: 138976, time: 0.008) nll: 2620.816895 \n",
      "(GPU: 0, epoch: 6, iters: 139776, time: 0.008) nll: 4126.301758 \n",
      "(GPU: 0, epoch: 6, iters: 140576, time: 0.007) nll: 3675.167969 \n",
      "saving the model at the end of epoch 6, iters 984928\n",
      "([test] GPU: 0, epoch: 6) \n",
      "OrderedDict()\n",
      "[*] End of epoch 6 / 25 \t Time Taken: 1257 sec \n",
      "/cluster/54/streakfull/ADL4CV/Project/src/plz-autosdf/logs/bert2vqsc-text2shape-seq-LR1e-4-new-bert-2-prev_z_shape_input-try-2\n",
      "[*] learning rate = 0.0000700\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 2995/4397 [13:59<06:01,  3.88it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 7, iters: 32, time: 0.004) nll: 5149.807617 \n",
      "(GPU: 0, epoch: 7, iters: 32, time: 0.004) nll: 4419.116699 \n",
      "(GPU: 0, epoch: 7, iters: 672, time: 0.008) nll: 3951.502930 \n",
      "(GPU: 0, epoch: 7, iters: 1472, time: 0.008) nll: 3238.427246 \n",
      "(GPU: 0, epoch: 7, iters: 2272, time: 0.008) nll: 3439.863281 \n",
      "(GPU: 0, epoch: 7, iters: 3072, time: 0.008) nll: 3789.034180 \n",
      "(GPU: 0, epoch: 7, iters: 3872, time: 0.008) nll: 3252.322510 \n",
      "(GPU: 0, epoch: 7, iters: 4672, time: 0.008) nll: 3687.373779 \n",
      "(GPU: 0, epoch: 7, iters: 5472, time: 0.008) nll: 4078.539062 \n",
      "(GPU: 0, epoch: 7, iters: 6272, time: 0.008) nll: 4000.853516 \n",
      "(GPU: 0, epoch: 7, iters: 7072, time: 0.008) nll: 4223.665527 \n",
      "(GPU: 0, epoch: 7, iters: 7872, time: 0.008) nll: 4828.204590 \n",
      "(GPU: 0, epoch: 7, iters: 8672, time: 0.008) nll: 3392.113770 \n",
      "(GPU: 0, epoch: 7, iters: 9472, time: 0.008) nll: 4408.166992 \n",
      "(GPU: 0, epoch: 7, iters: 10272, time: 0.008) nll: 5205.190430 \n",
      "(GPU: 0, epoch: 7, iters: 11072, time: 0.008) nll: 3688.272949 \n",
      "(GPU: 0, epoch: 7, iters: 11872, time: 0.008) nll: 3184.486328 \n",
      "(GPU: 0, epoch: 7, iters: 12672, time: 0.008) nll: 4313.924805 \n",
      "(GPU: 0, epoch: 7, iters: 13472, time: 0.008) nll: 3581.719238 \n",
      "(GPU: 0, epoch: 7, iters: 14272, time: 0.008) nll: 2842.842773 \n",
      "(GPU: 0, epoch: 7, iters: 15072, time: 0.008) nll: 4654.347656 \n",
      "saving the latest model (epoch 7, total_steps 1000000)\n",
      "(GPU: 0, epoch: 7, iters: 15872, time: 0.008) nll: 3568.861328 \n",
      "(GPU: 0, epoch: 7, iters: 16672, time: 0.007) nll: 3417.378906 \n",
      "(GPU: 0, epoch: 7, iters: 17472, time: 0.008) nll: 4156.145020 \n",
      "(GPU: 0, epoch: 7, iters: 18272, time: 0.008) nll: 4204.306641 \n",
      "(GPU: 0, epoch: 7, iters: 19072, time: 0.008) nll: 4343.984863 \n",
      "(GPU: 0, epoch: 7, iters: 19872, time: 0.008) nll: 4063.525635 \n",
      "(GPU: 0, epoch: 7, iters: 20672, time: 0.008) nll: 5804.741211 \n",
      "(GPU: 0, epoch: 7, iters: 21472, time: 0.008) nll: 3708.555176 \n",
      "(GPU: 0, epoch: 7, iters: 22272, time: 0.008) nll: 4029.350586 \n",
      "(GPU: 0, epoch: 7, iters: 23072, time: 0.008) nll: 4269.964844 \n",
      "(GPU: 0, epoch: 7, iters: 23872, time: 0.008) nll: 4469.125488 \n",
      "(GPU: 0, epoch: 7, iters: 24672, time: 0.008) nll: 4281.632812 \n",
      "(GPU: 0, epoch: 7, iters: 25472, time: 0.008) nll: 4607.769043 \n",
      "(GPU: 0, epoch: 7, iters: 26272, time: 0.008) nll: 4529.063477 \n",
      "(GPU: 0, epoch: 7, iters: 27072, time: 0.008) nll: 5324.498047 \n",
      "(GPU: 0, epoch: 7, iters: 27872, time: 0.008) nll: 3538.310059 \n",
      "(GPU: 0, epoch: 7, iters: 28672, time: 0.008) nll: 3007.020996 \n",
      "(GPU: 0, epoch: 7, iters: 29472, time: 0.008) nll: 3542.510254 \n",
      "(GPU: 0, epoch: 7, iters: 30272, time: 0.008) nll: 4565.398438 \n",
      "(GPU: 0, epoch: 7, iters: 31072, time: 0.007) nll: 4611.553711 \n",
      "(GPU: 0, epoch: 7, iters: 31872, time: 0.008) nll: 4974.883789 \n",
      "(GPU: 0, epoch: 7, iters: 32672, time: 0.008) nll: 3760.689697 \n",
      "(GPU: 0, epoch: 7, iters: 33472, time: 0.008) nll: 4775.859375 \n",
      "(GPU: 0, epoch: 7, iters: 34272, time: 0.007) nll: 4139.864258 \n",
      "(GPU: 0, epoch: 7, iters: 35072, time: 0.008) nll: 5250.718750 \n",
      "saving the latest model (epoch 7, total_steps 1020000)\n",
      "(GPU: 0, epoch: 7, iters: 35872, time: 0.008) nll: 3364.843018 \n",
      "(GPU: 0, epoch: 7, iters: 36672, time: 0.008) nll: 5075.020020 \n",
      "(GPU: 0, epoch: 7, iters: 37472, time: 0.008) nll: 4525.418945 \n",
      "(GPU: 0, epoch: 7, iters: 38272, time: 0.008) nll: 4211.390137 \n",
      "(GPU: 0, epoch: 7, iters: 39072, time: 0.008) nll: 4222.142578 \n",
      "(GPU: 0, epoch: 7, iters: 39872, time: 0.008) nll: 3503.533203 \n",
      "(GPU: 0, epoch: 7, iters: 40672, time: 0.008) nll: 3025.103027 \n",
      "(GPU: 0, epoch: 7, iters: 41472, time: 0.008) nll: 4073.500488 \n",
      "(GPU: 0, epoch: 7, iters: 42272, time: 0.008) nll: 5321.749023 \n",
      "(GPU: 0, epoch: 7, iters: 43072, time: 0.008) nll: 4083.761475 \n",
      "(GPU: 0, epoch: 7, iters: 43872, time: 0.008) nll: 3989.168701 \n",
      "(GPU: 0, epoch: 7, iters: 44672, time: 0.008) nll: 3614.175293 \n",
      "(GPU: 0, epoch: 7, iters: 45472, time: 0.008) nll: 5588.795898 \n",
      "(GPU: 0, epoch: 7, iters: 46272, time: 0.008) nll: 4084.539062 \n",
      "(GPU: 0, epoch: 7, iters: 47072, time: 0.008) nll: 4775.136230 \n",
      "(GPU: 0, epoch: 7, iters: 47872, time: 0.008) nll: 4767.567383 \n",
      "(GPU: 0, epoch: 7, iters: 48672, time: 0.008) nll: 4277.866699 \n",
      "(GPU: 0, epoch: 7, iters: 49472, time: 0.008) nll: 4441.331543 \n",
      "(GPU: 0, epoch: 7, iters: 50272, time: 0.008) nll: 2881.525391 \n",
      "(GPU: 0, epoch: 7, iters: 51072, time: 0.008) nll: 5776.359863 \n",
      "(GPU: 0, epoch: 7, iters: 51872, time: 0.008) nll: 2796.586182 \n",
      "(GPU: 0, epoch: 7, iters: 52672, time: 0.008) nll: 4088.872070 \n",
      "(GPU: 0, epoch: 7, iters: 53472, time: 0.008) nll: 4063.363770 \n",
      "(GPU: 0, epoch: 7, iters: 54272, time: 0.008) nll: 4490.629395 \n",
      "(GPU: 0, epoch: 7, iters: 55072, time: 0.008) nll: 3870.196533 \n",
      "saving the latest model (epoch 7, total_steps 1040000)\n",
      "(GPU: 0, epoch: 7, iters: 55872, time: 0.008) nll: 4862.910156 \n",
      "(GPU: 0, epoch: 7, iters: 56672, time: 0.008) nll: 2753.394775 \n",
      "(GPU: 0, epoch: 7, iters: 57472, time: 0.008) nll: 4657.318848 \n",
      "(GPU: 0, epoch: 7, iters: 58272, time: 0.008) nll: 3505.759521 \n",
      "(GPU: 0, epoch: 7, iters: 59072, time: 0.008) nll: 3454.699219 \n",
      "(GPU: 0, epoch: 7, iters: 59872, time: 0.008) nll: 3484.416504 \n",
      "(GPU: 0, epoch: 7, iters: 60672, time: 0.008) nll: 5953.384766 \n",
      "(GPU: 0, epoch: 7, iters: 61472, time: 0.008) nll: 5480.995117 \n",
      "(GPU: 0, epoch: 7, iters: 62272, time: 0.008) nll: 3705.079346 \n",
      "(GPU: 0, epoch: 7, iters: 63072, time: 0.008) nll: 3920.271973 \n",
      "(GPU: 0, epoch: 7, iters: 63872, time: 0.008) nll: 3509.361328 \n",
      "(GPU: 0, epoch: 7, iters: 64672, time: 0.008) nll: 2929.370605 \n",
      "(GPU: 0, epoch: 7, iters: 65472, time: 0.008) nll: 2345.646484 \n",
      "(GPU: 0, epoch: 7, iters: 66272, time: 0.008) nll: 4548.323242 \n",
      "(GPU: 0, epoch: 7, iters: 67072, time: 0.008) nll: 5849.823730 \n",
      "(GPU: 0, epoch: 7, iters: 67872, time: 0.008) nll: 4998.912109 \n",
      "(GPU: 0, epoch: 7, iters: 68672, time: 0.008) nll: 4411.249023 \n",
      "(GPU: 0, epoch: 7, iters: 69472, time: 0.008) nll: 4018.614746 \n",
      "(GPU: 0, epoch: 7, iters: 70272, time: 0.008) nll: 3218.144531 \n",
      "(GPU: 0, epoch: 7, iters: 71072, time: 0.008) nll: 3462.537354 \n",
      "(GPU: 0, epoch: 7, iters: 71072, time: 0.013) nll: 3405.207520 \n",
      "(GPU: 0, epoch: 7, iters: 71072, time: 0.013) nll: 4830.882812 \n",
      "(GPU: 0, epoch: 7, iters: 71872, time: 0.008) nll: 3338.720459 \n",
      "(GPU: 0, epoch: 7, iters: 72672, time: 0.008) nll: 4030.950439 \n",
      "(GPU: 0, epoch: 7, iters: 73472, time: 0.008) nll: 4141.138672 \n",
      "(GPU: 0, epoch: 7, iters: 74272, time: 0.008) nll: 4218.926758 \n",
      "(GPU: 0, epoch: 7, iters: 75072, time: 0.008) nll: 4015.776367 \n",
      "saving the latest model (epoch 7, total_steps 1060000)\n",
      "(GPU: 0, epoch: 7, iters: 75872, time: 0.007) nll: 4922.791992 \n",
      "(GPU: 0, epoch: 7, iters: 76672, time: 0.008) nll: 3357.513184 \n",
      "(GPU: 0, epoch: 7, iters: 77472, time: 0.008) nll: 4269.867188 \n",
      "(GPU: 0, epoch: 7, iters: 78272, time: 0.008) nll: 3376.795410 \n",
      "(GPU: 0, epoch: 7, iters: 79072, time: 0.008) nll: 3689.058350 \n",
      "(GPU: 0, epoch: 7, iters: 79872, time: 0.008) nll: 4214.538086 \n",
      "(GPU: 0, epoch: 7, iters: 80672, time: 0.008) nll: 6479.624512 \n",
      "(GPU: 0, epoch: 7, iters: 81472, time: 0.008) nll: 4682.013672 \n",
      "(GPU: 0, epoch: 7, iters: 82272, time: 0.008) nll: 4610.477539 \n",
      "(GPU: 0, epoch: 7, iters: 83072, time: 0.008) nll: 3366.133789 \n",
      "(GPU: 0, epoch: 7, iters: 83872, time: 0.007) nll: 4553.243164 \n",
      "(GPU: 0, epoch: 7, iters: 84672, time: 0.008) nll: 4035.269775 \n",
      "(GPU: 0, epoch: 7, iters: 85472, time: 0.008) nll: 4192.268066 \n",
      "(GPU: 0, epoch: 7, iters: 86272, time: 0.008) nll: 3887.303711 \n",
      "(GPU: 0, epoch: 7, iters: 87072, time: 0.008) nll: 5816.612305 \n",
      "(GPU: 0, epoch: 7, iters: 87872, time: 0.008) nll: 4726.339355 \n",
      "(GPU: 0, epoch: 7, iters: 88672, time: 0.008) nll: 5476.798340 \n",
      "(GPU: 0, epoch: 7, iters: 89472, time: 0.008) nll: 2763.913086 \n",
      "(GPU: 0, epoch: 7, iters: 90272, time: 0.008) nll: 3158.922363 \n",
      "(GPU: 0, epoch: 7, iters: 91072, time: 0.008) nll: 4374.757324 \n",
      "(GPU: 0, epoch: 7, iters: 91872, time: 0.008) nll: 4527.140137 \n",
      "(GPU: 0, epoch: 7, iters: 92672, time: 0.008) nll: 4235.734375 \n",
      "(GPU: 0, epoch: 7, iters: 93472, time: 0.008) nll: 3821.252441 \n",
      "(GPU: 0, epoch: 7, iters: 94272, time: 0.008) nll: 3342.010742 \n",
      "(GPU: 0, epoch: 7, iters: 95072, time: 0.008) nll: 4082.293457 \n",
      "saving the latest model (epoch 7, total_steps 1080000)\n",
      "(GPU: 0, epoch: 7, iters: 95872, time: 0.008) nll: 4061.727539 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4397/4397 [20:28<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 7, iters: 96672, time: 0.008) nll: 3570.185547 \n",
      "(GPU: 0, epoch: 7, iters: 97472, time: 0.008) nll: 5065.677734 \n",
      "(GPU: 0, epoch: 7, iters: 98272, time: 0.008) nll: 4801.262695 \n",
      "(GPU: 0, epoch: 7, iters: 99072, time: 0.008) nll: 4998.192383 \n",
      "(GPU: 0, epoch: 7, iters: 99872, time: 0.008) nll: 3041.575195 \n",
      "(GPU: 0, epoch: 7, iters: 100672, time: 0.008) nll: 4260.748047 \n",
      "(GPU: 0, epoch: 7, iters: 101472, time: 0.008) nll: 2759.338623 \n",
      "(GPU: 0, epoch: 7, iters: 102272, time: 0.008) nll: 5015.624023 \n",
      "(GPU: 0, epoch: 7, iters: 103072, time: 0.008) nll: 4521.612305 \n",
      "(GPU: 0, epoch: 7, iters: 103872, time: 0.008) nll: 4176.783691 \n",
      "(GPU: 0, epoch: 7, iters: 104672, time: 0.008) nll: 4800.510742 \n",
      "(GPU: 0, epoch: 7, iters: 105472, time: 0.008) nll: 2534.766113 \n",
      "(GPU: 0, epoch: 7, iters: 106272, time: 0.008) nll: 5001.468750 \n",
      "(GPU: 0, epoch: 7, iters: 107072, time: 0.008) nll: 3525.779297 \n",
      "(GPU: 0, epoch: 7, iters: 107872, time: 0.007) nll: 3611.602295 \n",
      "(GPU: 0, epoch: 7, iters: 108672, time: 0.008) nll: 6024.519043 \n",
      "(GPU: 0, epoch: 7, iters: 109472, time: 0.008) nll: 3618.364258 \n",
      "(GPU: 0, epoch: 7, iters: 110272, time: 0.008) nll: 4623.625977 \n",
      "(GPU: 0, epoch: 7, iters: 111072, time: 0.008) nll: 3296.274902 \n",
      "(GPU: 0, epoch: 7, iters: 111872, time: 0.008) nll: 3422.067627 \n",
      "(GPU: 0, epoch: 7, iters: 112672, time: 0.008) nll: 3870.853271 \n",
      "(GPU: 0, epoch: 7, iters: 113472, time: 0.008) nll: 5904.958984 \n",
      "(GPU: 0, epoch: 7, iters: 114272, time: 0.008) nll: 4818.898926 \n",
      "(GPU: 0, epoch: 7, iters: 115072, time: 0.008) nll: 2823.305664 \n",
      "saving the latest model (epoch 7, total_steps 1100000)\n",
      "(GPU: 0, epoch: 7, iters: 115872, time: 0.008) nll: 3664.089844 \n",
      "(GPU: 0, epoch: 7, iters: 116672, time: 0.008) nll: 3363.251953 \n",
      "(GPU: 0, epoch: 7, iters: 117472, time: 0.008) nll: 2796.046143 \n",
      "(GPU: 0, epoch: 7, iters: 118272, time: 0.008) nll: 3743.877441 \n",
      "(GPU: 0, epoch: 7, iters: 119072, time: 0.008) nll: 3957.384521 \n",
      "(GPU: 0, epoch: 7, iters: 119872, time: 0.008) nll: 5354.471680 \n",
      "(GPU: 0, epoch: 7, iters: 120672, time: 0.008) nll: 4173.222656 \n",
      "(GPU: 0, epoch: 7, iters: 121472, time: 0.008) nll: 4201.751465 \n",
      "(GPU: 0, epoch: 7, iters: 122272, time: 0.008) nll: 4282.167480 \n",
      "(GPU: 0, epoch: 7, iters: 123072, time: 0.008) nll: 3756.092773 \n",
      "(GPU: 0, epoch: 7, iters: 123872, time: 0.008) nll: 4002.890137 \n",
      "(GPU: 0, epoch: 7, iters: 124672, time: 0.008) nll: 3644.380127 \n",
      "(GPU: 0, epoch: 7, iters: 125472, time: 0.008) nll: 3174.116211 \n",
      "(GPU: 0, epoch: 7, iters: 126272, time: 0.008) nll: 3809.435059 \n",
      "(GPU: 0, epoch: 7, iters: 127072, time: 0.007) nll: 3414.396973 \n",
      "(GPU: 0, epoch: 7, iters: 127872, time: 0.008) nll: 4436.833984 \n",
      "(GPU: 0, epoch: 7, iters: 128672, time: 0.008) nll: 3393.660156 \n",
      "(GPU: 0, epoch: 7, iters: 129472, time: 0.008) nll: 3535.356445 \n",
      "(GPU: 0, epoch: 7, iters: 130272, time: 0.008) nll: 3408.048828 \n",
      "(GPU: 0, epoch: 7, iters: 131072, time: 0.008) nll: 4220.531250 \n",
      "(GPU: 0, epoch: 7, iters: 131872, time: 0.008) nll: 3784.509521 \n",
      "(GPU: 0, epoch: 7, iters: 132672, time: 0.008) nll: 3990.898926 \n",
      "(GPU: 0, epoch: 7, iters: 133472, time: 0.007) nll: 3272.126709 \n",
      "(GPU: 0, epoch: 7, iters: 134272, time: 0.008) nll: 3904.298340 \n",
      "(GPU: 0, epoch: 7, iters: 135072, time: 0.007) nll: 4810.203613 \n",
      "saving the latest model (epoch 7, total_steps 1120000)\n",
      "(GPU: 0, epoch: 7, iters: 135872, time: 0.008) nll: 3752.775391 \n",
      "(GPU: 0, epoch: 7, iters: 136672, time: 0.008) nll: 3657.655518 \n",
      "(GPU: 0, epoch: 7, iters: 137472, time: 0.008) nll: 4212.755859 \n",
      "(GPU: 0, epoch: 7, iters: 138272, time: 0.008) nll: 4741.372070 \n",
      "(GPU: 0, epoch: 7, iters: 139072, time: 0.008) nll: 4240.729980 \n",
      "(GPU: 0, epoch: 7, iters: 139872, time: 0.008) nll: 4684.407227 \n",
      "(GPU: 0, epoch: 7, iters: 140672, time: 0.008) nll: 2653.588867 \n",
      "[*] End of epoch 7 / 25 \t Time Taken: 1229 sec \n",
      "/cluster/54/streakfull/ADL4CV/Project/src/plz-autosdf/logs/bert2vqsc-text2shape-seq-LR1e-4-new-bert-2-prev_z_shape_input-try-2\n",
      "[*] learning rate = 0.0000800\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 2998/4397 [14:03<05:58,  3.91it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 8, iters: 32, time: 0.004) nll: 4028.183105 \n",
      "(GPU: 0, epoch: 8, iters: 32, time: 0.004) nll: 3036.190674 \n",
      "(GPU: 0, epoch: 8, iters: 768, time: 0.008) nll: 3278.721680 \n",
      "(GPU: 0, epoch: 8, iters: 1568, time: 0.008) nll: 4162.786133 \n",
      "(GPU: 0, epoch: 8, iters: 2368, time: 0.008) nll: 3269.246582 \n",
      "(GPU: 0, epoch: 8, iters: 3168, time: 0.008) nll: 4345.273438 \n",
      "(GPU: 0, epoch: 8, iters: 3968, time: 0.008) nll: 4225.941406 \n",
      "(GPU: 0, epoch: 8, iters: 4768, time: 0.007) nll: 2138.008789 \n",
      "(GPU: 0, epoch: 8, iters: 5568, time: 0.008) nll: 3016.838867 \n",
      "(GPU: 0, epoch: 8, iters: 6368, time: 0.008) nll: 3972.211426 \n",
      "(GPU: 0, epoch: 8, iters: 7168, time: 0.008) nll: 4233.197266 \n",
      "(GPU: 0, epoch: 8, iters: 7968, time: 0.008) nll: 3801.032715 \n",
      "(GPU: 0, epoch: 8, iters: 8768, time: 0.008) nll: 4007.298340 \n",
      "(GPU: 0, epoch: 8, iters: 9568, time: 0.008) nll: 3365.825195 \n",
      "(GPU: 0, epoch: 8, iters: 10368, time: 0.008) nll: 4250.383789 \n",
      "(GPU: 0, epoch: 8, iters: 11168, time: 0.008) nll: 3513.163330 \n",
      "(GPU: 0, epoch: 8, iters: 11968, time: 0.008) nll: 4164.905273 \n",
      "(GPU: 0, epoch: 8, iters: 12768, time: 0.008) nll: 3335.118652 \n",
      "(GPU: 0, epoch: 8, iters: 13568, time: 0.008) nll: 2449.090576 \n",
      "(GPU: 0, epoch: 8, iters: 14368, time: 0.008) nll: 4702.254395 \n",
      "saving the latest model (epoch 8, total_steps 1140000)\n",
      "(GPU: 0, epoch: 8, iters: 15168, time: 0.008) nll: 5463.428711 \n",
      "(GPU: 0, epoch: 8, iters: 15968, time: 0.008) nll: 3632.076904 \n",
      "(GPU: 0, epoch: 8, iters: 16768, time: 0.008) nll: 3870.970459 \n",
      "(GPU: 0, epoch: 8, iters: 17568, time: 0.008) nll: 3526.325195 \n",
      "(GPU: 0, epoch: 8, iters: 18368, time: 0.008) nll: 4149.522949 \n",
      "(GPU: 0, epoch: 8, iters: 19168, time: 0.008) nll: 4789.298340 \n",
      "(GPU: 0, epoch: 8, iters: 19968, time: 0.008) nll: 4250.594727 \n",
      "(GPU: 0, epoch: 8, iters: 20768, time: 0.008) nll: 4477.290527 \n",
      "(GPU: 0, epoch: 8, iters: 21568, time: 0.008) nll: 2702.633057 \n",
      "(GPU: 0, epoch: 8, iters: 22368, time: 0.008) nll: 4204.809570 \n",
      "(GPU: 0, epoch: 8, iters: 23168, time: 0.008) nll: 3402.879395 \n",
      "(GPU: 0, epoch: 8, iters: 23968, time: 0.008) nll: 4950.387695 \n",
      "(GPU: 0, epoch: 8, iters: 24768, time: 0.008) nll: 4176.234375 \n",
      "(GPU: 0, epoch: 8, iters: 25568, time: 0.007) nll: 5028.147949 \n",
      "(GPU: 0, epoch: 8, iters: 26368, time: 0.008) nll: 3621.774658 \n",
      "(GPU: 0, epoch: 8, iters: 26368, time: 0.013) nll: 3526.688965 \n",
      "(GPU: 0, epoch: 8, iters: 26368, time: 0.013) nll: 4516.057129 \n",
      "(GPU: 0, epoch: 8, iters: 27168, time: 0.008) nll: 5277.058594 \n",
      "(GPU: 0, epoch: 8, iters: 27968, time: 0.008) nll: 3660.316895 \n",
      "(GPU: 0, epoch: 8, iters: 28768, time: 0.007) nll: 2283.020752 \n",
      "(GPU: 0, epoch: 8, iters: 29568, time: 0.008) nll: 5752.718262 \n",
      "(GPU: 0, epoch: 8, iters: 30368, time: 0.008) nll: 4223.373047 \n",
      "(GPU: 0, epoch: 8, iters: 31168, time: 0.008) nll: 4619.756348 \n",
      "(GPU: 0, epoch: 8, iters: 31968, time: 0.007) nll: 4009.400146 \n",
      "(GPU: 0, epoch: 8, iters: 32768, time: 0.008) nll: 4286.049316 \n",
      "(GPU: 0, epoch: 8, iters: 33568, time: 0.008) nll: 4131.823242 \n",
      "(GPU: 0, epoch: 8, iters: 34368, time: 0.008) nll: 4145.564941 \n",
      "saving the latest model (epoch 8, total_steps 1160000)\n",
      "(GPU: 0, epoch: 8, iters: 35168, time: 0.008) nll: 4950.788086 \n",
      "(GPU: 0, epoch: 8, iters: 35968, time: 0.008) nll: 4237.246094 \n",
      "(GPU: 0, epoch: 8, iters: 36768, time: 0.008) nll: 4265.541992 \n",
      "(GPU: 0, epoch: 8, iters: 37568, time: 0.008) nll: 2713.793213 \n",
      "(GPU: 0, epoch: 8, iters: 38368, time: 0.008) nll: 4342.586914 \n",
      "(GPU: 0, epoch: 8, iters: 39168, time: 0.008) nll: 5242.844727 \n",
      "(GPU: 0, epoch: 8, iters: 39968, time: 0.008) nll: 5163.908691 \n",
      "(GPU: 0, epoch: 8, iters: 40768, time: 0.008) nll: 4259.269043 \n",
      "(GPU: 0, epoch: 8, iters: 41568, time: 0.008) nll: 3817.609863 \n",
      "(GPU: 0, epoch: 8, iters: 42368, time: 0.008) nll: 4579.471680 \n",
      "(GPU: 0, epoch: 8, iters: 43168, time: 0.007) nll: 5577.094727 \n",
      "(GPU: 0, epoch: 8, iters: 43968, time: 0.008) nll: 4173.164062 \n",
      "(GPU: 0, epoch: 8, iters: 44768, time: 0.008) nll: 4352.613770 \n",
      "(GPU: 0, epoch: 8, iters: 45568, time: 0.008) nll: 4261.338867 \n",
      "(GPU: 0, epoch: 8, iters: 46368, time: 0.008) nll: 4303.255859 \n",
      "(GPU: 0, epoch: 8, iters: 47168, time: 0.008) nll: 3988.720947 \n",
      "(GPU: 0, epoch: 8, iters: 47968, time: 0.008) nll: 3534.533447 \n",
      "(GPU: 0, epoch: 8, iters: 48768, time: 0.008) nll: 4596.883789 \n",
      "(GPU: 0, epoch: 8, iters: 49568, time: 0.007) nll: 1823.200806 \n",
      "(GPU: 0, epoch: 8, iters: 50368, time: 0.008) nll: 3442.088623 \n",
      "(GPU: 0, epoch: 8, iters: 51168, time: 0.007) nll: 3818.240234 \n",
      "(GPU: 0, epoch: 8, iters: 51968, time: 0.008) nll: 3391.913086 \n",
      "(GPU: 0, epoch: 8, iters: 52768, time: 0.008) nll: 3150.805176 \n",
      "(GPU: 0, epoch: 8, iters: 53568, time: 0.008) nll: 2478.404297 \n",
      "(GPU: 0, epoch: 8, iters: 54368, time: 0.008) nll: 3660.080566 \n",
      "saving the latest model (epoch 8, total_steps 1180000)\n",
      "(GPU: 0, epoch: 8, iters: 55168, time: 0.008) nll: 4425.761719 \n",
      "(GPU: 0, epoch: 8, iters: 55968, time: 0.007) nll: 2728.339355 \n",
      "(GPU: 0, epoch: 8, iters: 56768, time: 0.008) nll: 3091.026123 \n",
      "(GPU: 0, epoch: 8, iters: 57568, time: 0.008) nll: 4151.943359 \n",
      "(GPU: 0, epoch: 8, iters: 58368, time: 0.008) nll: 4162.941406 \n",
      "(GPU: 0, epoch: 8, iters: 59168, time: 0.008) nll: 4654.566406 \n",
      "(GPU: 0, epoch: 8, iters: 59968, time: 0.008) nll: 3554.363770 \n",
      "(GPU: 0, epoch: 8, iters: 60768, time: 0.008) nll: 5019.980469 \n",
      "(GPU: 0, epoch: 8, iters: 61568, time: 0.008) nll: 3755.544922 \n",
      "(GPU: 0, epoch: 8, iters: 62368, time: 0.008) nll: 4611.563477 \n",
      "(GPU: 0, epoch: 8, iters: 63168, time: 0.008) nll: 4934.250977 \n",
      "(GPU: 0, epoch: 8, iters: 63968, time: 0.007) nll: 4902.746582 \n",
      "(GPU: 0, epoch: 8, iters: 64768, time: 0.008) nll: 3939.507324 \n",
      "(GPU: 0, epoch: 8, iters: 65568, time: 0.008) nll: 4439.456543 \n",
      "(GPU: 0, epoch: 8, iters: 66368, time: 0.008) nll: 2771.867676 \n",
      "(GPU: 0, epoch: 8, iters: 67168, time: 0.008) nll: 5944.862793 \n",
      "(GPU: 0, epoch: 8, iters: 67968, time: 0.008) nll: 3290.621094 \n",
      "(GPU: 0, epoch: 8, iters: 68768, time: 0.008) nll: 3588.413086 \n",
      "(GPU: 0, epoch: 8, iters: 69568, time: 0.008) nll: 3399.433838 \n",
      "(GPU: 0, epoch: 8, iters: 70368, time: 0.008) nll: 4552.095703 \n",
      "(GPU: 0, epoch: 8, iters: 71168, time: 0.008) nll: 4267.736816 \n",
      "(GPU: 0, epoch: 8, iters: 71968, time: 0.008) nll: 3625.927979 \n",
      "(GPU: 0, epoch: 8, iters: 72768, time: 0.008) nll: 1990.588623 \n",
      "(GPU: 0, epoch: 8, iters: 73568, time: 0.008) nll: 3347.770996 \n",
      "(GPU: 0, epoch: 8, iters: 74368, time: 0.008) nll: 3141.979248 \n",
      "saving the latest model (epoch 8, total_steps 1200000)\n",
      "(GPU: 0, epoch: 8, iters: 75168, time: 0.008) nll: 3757.835938 \n",
      "(GPU: 0, epoch: 8, iters: 75968, time: 0.008) nll: 4411.386719 \n",
      "(GPU: 0, epoch: 8, iters: 76768, time: 0.008) nll: 4478.527344 \n",
      "(GPU: 0, epoch: 8, iters: 77568, time: 0.008) nll: 5136.583984 \n",
      "(GPU: 0, epoch: 8, iters: 78368, time: 0.008) nll: 4273.260742 \n",
      "(GPU: 0, epoch: 8, iters: 79168, time: 0.008) nll: 3704.091309 \n",
      "(GPU: 0, epoch: 8, iters: 79968, time: 0.008) nll: 3441.150146 \n",
      "(GPU: 0, epoch: 8, iters: 80768, time: 0.008) nll: 4495.489258 \n",
      "(GPU: 0, epoch: 8, iters: 81568, time: 0.008) nll: 3930.110352 \n",
      "(GPU: 0, epoch: 8, iters: 82368, time: 0.008) nll: 3569.265625 \n",
      "(GPU: 0, epoch: 8, iters: 83168, time: 0.008) nll: 4501.422363 \n",
      "(GPU: 0, epoch: 8, iters: 83968, time: 0.008) nll: 4053.576904 \n",
      "(GPU: 0, epoch: 8, iters: 84768, time: 0.008) nll: 3692.461182 \n",
      "(GPU: 0, epoch: 8, iters: 85568, time: 0.008) nll: 3508.516602 \n",
      "(GPU: 0, epoch: 8, iters: 86368, time: 0.008) nll: 4089.236572 \n",
      "(GPU: 0, epoch: 8, iters: 87168, time: 0.008) nll: 4144.335938 \n",
      "(GPU: 0, epoch: 8, iters: 87968, time: 0.008) nll: 4891.732422 \n",
      "(GPU: 0, epoch: 8, iters: 88768, time: 0.008) nll: 2624.612549 \n",
      "(GPU: 0, epoch: 8, iters: 89568, time: 0.008) nll: 3708.727051 \n",
      "(GPU: 0, epoch: 8, iters: 90368, time: 0.008) nll: 5068.732422 \n",
      "(GPU: 0, epoch: 8, iters: 91168, time: 0.008) nll: 3690.318848 \n",
      "(GPU: 0, epoch: 8, iters: 91968, time: 0.008) nll: 2806.197754 \n",
      "(GPU: 0, epoch: 8, iters: 92768, time: 0.008) nll: 4019.221191 \n",
      "(GPU: 0, epoch: 8, iters: 93568, time: 0.008) nll: 4495.394531 \n",
      "(GPU: 0, epoch: 8, iters: 94368, time: 0.008) nll: 3326.745605 \n",
      "saving the latest model (epoch 8, total_steps 1220000)\n",
      "(GPU: 0, epoch: 8, iters: 95168, time: 0.008) nll: 5246.433594 \n",
      "(GPU: 0, epoch: 8, iters: 95968, time: 0.008) nll: 2853.959961 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4397/4397 [20:32<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 8, iters: 96768, time: 0.008) nll: 4100.389160 \n",
      "(GPU: 0, epoch: 8, iters: 97568, time: 0.007) nll: 4564.387207 \n",
      "(GPU: 0, epoch: 8, iters: 98368, time: 0.008) nll: 4983.944824 \n",
      "(GPU: 0, epoch: 8, iters: 99168, time: 0.008) nll: 4503.182129 \n",
      "(GPU: 0, epoch: 8, iters: 99968, time: 0.008) nll: 3917.610840 \n",
      "(GPU: 0, epoch: 8, iters: 100768, time: 0.007) nll: 4354.682617 \n",
      "(GPU: 0, epoch: 8, iters: 101568, time: 0.008) nll: 3656.703369 \n",
      "(GPU: 0, epoch: 8, iters: 102368, time: 0.007) nll: 3870.082031 \n",
      "(GPU: 0, epoch: 8, iters: 103168, time: 0.008) nll: 4142.880371 \n",
      "(GPU: 0, epoch: 8, iters: 103968, time: 0.008) nll: 3813.975586 \n",
      "(GPU: 0, epoch: 8, iters: 104768, time: 0.008) nll: 3739.896973 \n",
      "(GPU: 0, epoch: 8, iters: 105568, time: 0.008) nll: 4703.903320 \n",
      "(GPU: 0, epoch: 8, iters: 106368, time: 0.008) nll: 2911.995117 \n",
      "(GPU: 0, epoch: 8, iters: 107168, time: 0.008) nll: 2931.258789 \n",
      "(GPU: 0, epoch: 8, iters: 107968, time: 0.008) nll: 4793.616699 \n",
      "(GPU: 0, epoch: 8, iters: 108768, time: 0.008) nll: 2910.897217 \n",
      "(GPU: 0, epoch: 8, iters: 109568, time: 0.008) nll: 4524.993652 \n",
      "(GPU: 0, epoch: 8, iters: 110368, time: 0.008) nll: 2631.109863 \n",
      "(GPU: 0, epoch: 8, iters: 111168, time: 0.008) nll: 4170.753906 \n",
      "(GPU: 0, epoch: 8, iters: 111968, time: 0.008) nll: 4371.912598 \n",
      "(GPU: 0, epoch: 8, iters: 112768, time: 0.008) nll: 3962.166748 \n",
      "(GPU: 0, epoch: 8, iters: 113568, time: 0.008) nll: 4713.546387 \n",
      "(GPU: 0, epoch: 8, iters: 114368, time: 0.008) nll: 2243.229004 \n",
      "saving the latest model (epoch 8, total_steps 1240000)\n",
      "(GPU: 0, epoch: 8, iters: 115168, time: 0.008) nll: 4872.991699 \n",
      "(GPU: 0, epoch: 8, iters: 115968, time: 0.008) nll: 5251.467285 \n",
      "(GPU: 0, epoch: 8, iters: 116768, time: 0.008) nll: 5515.352539 \n",
      "(GPU: 0, epoch: 8, iters: 117568, time: 0.008) nll: 4648.565430 \n",
      "(GPU: 0, epoch: 8, iters: 118368, time: 0.008) nll: 5261.802734 \n",
      "(GPU: 0, epoch: 8, iters: 119168, time: 0.008) nll: 5493.438477 \n",
      "(GPU: 0, epoch: 8, iters: 119968, time: 0.008) nll: 3064.615967 \n",
      "(GPU: 0, epoch: 8, iters: 120768, time: 0.008) nll: 3695.891113 \n",
      "(GPU: 0, epoch: 8, iters: 121568, time: 0.008) nll: 3966.969971 \n",
      "(GPU: 0, epoch: 8, iters: 122368, time: 0.008) nll: 4458.533203 \n",
      "(GPU: 0, epoch: 8, iters: 122368, time: 0.013) nll: 4396.837402 \n",
      "(GPU: 0, epoch: 8, iters: 122368, time: 0.013) nll: 4633.952148 \n",
      "(GPU: 0, epoch: 8, iters: 123168, time: 0.007) nll: 3893.344482 \n",
      "(GPU: 0, epoch: 8, iters: 123968, time: 0.008) nll: 3538.741699 \n",
      "(GPU: 0, epoch: 8, iters: 124768, time: 0.008) nll: 3742.958984 \n",
      "(GPU: 0, epoch: 8, iters: 125568, time: 0.008) nll: 2786.742188 \n",
      "(GPU: 0, epoch: 8, iters: 126368, time: 0.007) nll: 5342.246582 \n",
      "(GPU: 0, epoch: 8, iters: 127168, time: 0.008) nll: 4313.323242 \n",
      "(GPU: 0, epoch: 8, iters: 127968, time: 0.007) nll: 3961.987061 \n",
      "(GPU: 0, epoch: 8, iters: 128768, time: 0.008) nll: 4255.546875 \n",
      "(GPU: 0, epoch: 8, iters: 129568, time: 0.008) nll: 5462.744141 \n",
      "(GPU: 0, epoch: 8, iters: 130368, time: 0.008) nll: 3292.073975 \n",
      "(GPU: 0, epoch: 8, iters: 131168, time: 0.008) nll: 3488.301514 \n",
      "(GPU: 0, epoch: 8, iters: 131968, time: 0.008) nll: 5292.924805 \n",
      "(GPU: 0, epoch: 8, iters: 132768, time: 0.007) nll: 4174.929688 \n",
      "(GPU: 0, epoch: 8, iters: 133568, time: 0.008) nll: 4111.994141 \n",
      "(GPU: 0, epoch: 8, iters: 134368, time: 0.008) nll: 4556.736328 \n",
      "saving the latest model (epoch 8, total_steps 1260000)\n",
      "(GPU: 0, epoch: 8, iters: 135168, time: 0.008) nll: 4004.149170 \n",
      "(GPU: 0, epoch: 8, iters: 135968, time: 0.008) nll: 5581.912109 \n",
      "(GPU: 0, epoch: 8, iters: 136768, time: 0.008) nll: 4668.686523 \n",
      "(GPU: 0, epoch: 8, iters: 137568, time: 0.008) nll: 4050.921875 \n",
      "(GPU: 0, epoch: 8, iters: 138368, time: 0.008) nll: 1739.085205 \n",
      "(GPU: 0, epoch: 8, iters: 139168, time: 0.008) nll: 5468.414062 \n",
      "(GPU: 0, epoch: 8, iters: 139968, time: 0.008) nll: 4952.265625 \n",
      "[*] End of epoch 8 / 25 \t Time Taken: 1232 sec \n",
      "/cluster/54/streakfull/ADL4CV/Project/src/plz-autosdf/logs/bert2vqsc-text2shape-seq-LR1e-4-new-bert-2-prev_z_shape_input-try-2\n",
      "[*] learning rate = 0.0000900\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 3001/4397 [14:04<05:58,  3.90it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 9, iters: 32, time: 0.004) nll: 4632.177734 \n",
      "(GPU: 0, epoch: 9, iters: 32, time: 0.004) nll: 5426.461914 \n",
      "(GPU: 0, epoch: 9, iters: 64, time: 0.003) nll: 4390.902832 \n",
      "(GPU: 0, epoch: 9, iters: 864, time: 0.008) nll: 5180.675293 \n",
      "(GPU: 0, epoch: 9, iters: 1664, time: 0.008) nll: 2481.797363 \n",
      "(GPU: 0, epoch: 9, iters: 2464, time: 0.008) nll: 3780.268066 \n",
      "(GPU: 0, epoch: 9, iters: 3264, time: 0.008) nll: 3225.225098 \n",
      "(GPU: 0, epoch: 9, iters: 4064, time: 0.008) nll: 4384.261719 \n",
      "(GPU: 0, epoch: 9, iters: 4864, time: 0.008) nll: 2824.965820 \n",
      "(GPU: 0, epoch: 9, iters: 5664, time: 0.008) nll: 3152.661133 \n",
      "(GPU: 0, epoch: 9, iters: 6464, time: 0.008) nll: 4914.000488 \n",
      "(GPU: 0, epoch: 9, iters: 7264, time: 0.008) nll: 3374.990967 \n",
      "(GPU: 0, epoch: 9, iters: 8064, time: 0.008) nll: 4850.485840 \n",
      "(GPU: 0, epoch: 9, iters: 8864, time: 0.008) nll: 2463.730957 \n",
      "(GPU: 0, epoch: 9, iters: 9664, time: 0.008) nll: 5721.694824 \n",
      "(GPU: 0, epoch: 9, iters: 10464, time: 0.008) nll: 3196.385498 \n",
      "(GPU: 0, epoch: 9, iters: 11264, time: 0.008) nll: 3987.448486 \n",
      "(GPU: 0, epoch: 9, iters: 12064, time: 0.008) nll: 4299.796875 \n",
      "(GPU: 0, epoch: 9, iters: 12864, time: 0.008) nll: 5300.729004 \n",
      "(GPU: 0, epoch: 9, iters: 13664, time: 0.008) nll: 2994.250977 \n",
      "saving the latest model (epoch 9, total_steps 1280000)\n",
      "(GPU: 0, epoch: 9, iters: 14464, time: 0.008) nll: 4296.149902 \n",
      "(GPU: 0, epoch: 9, iters: 15264, time: 0.008) nll: 3635.462891 \n",
      "(GPU: 0, epoch: 9, iters: 16064, time: 0.008) nll: 4372.524414 \n",
      "(GPU: 0, epoch: 9, iters: 16864, time: 0.007) nll: 4680.803711 \n",
      "(GPU: 0, epoch: 9, iters: 17664, time: 0.008) nll: 4418.454102 \n",
      "(GPU: 0, epoch: 9, iters: 18464, time: 0.008) nll: 3696.061523 \n",
      "(GPU: 0, epoch: 9, iters: 19264, time: 0.008) nll: 3009.546143 \n",
      "(GPU: 0, epoch: 9, iters: 20064, time: 0.007) nll: 4738.569336 \n",
      "(GPU: 0, epoch: 9, iters: 20864, time: 0.008) nll: 4038.518555 \n",
      "(GPU: 0, epoch: 9, iters: 21664, time: 0.008) nll: 5190.927734 \n",
      "(GPU: 0, epoch: 9, iters: 22464, time: 0.008) nll: 3499.663818 \n",
      "(GPU: 0, epoch: 9, iters: 23264, time: 0.007) nll: 4392.795898 \n",
      "(GPU: 0, epoch: 9, iters: 24064, time: 0.008) nll: 4328.984375 \n",
      "(GPU: 0, epoch: 9, iters: 24864, time: 0.008) nll: 3898.474609 \n",
      "(GPU: 0, epoch: 9, iters: 25664, time: 0.008) nll: 3858.848389 \n",
      "(GPU: 0, epoch: 9, iters: 26464, time: 0.007) nll: 5674.063477 \n",
      "(GPU: 0, epoch: 9, iters: 27264, time: 0.008) nll: 2955.458008 \n",
      "(GPU: 0, epoch: 9, iters: 28064, time: 0.008) nll: 3777.632324 \n",
      "(GPU: 0, epoch: 9, iters: 28864, time: 0.008) nll: 3807.499023 \n",
      "(GPU: 0, epoch: 9, iters: 29664, time: 0.007) nll: 4480.959473 \n",
      "(GPU: 0, epoch: 9, iters: 30464, time: 0.008) nll: 3158.859619 \n",
      "(GPU: 0, epoch: 9, iters: 31264, time: 0.008) nll: 4472.938965 \n",
      "(GPU: 0, epoch: 9, iters: 32064, time: 0.008) nll: 4062.064941 \n",
      "(GPU: 0, epoch: 9, iters: 32864, time: 0.008) nll: 2850.237305 \n",
      "(GPU: 0, epoch: 9, iters: 33664, time: 0.008) nll: 3483.259277 \n",
      "saving the latest model (epoch 9, total_steps 1300000)\n",
      "(GPU: 0, epoch: 9, iters: 34464, time: 0.008) nll: 4057.854492 \n",
      "(GPU: 0, epoch: 9, iters: 35264, time: 0.008) nll: 5205.594238 \n",
      "(GPU: 0, epoch: 9, iters: 36064, time: 0.008) nll: 4817.520508 \n",
      "(GPU: 0, epoch: 9, iters: 36864, time: 0.008) nll: 3407.997070 \n",
      "(GPU: 0, epoch: 9, iters: 37664, time: 0.008) nll: 3764.763428 \n",
      "(GPU: 0, epoch: 9, iters: 38464, time: 0.008) nll: 3368.319336 \n",
      "(GPU: 0, epoch: 9, iters: 39264, time: 0.008) nll: 4917.821289 \n",
      "(GPU: 0, epoch: 9, iters: 40064, time: 0.008) nll: 4845.621582 \n",
      "(GPU: 0, epoch: 9, iters: 40864, time: 0.008) nll: 4028.053223 \n",
      "(GPU: 0, epoch: 9, iters: 41664, time: 0.008) nll: 4001.924805 \n",
      "(GPU: 0, epoch: 9, iters: 42464, time: 0.007) nll: 4384.379883 \n",
      "(GPU: 0, epoch: 9, iters: 43264, time: 0.008) nll: 4296.374023 \n",
      "(GPU: 0, epoch: 9, iters: 44064, time: 0.008) nll: 3004.947266 \n",
      "(GPU: 0, epoch: 9, iters: 44864, time: 0.008) nll: 3348.345703 \n",
      "(GPU: 0, epoch: 9, iters: 45664, time: 0.008) nll: 3530.242188 \n",
      "(GPU: 0, epoch: 9, iters: 46464, time: 0.008) nll: 3941.815918 \n",
      "(GPU: 0, epoch: 9, iters: 47264, time: 0.008) nll: 2442.272461 \n",
      "(GPU: 0, epoch: 9, iters: 48064, time: 0.008) nll: 3952.689453 \n",
      "(GPU: 0, epoch: 9, iters: 48864, time: 0.008) nll: 3486.061523 \n",
      "(GPU: 0, epoch: 9, iters: 49664, time: 0.008) nll: 4851.555664 \n",
      "(GPU: 0, epoch: 9, iters: 50464, time: 0.008) nll: 3725.338379 \n",
      "(GPU: 0, epoch: 9, iters: 51264, time: 0.008) nll: 3129.147949 \n",
      "(GPU: 0, epoch: 9, iters: 52064, time: 0.007) nll: 3002.185547 \n",
      "(GPU: 0, epoch: 9, iters: 52864, time: 0.008) nll: 4323.177734 \n",
      "(GPU: 0, epoch: 9, iters: 53664, time: 0.008) nll: 4835.061523 \n",
      "saving the latest model (epoch 9, total_steps 1320000)\n",
      "(GPU: 0, epoch: 9, iters: 54464, time: 0.008) nll: 3717.266357 \n",
      "(GPU: 0, epoch: 9, iters: 55264, time: 0.007) nll: 3359.769043 \n",
      "(GPU: 0, epoch: 9, iters: 56064, time: 0.008) nll: 4671.817871 \n",
      "(GPU: 0, epoch: 9, iters: 56864, time: 0.008) nll: 4438.372559 \n",
      "(GPU: 0, epoch: 9, iters: 57664, time: 0.008) nll: 2491.798828 \n",
      "(GPU: 0, epoch: 9, iters: 58464, time: 0.007) nll: 4084.043457 \n",
      "(GPU: 0, epoch: 9, iters: 59264, time: 0.008) nll: 5290.263672 \n",
      "(GPU: 0, epoch: 9, iters: 60064, time: 0.008) nll: 3925.306641 \n",
      "(GPU: 0, epoch: 9, iters: 60864, time: 0.008) nll: 5142.439453 \n",
      "(GPU: 0, epoch: 9, iters: 61664, time: 0.008) nll: 3131.192139 \n",
      "(GPU: 0, epoch: 9, iters: 62464, time: 0.008) nll: 4170.173828 \n",
      "(GPU: 0, epoch: 9, iters: 63264, time: 0.008) nll: 2860.460449 \n",
      "(GPU: 0, epoch: 9, iters: 64064, time: 0.008) nll: 3180.301025 \n",
      "(GPU: 0, epoch: 9, iters: 64864, time: 0.008) nll: 3915.749023 \n",
      "(GPU: 0, epoch: 9, iters: 65664, time: 0.008) nll: 3424.989502 \n",
      "(GPU: 0, epoch: 9, iters: 66464, time: 0.008) nll: 2409.398926 \n",
      "(GPU: 0, epoch: 9, iters: 67264, time: 0.008) nll: 3339.183594 \n",
      "(GPU: 0, epoch: 9, iters: 68064, time: 0.007) nll: 2645.994873 \n",
      "(GPU: 0, epoch: 9, iters: 68864, time: 0.008) nll: 3424.236328 \n",
      "(GPU: 0, epoch: 9, iters: 69664, time: 0.008) nll: 3318.445801 \n",
      "(GPU: 0, epoch: 9, iters: 70464, time: 0.008) nll: 4336.195312 \n",
      "(GPU: 0, epoch: 9, iters: 71264, time: 0.007) nll: 3091.740234 \n",
      "(GPU: 0, epoch: 9, iters: 72064, time: 0.008) nll: 2552.536133 \n",
      "(GPU: 0, epoch: 9, iters: 72864, time: 0.007) nll: 3428.493408 \n",
      "(GPU: 0, epoch: 9, iters: 73664, time: 0.008) nll: 4420.306641 \n",
      "saving the latest model (epoch 9, total_steps 1340000)\n",
      "(GPU: 0, epoch: 9, iters: 74464, time: 0.007) nll: 3111.059082 \n",
      "(GPU: 0, epoch: 9, iters: 75264, time: 0.008) nll: 4006.043457 \n",
      "(GPU: 0, epoch: 9, iters: 76064, time: 0.008) nll: 3337.653076 \n",
      "(GPU: 0, epoch: 9, iters: 76864, time: 0.008) nll: 3238.522461 \n",
      "(GPU: 0, epoch: 9, iters: 77664, time: 0.007) nll: 3984.489746 \n",
      "(GPU: 0, epoch: 9, iters: 77664, time: 0.012) nll: 3961.671875 \n",
      "(GPU: 0, epoch: 9, iters: 77664, time: 0.012) nll: 3388.447266 \n",
      "(GPU: 0, epoch: 9, iters: 78464, time: 0.008) nll: 5095.940430 \n",
      "(GPU: 0, epoch: 9, iters: 79264, time: 0.008) nll: 3318.781494 \n",
      "(GPU: 0, epoch: 9, iters: 80064, time: 0.008) nll: 2345.667969 \n",
      "(GPU: 0, epoch: 9, iters: 80864, time: 0.008) nll: 2983.022949 \n",
      "(GPU: 0, epoch: 9, iters: 81664, time: 0.008) nll: 2857.933105 \n",
      "(GPU: 0, epoch: 9, iters: 82464, time: 0.007) nll: 3435.780273 \n",
      "(GPU: 0, epoch: 9, iters: 83264, time: 0.008) nll: 3458.249512 \n",
      "(GPU: 0, epoch: 9, iters: 84064, time: 0.008) nll: 2805.076904 \n",
      "(GPU: 0, epoch: 9, iters: 84864, time: 0.008) nll: 4019.239258 \n",
      "(GPU: 0, epoch: 9, iters: 85664, time: 0.008) nll: 3050.887207 \n",
      "(GPU: 0, epoch: 9, iters: 86464, time: 0.008) nll: 4376.080078 \n",
      "(GPU: 0, epoch: 9, iters: 87264, time: 0.007) nll: 3556.267090 \n",
      "(GPU: 0, epoch: 9, iters: 88064, time: 0.008) nll: 3989.498047 \n",
      "(GPU: 0, epoch: 9, iters: 88864, time: 0.008) nll: 3538.067383 \n",
      "(GPU: 0, epoch: 9, iters: 89664, time: 0.008) nll: 3625.873535 \n",
      "(GPU: 0, epoch: 9, iters: 90464, time: 0.008) nll: 3909.180664 \n",
      "(GPU: 0, epoch: 9, iters: 91264, time: 0.008) nll: 5130.982910 \n",
      "(GPU: 0, epoch: 9, iters: 92064, time: 0.008) nll: 3749.436035 \n",
      "(GPU: 0, epoch: 9, iters: 92864, time: 0.008) nll: 3784.403076 \n",
      "(GPU: 0, epoch: 9, iters: 93664, time: 0.007) nll: 3693.658203 \n",
      "saving the latest model (epoch 9, total_steps 1360000)\n",
      "(GPU: 0, epoch: 9, iters: 94464, time: 0.008) nll: 5132.424805 \n",
      "(GPU: 0, epoch: 9, iters: 95264, time: 0.008) nll: 4715.042969 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4397/4397 [20:32<00:00,  3.57it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 9, iters: 96064, time: 0.008) nll: 4165.122070 \n",
      "(GPU: 0, epoch: 9, iters: 96864, time: 0.007) nll: 4291.019043 \n",
      "(GPU: 0, epoch: 9, iters: 97664, time: 0.008) nll: 3654.206299 \n",
      "(GPU: 0, epoch: 9, iters: 98464, time: 0.008) nll: 5153.710938 \n",
      "(GPU: 0, epoch: 9, iters: 99264, time: 0.008) nll: 3617.842285 \n",
      "(GPU: 0, epoch: 9, iters: 100064, time: 0.008) nll: 4549.301758 \n",
      "(GPU: 0, epoch: 9, iters: 100864, time: 0.008) nll: 2873.963867 \n",
      "(GPU: 0, epoch: 9, iters: 101664, time: 0.007) nll: 4171.440430 \n",
      "(GPU: 0, epoch: 9, iters: 102464, time: 0.008) nll: 3515.098145 \n",
      "(GPU: 0, epoch: 9, iters: 103264, time: 0.008) nll: 5240.017090 \n",
      "(GPU: 0, epoch: 9, iters: 104064, time: 0.008) nll: 3350.802979 \n",
      "(GPU: 0, epoch: 9, iters: 104864, time: 0.008) nll: 4232.966797 \n",
      "(GPU: 0, epoch: 9, iters: 105664, time: 0.008) nll: 4988.189453 \n",
      "(GPU: 0, epoch: 9, iters: 106464, time: 0.008) nll: 3208.616699 \n",
      "(GPU: 0, epoch: 9, iters: 107264, time: 0.008) nll: 4235.684570 \n",
      "(GPU: 0, epoch: 9, iters: 108064, time: 0.008) nll: 2800.148438 \n",
      "(GPU: 0, epoch: 9, iters: 108864, time: 0.008) nll: 4500.506836 \n",
      "(GPU: 0, epoch: 9, iters: 109664, time: 0.008) nll: 4034.175049 \n",
      "(GPU: 0, epoch: 9, iters: 110464, time: 0.008) nll: 3578.479736 \n",
      "(GPU: 0, epoch: 9, iters: 111264, time: 0.008) nll: 3752.187988 \n",
      "(GPU: 0, epoch: 9, iters: 112064, time: 0.008) nll: 3511.072266 \n",
      "(GPU: 0, epoch: 9, iters: 112864, time: 0.008) nll: 4674.689453 \n",
      "(GPU: 0, epoch: 9, iters: 113664, time: 0.008) nll: 4410.881836 \n",
      "saving the latest model (epoch 9, total_steps 1380000)\n",
      "(GPU: 0, epoch: 9, iters: 114464, time: 0.008) nll: 4720.532227 \n",
      "(GPU: 0, epoch: 9, iters: 115264, time: 0.008) nll: 3683.670410 \n",
      "(GPU: 0, epoch: 9, iters: 116064, time: 0.008) nll: 4451.478027 \n",
      "(GPU: 0, epoch: 9, iters: 116864, time: 0.008) nll: 3927.218506 \n",
      "(GPU: 0, epoch: 9, iters: 117664, time: 0.007) nll: 4957.929688 \n",
      "(GPU: 0, epoch: 9, iters: 118464, time: 0.008) nll: 3125.693848 \n",
      "(GPU: 0, epoch: 9, iters: 119264, time: 0.008) nll: 3413.163818 \n",
      "(GPU: 0, epoch: 9, iters: 120064, time: 0.008) nll: 4148.722656 \n",
      "(GPU: 0, epoch: 9, iters: 120864, time: 0.007) nll: 3753.194336 \n",
      "(GPU: 0, epoch: 9, iters: 121664, time: 0.008) nll: 6159.994141 \n",
      "(GPU: 0, epoch: 9, iters: 122464, time: 0.008) nll: 4351.070801 \n",
      "(GPU: 0, epoch: 9, iters: 123264, time: 0.008) nll: 3968.885254 \n",
      "(GPU: 0, epoch: 9, iters: 124064, time: 0.008) nll: 4508.226562 \n",
      "(GPU: 0, epoch: 9, iters: 124864, time: 0.008) nll: 4366.907227 \n",
      "(GPU: 0, epoch: 9, iters: 125664, time: 0.007) nll: 4485.504395 \n",
      "(GPU: 0, epoch: 9, iters: 126464, time: 0.008) nll: 4820.096680 \n",
      "(GPU: 0, epoch: 9, iters: 127264, time: 0.008) nll: 3300.833496 \n",
      "(GPU: 0, epoch: 9, iters: 128064, time: 0.008) nll: 3703.441895 \n",
      "(GPU: 0, epoch: 9, iters: 128864, time: 0.008) nll: 4361.641602 \n",
      "(GPU: 0, epoch: 9, iters: 129664, time: 0.008) nll: 4773.497559 \n",
      "(GPU: 0, epoch: 9, iters: 130464, time: 0.008) nll: 4531.167969 \n",
      "(GPU: 0, epoch: 9, iters: 131264, time: 0.008) nll: 4711.660156 \n",
      "(GPU: 0, epoch: 9, iters: 132064, time: 0.008) nll: 4291.064453 \n",
      "(GPU: 0, epoch: 9, iters: 132864, time: 0.008) nll: 4282.066406 \n",
      "(GPU: 0, epoch: 9, iters: 133664, time: 0.008) nll: 4779.824219 \n",
      "saving the latest model (epoch 9, total_steps 1400000)\n",
      "(GPU: 0, epoch: 9, iters: 134464, time: 0.008) nll: 3614.250977 \n",
      "(GPU: 0, epoch: 9, iters: 135264, time: 0.007) nll: 3662.104980 \n",
      "(GPU: 0, epoch: 9, iters: 136064, time: 0.008) nll: 2702.821533 \n",
      "(GPU: 0, epoch: 9, iters: 136864, time: 0.008) nll: 4172.520508 \n",
      "(GPU: 0, epoch: 9, iters: 137664, time: 0.008) nll: 4172.402344 \n",
      "(GPU: 0, epoch: 9, iters: 138464, time: 0.007) nll: 4625.316406 \n",
      "(GPU: 0, epoch: 9, iters: 139264, time: 0.008) nll: 4231.458984 \n",
      "(GPU: 0, epoch: 9, iters: 140064, time: 0.008) nll: 4499.607422 \n",
      "saving the model at the end of epoch 9, iters 1407040\n",
      "([test] GPU: 0, epoch: 9) \n",
      "OrderedDict()\n",
      "[*] End of epoch 9 / 25 \t Time Taken: 1260 sec \n",
      "/cluster/54/streakfull/ADL4CV/Project/src/plz-autosdf/logs/bert2vqsc-text2shape-seq-LR1e-4-new-bert-2-prev_z_shape_input-try-2\n",
      "[*] learning rate = 0.0001000\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2954/4397 [13:50<06:13,  3.87it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 10, iters: 32, time: 0.004) nll: 3150.787109 \n",
      "(GPU: 0, epoch: 10, iters: 32, time: 0.004) nll: 4404.478516 \n",
      "(GPU: 0, epoch: 10, iters: 160, time: 0.008) nll: 4108.597168 \n",
      "(GPU: 0, epoch: 10, iters: 960, time: 0.008) nll: 4660.493164 \n",
      "(GPU: 0, epoch: 10, iters: 1760, time: 0.007) nll: 2366.054688 \n",
      "(GPU: 0, epoch: 10, iters: 2560, time: 0.008) nll: 3355.028320 \n",
      "(GPU: 0, epoch: 10, iters: 3360, time: 0.008) nll: 4896.701172 \n",
      "(GPU: 0, epoch: 10, iters: 4160, time: 0.008) nll: 4719.749023 \n",
      "(GPU: 0, epoch: 10, iters: 4960, time: 0.008) nll: 4100.036621 \n",
      "(GPU: 0, epoch: 10, iters: 5760, time: 0.008) nll: 3807.416504 \n",
      "(GPU: 0, epoch: 10, iters: 6560, time: 0.008) nll: 3058.296875 \n",
      "(GPU: 0, epoch: 10, iters: 7360, time: 0.008) nll: 4169.481445 \n",
      "(GPU: 0, epoch: 10, iters: 8160, time: 0.008) nll: 3756.111816 \n",
      "(GPU: 0, epoch: 10, iters: 8960, time: 0.008) nll: 2651.245850 \n",
      "(GPU: 0, epoch: 10, iters: 9760, time: 0.008) nll: 4661.414062 \n",
      "(GPU: 0, epoch: 10, iters: 10560, time: 0.008) nll: 4539.000000 \n",
      "(GPU: 0, epoch: 10, iters: 11360, time: 0.008) nll: 4067.217773 \n",
      "(GPU: 0, epoch: 10, iters: 12160, time: 0.008) nll: 4833.223633 \n",
      "(GPU: 0, epoch: 10, iters: 12960, time: 0.008) nll: 3876.612549 \n",
      "saving the latest model (epoch 10, total_steps 1420000)\n",
      "(GPU: 0, epoch: 10, iters: 13760, time: 0.008) nll: 5071.405273 \n",
      "(GPU: 0, epoch: 10, iters: 14560, time: 0.007) nll: 5211.127930 \n",
      "(GPU: 0, epoch: 10, iters: 15360, time: 0.008) nll: 3719.926758 \n",
      "(GPU: 0, epoch: 10, iters: 16160, time: 0.008) nll: 3918.913086 \n",
      "(GPU: 0, epoch: 10, iters: 16960, time: 0.008) nll: 4323.850098 \n",
      "(GPU: 0, epoch: 10, iters: 17760, time: 0.007) nll: 4317.320312 \n",
      "(GPU: 0, epoch: 10, iters: 18560, time: 0.008) nll: 4615.255859 \n",
      "(GPU: 0, epoch: 10, iters: 19360, time: 0.008) nll: 2634.536621 \n",
      "(GPU: 0, epoch: 10, iters: 20160, time: 0.008) nll: 3804.000488 \n",
      "(GPU: 0, epoch: 10, iters: 20960, time: 0.008) nll: 3211.359619 \n",
      "(GPU: 0, epoch: 10, iters: 21760, time: 0.008) nll: 5591.785645 \n",
      "(GPU: 0, epoch: 10, iters: 22560, time: 0.008) nll: 2464.951660 \n",
      "(GPU: 0, epoch: 10, iters: 23360, time: 0.008) nll: 4269.433594 \n",
      "(GPU: 0, epoch: 10, iters: 24160, time: 0.008) nll: 4793.358887 \n",
      "(GPU: 0, epoch: 10, iters: 24960, time: 0.008) nll: 3383.235840 \n",
      "(GPU: 0, epoch: 10, iters: 25760, time: 0.008) nll: 3501.865234 \n",
      "(GPU: 0, epoch: 10, iters: 26560, time: 0.008) nll: 4080.228516 \n",
      "(GPU: 0, epoch: 10, iters: 27360, time: 0.008) nll: 3999.530029 \n",
      "(GPU: 0, epoch: 10, iters: 28160, time: 0.008) nll: 3808.927246 \n",
      "(GPU: 0, epoch: 10, iters: 28960, time: 0.008) nll: 5220.303711 \n",
      "(GPU: 0, epoch: 10, iters: 29760, time: 0.008) nll: 5284.937988 \n",
      "(GPU: 0, epoch: 10, iters: 30560, time: 0.007) nll: 4798.634766 \n",
      "(GPU: 0, epoch: 10, iters: 31360, time: 0.008) nll: 2709.540283 \n",
      "(GPU: 0, epoch: 10, iters: 32160, time: 0.008) nll: 4007.412842 \n",
      "(GPU: 0, epoch: 10, iters: 32960, time: 0.008) nll: 5462.939941 \n",
      "(GPU: 0, epoch: 10, iters: 32960, time: 0.013) nll: 5326.376953 \n",
      "(GPU: 0, epoch: 10, iters: 32960, time: 0.013) nll: 2441.322754 \n",
      "saving the latest model (epoch 10, total_steps 1440000)\n",
      "(GPU: 0, epoch: 10, iters: 33760, time: 0.008) nll: 4177.258789 \n",
      "(GPU: 0, epoch: 10, iters: 34560, time: 0.008) nll: 5673.242188 \n",
      "(GPU: 0, epoch: 10, iters: 35360, time: 0.008) nll: 3881.449707 \n",
      "(GPU: 0, epoch: 10, iters: 36160, time: 0.008) nll: 3412.635986 \n",
      "(GPU: 0, epoch: 10, iters: 36960, time: 0.008) nll: 3513.961914 \n",
      "(GPU: 0, epoch: 10, iters: 37760, time: 0.008) nll: 4986.225586 \n",
      "(GPU: 0, epoch: 10, iters: 38560, time: 0.008) nll: 3422.861084 \n",
      "(GPU: 0, epoch: 10, iters: 39360, time: 0.008) nll: 3950.745361 \n",
      "(GPU: 0, epoch: 10, iters: 40160, time: 0.008) nll: 3606.015137 \n",
      "(GPU: 0, epoch: 10, iters: 40960, time: 0.008) nll: 4364.811035 \n",
      "(GPU: 0, epoch: 10, iters: 41760, time: 0.008) nll: 4180.201172 \n",
      "(GPU: 0, epoch: 10, iters: 42560, time: 0.008) nll: 4160.834961 \n",
      "(GPU: 0, epoch: 10, iters: 43360, time: 0.008) nll: 4067.759766 \n",
      "(GPU: 0, epoch: 10, iters: 44160, time: 0.008) nll: 3309.266602 \n",
      "(GPU: 0, epoch: 10, iters: 44960, time: 0.008) nll: 3402.148926 \n",
      "(GPU: 0, epoch: 10, iters: 45760, time: 0.008) nll: 3638.021484 \n",
      "(GPU: 0, epoch: 10, iters: 46560, time: 0.008) nll: 4321.598633 \n",
      "(GPU: 0, epoch: 10, iters: 47360, time: 0.008) nll: 3217.645752 \n",
      "(GPU: 0, epoch: 10, iters: 48160, time: 0.007) nll: 3062.603760 \n",
      "(GPU: 0, epoch: 10, iters: 48960, time: 0.008) nll: 5020.160156 \n",
      "(GPU: 0, epoch: 10, iters: 49760, time: 0.008) nll: 5344.484375 \n",
      "(GPU: 0, epoch: 10, iters: 50560, time: 0.008) nll: 4082.276367 \n",
      "(GPU: 0, epoch: 10, iters: 51360, time: 0.008) nll: 4879.562012 \n",
      "(GPU: 0, epoch: 10, iters: 52160, time: 0.008) nll: 4621.938965 \n",
      "(GPU: 0, epoch: 10, iters: 52960, time: 0.008) nll: 3914.270020 \n",
      "saving the latest model (epoch 10, total_steps 1460000)\n",
      "(GPU: 0, epoch: 10, iters: 53760, time: 0.008) nll: 4704.367188 \n",
      "(GPU: 0, epoch: 10, iters: 54560, time: 0.008) nll: 5000.425781 \n",
      "(GPU: 0, epoch: 10, iters: 55360, time: 0.008) nll: 2881.791016 \n",
      "(GPU: 0, epoch: 10, iters: 56160, time: 0.008) nll: 4367.728516 \n",
      "(GPU: 0, epoch: 10, iters: 56960, time: 0.008) nll: 3704.985840 \n",
      "(GPU: 0, epoch: 10, iters: 57760, time: 0.007) nll: 3429.262207 \n",
      "(GPU: 0, epoch: 10, iters: 58560, time: 0.008) nll: 3871.794922 \n",
      "(GPU: 0, epoch: 10, iters: 59360, time: 0.008) nll: 4199.349609 \n",
      "(GPU: 0, epoch: 10, iters: 60160, time: 0.008) nll: 4536.741211 \n",
      "(GPU: 0, epoch: 10, iters: 60960, time: 0.008) nll: 5730.341797 \n",
      "(GPU: 0, epoch: 10, iters: 61760, time: 0.008) nll: 3705.837891 \n",
      "(GPU: 0, epoch: 10, iters: 62560, time: 0.007) nll: 3212.883789 \n",
      "(GPU: 0, epoch: 10, iters: 63360, time: 0.008) nll: 4341.018555 \n",
      "(GPU: 0, epoch: 10, iters: 64160, time: 0.008) nll: 4205.157227 \n",
      "(GPU: 0, epoch: 10, iters: 64960, time: 0.008) nll: 3835.144043 \n",
      "(GPU: 0, epoch: 10, iters: 65760, time: 0.008) nll: 3406.145996 \n",
      "(GPU: 0, epoch: 10, iters: 66560, time: 0.008) nll: 4044.034912 \n",
      "(GPU: 0, epoch: 10, iters: 67360, time: 0.008) nll: 5371.309082 \n",
      "(GPU: 0, epoch: 10, iters: 68160, time: 0.008) nll: 4668.566406 \n",
      "(GPU: 0, epoch: 10, iters: 68960, time: 0.008) nll: 4198.891602 \n",
      "(GPU: 0, epoch: 10, iters: 69760, time: 0.008) nll: 4389.080078 \n",
      "(GPU: 0, epoch: 10, iters: 70560, time: 0.008) nll: 3803.845703 \n",
      "(GPU: 0, epoch: 10, iters: 71360, time: 0.008) nll: 3409.213379 \n",
      "(GPU: 0, epoch: 10, iters: 72160, time: 0.008) nll: 3279.638672 \n",
      "(GPU: 0, epoch: 10, iters: 72960, time: 0.008) nll: 3622.736816 \n",
      "saving the latest model (epoch 10, total_steps 1480000)\n",
      "(GPU: 0, epoch: 10, iters: 73760, time: 0.008) nll: 3080.565674 \n",
      "(GPU: 0, epoch: 10, iters: 74560, time: 0.008) nll: 3882.644775 \n",
      "(GPU: 0, epoch: 10, iters: 75360, time: 0.008) nll: 4224.120117 \n",
      "(GPU: 0, epoch: 10, iters: 76160, time: 0.008) nll: 4538.130859 \n",
      "(GPU: 0, epoch: 10, iters: 76960, time: 0.007) nll: 4502.435547 \n",
      "(GPU: 0, epoch: 10, iters: 77760, time: 0.008) nll: 4088.850586 \n",
      "(GPU: 0, epoch: 10, iters: 78560, time: 0.008) nll: 4401.093750 \n",
      "(GPU: 0, epoch: 10, iters: 79360, time: 0.008) nll: 3912.046631 \n",
      "(GPU: 0, epoch: 10, iters: 80160, time: 0.007) nll: 4860.916992 \n",
      "(GPU: 0, epoch: 10, iters: 80960, time: 0.008) nll: 2849.054199 \n",
      "(GPU: 0, epoch: 10, iters: 81760, time: 0.008) nll: 3657.955566 \n",
      "(GPU: 0, epoch: 10, iters: 82560, time: 0.008) nll: 4950.565918 \n",
      "(GPU: 0, epoch: 10, iters: 83360, time: 0.008) nll: 3757.004395 \n",
      "(GPU: 0, epoch: 10, iters: 84160, time: 0.008) nll: 5275.175293 \n",
      "(GPU: 0, epoch: 10, iters: 84960, time: 0.008) nll: 3871.333008 \n",
      "(GPU: 0, epoch: 10, iters: 85760, time: 0.008) nll: 5343.308594 \n",
      "(GPU: 0, epoch: 10, iters: 86560, time: 0.008) nll: 4927.944336 \n",
      "(GPU: 0, epoch: 10, iters: 87360, time: 0.008) nll: 3761.750488 \n",
      "(GPU: 0, epoch: 10, iters: 88160, time: 0.008) nll: 4144.092773 \n",
      "(GPU: 0, epoch: 10, iters: 88960, time: 0.008) nll: 5066.041016 \n",
      "(GPU: 0, epoch: 10, iters: 89760, time: 0.007) nll: 4438.512207 \n",
      "(GPU: 0, epoch: 10, iters: 90560, time: 0.008) nll: 3766.041504 \n",
      "(GPU: 0, epoch: 10, iters: 91360, time: 0.008) nll: 3494.687256 \n",
      "(GPU: 0, epoch: 10, iters: 92160, time: 0.008) nll: 3119.893311 \n",
      "(GPU: 0, epoch: 10, iters: 92960, time: 0.008) nll: 4327.599609 \n",
      "saving the latest model (epoch 10, total_steps 1500000)\n",
      "(GPU: 0, epoch: 10, iters: 93760, time: 0.008) nll: 2890.307617 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4397/4397 [20:31<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 10, iters: 94560, time: 0.007) nll: 5485.906250 \n",
      "(GPU: 0, epoch: 10, iters: 95360, time: 0.008) nll: 3560.540527 \n",
      "(GPU: 0, epoch: 10, iters: 96160, time: 0.008) nll: 2966.339355 \n",
      "(GPU: 0, epoch: 10, iters: 96960, time: 0.008) nll: 3573.973633 \n",
      "(GPU: 0, epoch: 10, iters: 97760, time: 0.007) nll: 4516.251953 \n",
      "(GPU: 0, epoch: 10, iters: 98560, time: 0.008) nll: 3591.282715 \n",
      "(GPU: 0, epoch: 10, iters: 99360, time: 0.008) nll: 5328.367676 \n",
      "(GPU: 0, epoch: 10, iters: 100160, time: 0.008) nll: 2527.811035 \n",
      "(GPU: 0, epoch: 10, iters: 100960, time: 0.008) nll: 4384.188477 \n",
      "(GPU: 0, epoch: 10, iters: 101760, time: 0.008) nll: 3347.047852 \n",
      "(GPU: 0, epoch: 10, iters: 102560, time: 0.008) nll: 3073.671387 \n",
      "(GPU: 0, epoch: 10, iters: 103360, time: 0.008) nll: 4416.538086 \n",
      "(GPU: 0, epoch: 10, iters: 104160, time: 0.008) nll: 3825.315918 \n",
      "(GPU: 0, epoch: 10, iters: 104960, time: 0.008) nll: 4158.286621 \n",
      "(GPU: 0, epoch: 10, iters: 105760, time: 0.008) nll: 2803.686035 \n",
      "(GPU: 0, epoch: 10, iters: 106560, time: 0.008) nll: 4916.980469 \n",
      "(GPU: 0, epoch: 10, iters: 107360, time: 0.008) nll: 3658.794922 \n",
      "(GPU: 0, epoch: 10, iters: 108160, time: 0.008) nll: 3351.611328 \n",
      "(GPU: 0, epoch: 10, iters: 108960, time: 0.007) nll: 3901.316162 \n",
      "(GPU: 0, epoch: 10, iters: 109760, time: 0.008) nll: 2675.619141 \n",
      "(GPU: 0, epoch: 10, iters: 110560, time: 0.007) nll: 4437.177734 \n",
      "(GPU: 0, epoch: 10, iters: 111360, time: 0.008) nll: 4171.720703 \n",
      "(GPU: 0, epoch: 10, iters: 112160, time: 0.008) nll: 2926.486572 \n",
      "(GPU: 0, epoch: 10, iters: 112960, time: 0.008) nll: 4362.402344 \n",
      "saving the latest model (epoch 10, total_steps 1520000)\n",
      "(GPU: 0, epoch: 10, iters: 113760, time: 0.008) nll: 3165.867676 \n",
      "(GPU: 0, epoch: 10, iters: 114560, time: 0.008) nll: 3326.159180 \n",
      "(GPU: 0, epoch: 10, iters: 115360, time: 0.008) nll: 4155.008301 \n",
      "(GPU: 0, epoch: 10, iters: 116160, time: 0.008) nll: 3725.396729 \n",
      "(GPU: 0, epoch: 10, iters: 116960, time: 0.007) nll: 4772.807129 \n",
      "(GPU: 0, epoch: 10, iters: 117760, time: 0.008) nll: 3989.011475 \n",
      "(GPU: 0, epoch: 10, iters: 118560, time: 0.007) nll: 4210.957031 \n",
      "(GPU: 0, epoch: 10, iters: 119360, time: 0.008) nll: 5069.851074 \n",
      "(GPU: 0, epoch: 10, iters: 120160, time: 0.008) nll: 4586.361328 \n",
      "(GPU: 0, epoch: 10, iters: 120960, time: 0.008) nll: 3990.942383 \n",
      "(GPU: 0, epoch: 10, iters: 121760, time: 0.008) nll: 4868.059082 \n",
      "(GPU: 0, epoch: 10, iters: 122560, time: 0.008) nll: 3492.574951 \n",
      "(GPU: 0, epoch: 10, iters: 123360, time: 0.008) nll: 4096.780273 \n",
      "(GPU: 0, epoch: 10, iters: 124160, time: 0.008) nll: 4086.987305 \n",
      "(GPU: 0, epoch: 10, iters: 124960, time: 0.008) nll: 3307.170898 \n",
      "(GPU: 0, epoch: 10, iters: 125760, time: 0.008) nll: 3982.364014 \n",
      "(GPU: 0, epoch: 10, iters: 126560, time: 0.008) nll: 4386.983887 \n",
      "(GPU: 0, epoch: 10, iters: 127360, time: 0.008) nll: 3896.419434 \n",
      "(GPU: 0, epoch: 10, iters: 128160, time: 0.008) nll: 4247.611328 \n",
      "(GPU: 0, epoch: 10, iters: 128960, time: 0.008) nll: 4476.419922 \n",
      "(GPU: 0, epoch: 10, iters: 128960, time: 0.013) nll: 4341.755371 \n",
      "(GPU: 0, epoch: 10, iters: 128960, time: 0.013) nll: 5441.616211 \n",
      "(GPU: 0, epoch: 10, iters: 129760, time: 0.008) nll: 4342.545898 \n",
      "(GPU: 0, epoch: 10, iters: 130560, time: 0.008) nll: 3761.706543 \n",
      "(GPU: 0, epoch: 10, iters: 131360, time: 0.008) nll: 4051.580078 \n",
      "(GPU: 0, epoch: 10, iters: 132160, time: 0.008) nll: 4247.985352 \n",
      "(GPU: 0, epoch: 10, iters: 132960, time: 0.008) nll: 4057.605225 \n",
      "saving the latest model (epoch 10, total_steps 1540000)\n",
      "(GPU: 0, epoch: 10, iters: 133760, time: 0.008) nll: 3216.741211 \n",
      "(GPU: 0, epoch: 10, iters: 134560, time: 0.007) nll: 4808.249023 \n",
      "(GPU: 0, epoch: 10, iters: 135360, time: 0.008) nll: 5050.840820 \n",
      "(GPU: 0, epoch: 10, iters: 136160, time: 0.007) nll: 4875.502930 \n",
      "(GPU: 0, epoch: 10, iters: 136960, time: 0.008) nll: 4229.192383 \n",
      "(GPU: 0, epoch: 10, iters: 137760, time: 0.008) nll: 4325.927734 \n",
      "(GPU: 0, epoch: 10, iters: 138560, time: 0.008) nll: 4482.568848 \n",
      "(GPU: 0, epoch: 10, iters: 139360, time: 0.008) nll: 3302.886230 \n",
      "(GPU: 0, epoch: 10, iters: 140160, time: 0.008) nll: 3769.339355 \n",
      "[*] End of epoch 10 / 25 \t Time Taken: 1232 sec \n",
      "/cluster/54/streakfull/ADL4CV/Project/src/plz-autosdf/logs/bert2vqsc-text2shape-seq-LR1e-4-new-bert-2-prev_z_shape_input-try-2\n",
      "[*] learning rate = 0.0000953\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2957/4397 [13:53<06:12,  3.86it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 11, iters: 32, time: 0.004) nll: 2438.162598 \n",
      "(GPU: 0, epoch: 11, iters: 32, time: 0.004) nll: 4409.113281 \n",
      "(GPU: 0, epoch: 11, iters: 256, time: 0.008) nll: 5097.033203 \n",
      "(GPU: 0, epoch: 11, iters: 1056, time: 0.008) nll: 3807.751465 \n",
      "(GPU: 0, epoch: 11, iters: 1856, time: 0.008) nll: 5704.208984 \n",
      "(GPU: 0, epoch: 11, iters: 2656, time: 0.008) nll: 3929.872559 \n",
      "(GPU: 0, epoch: 11, iters: 3456, time: 0.008) nll: 4181.121094 \n",
      "(GPU: 0, epoch: 11, iters: 4256, time: 0.007) nll: 4404.416016 \n",
      "(GPU: 0, epoch: 11, iters: 5056, time: 0.008) nll: 3666.094727 \n",
      "(GPU: 0, epoch: 11, iters: 5856, time: 0.008) nll: 4682.366211 \n",
      "(GPU: 0, epoch: 11, iters: 6656, time: 0.008) nll: 3328.101074 \n",
      "(GPU: 0, epoch: 11, iters: 7456, time: 0.007) nll: 5072.197266 \n",
      "(GPU: 0, epoch: 11, iters: 8256, time: 0.008) nll: 5129.312500 \n",
      "(GPU: 0, epoch: 11, iters: 9056, time: 0.008) nll: 4671.008789 \n",
      "(GPU: 0, epoch: 11, iters: 9856, time: 0.008) nll: 3425.246826 \n",
      "(GPU: 0, epoch: 11, iters: 10656, time: 0.008) nll: 4156.187988 \n",
      "(GPU: 0, epoch: 11, iters: 11456, time: 0.008) nll: 2776.946045 \n",
      "(GPU: 0, epoch: 11, iters: 12256, time: 0.008) nll: 3551.712402 \n",
      "saving the latest model (epoch 11, total_steps 1560000)\n",
      "(GPU: 0, epoch: 11, iters: 13056, time: 0.008) nll: 3952.905762 \n",
      "(GPU: 0, epoch: 11, iters: 13856, time: 0.008) nll: 4035.386719 \n",
      "(GPU: 0, epoch: 11, iters: 14656, time: 0.008) nll: 3962.639160 \n",
      "(GPU: 0, epoch: 11, iters: 15456, time: 0.008) nll: 4095.708496 \n",
      "(GPU: 0, epoch: 11, iters: 16256, time: 0.008) nll: 3231.114014 \n",
      "(GPU: 0, epoch: 11, iters: 17056, time: 0.008) nll: 4929.874512 \n",
      "(GPU: 0, epoch: 11, iters: 17856, time: 0.008) nll: 4143.858398 \n",
      "(GPU: 0, epoch: 11, iters: 18656, time: 0.008) nll: 4368.000000 \n",
      "(GPU: 0, epoch: 11, iters: 19456, time: 0.008) nll: 4120.032227 \n",
      "(GPU: 0, epoch: 11, iters: 20256, time: 0.008) nll: 4538.772949 \n",
      "(GPU: 0, epoch: 11, iters: 21056, time: 0.008) nll: 4023.900879 \n",
      "(GPU: 0, epoch: 11, iters: 21856, time: 0.008) nll: 3748.520020 \n",
      "(GPU: 0, epoch: 11, iters: 22656, time: 0.008) nll: 5068.525879 \n",
      "(GPU: 0, epoch: 11, iters: 23456, time: 0.008) nll: 3275.783691 \n",
      "(GPU: 0, epoch: 11, iters: 24256, time: 0.008) nll: 4684.139160 \n",
      "(GPU: 0, epoch: 11, iters: 25056, time: 0.007) nll: 5361.029297 \n",
      "(GPU: 0, epoch: 11, iters: 25856, time: 0.008) nll: 3607.955078 \n",
      "(GPU: 0, epoch: 11, iters: 26656, time: 0.008) nll: 3245.193359 \n",
      "(GPU: 0, epoch: 11, iters: 27456, time: 0.008) nll: 4963.772461 \n",
      "(GPU: 0, epoch: 11, iters: 28256, time: 0.008) nll: 2775.871826 \n",
      "(GPU: 0, epoch: 11, iters: 29056, time: 0.008) nll: 4707.863770 \n",
      "(GPU: 0, epoch: 11, iters: 29856, time: 0.007) nll: 4261.663574 \n",
      "(GPU: 0, epoch: 11, iters: 30656, time: 0.008) nll: 3001.637207 \n",
      "(GPU: 0, epoch: 11, iters: 31456, time: 0.008) nll: 4848.891602 \n",
      "(GPU: 0, epoch: 11, iters: 32256, time: 0.008) nll: 3344.842041 \n",
      "saving the latest model (epoch 11, total_steps 1580000)\n",
      "(GPU: 0, epoch: 11, iters: 33056, time: 0.008) nll: 3826.945312 \n",
      "(GPU: 0, epoch: 11, iters: 33856, time: 0.008) nll: 4334.170898 \n",
      "(GPU: 0, epoch: 11, iters: 34656, time: 0.008) nll: 3726.082520 \n",
      "(GPU: 0, epoch: 11, iters: 35456, time: 0.008) nll: 4336.017090 \n",
      "(GPU: 0, epoch: 11, iters: 36256, time: 0.008) nll: 3857.956543 \n",
      "(GPU: 0, epoch: 11, iters: 37056, time: 0.008) nll: 2751.401123 \n",
      "(GPU: 0, epoch: 11, iters: 37856, time: 0.008) nll: 3466.680176 \n",
      "(GPU: 0, epoch: 11, iters: 38656, time: 0.008) nll: 4446.641602 \n",
      "(GPU: 0, epoch: 11, iters: 39456, time: 0.008) nll: 4823.702637 \n",
      "(GPU: 0, epoch: 11, iters: 40256, time: 0.008) nll: 4011.459961 \n",
      "(GPU: 0, epoch: 11, iters: 41056, time: 0.007) nll: 4718.298828 \n",
      "(GPU: 0, epoch: 11, iters: 41856, time: 0.008) nll: 3677.660156 \n",
      "(GPU: 0, epoch: 11, iters: 42656, time: 0.008) nll: 3771.216797 \n",
      "(GPU: 0, epoch: 11, iters: 43456, time: 0.008) nll: 4496.504395 \n",
      "(GPU: 0, epoch: 11, iters: 44256, time: 0.008) nll: 3412.010986 \n",
      "(GPU: 0, epoch: 11, iters: 45056, time: 0.008) nll: 3260.465820 \n",
      "(GPU: 0, epoch: 11, iters: 45856, time: 0.008) nll: 3995.195557 \n",
      "(GPU: 0, epoch: 11, iters: 46656, time: 0.008) nll: 2572.279297 \n",
      "(GPU: 0, epoch: 11, iters: 47456, time: 0.007) nll: 4153.780273 \n",
      "(GPU: 0, epoch: 11, iters: 48256, time: 0.008) nll: 3922.756592 \n",
      "(GPU: 0, epoch: 11, iters: 49056, time: 0.008) nll: 4563.968750 \n",
      "(GPU: 0, epoch: 11, iters: 49856, time: 0.008) nll: 5119.562012 \n",
      "(GPU: 0, epoch: 11, iters: 50656, time: 0.008) nll: 4461.303711 \n",
      "(GPU: 0, epoch: 11, iters: 51456, time: 0.008) nll: 3194.140869 \n",
      "(GPU: 0, epoch: 11, iters: 52256, time: 0.008) nll: 3343.463623 \n",
      "saving the latest model (epoch 11, total_steps 1600000)\n",
      "(GPU: 0, epoch: 11, iters: 53056, time: 0.008) nll: 2596.252930 \n",
      "(GPU: 0, epoch: 11, iters: 53856, time: 0.008) nll: 3921.794922 \n",
      "(GPU: 0, epoch: 11, iters: 54656, time: 0.008) nll: 3082.942383 \n",
      "(GPU: 0, epoch: 11, iters: 55456, time: 0.007) nll: 4458.406250 \n",
      "(GPU: 0, epoch: 11, iters: 56256, time: 0.008) nll: 4467.927246 \n",
      "(GPU: 0, epoch: 11, iters: 57056, time: 0.007) nll: 5178.047852 \n",
      "(GPU: 0, epoch: 11, iters: 57856, time: 0.008) nll: 4087.463867 \n",
      "(GPU: 0, epoch: 11, iters: 58656, time: 0.008) nll: 3519.754395 \n",
      "(GPU: 0, epoch: 11, iters: 59456, time: 0.008) nll: 4435.848145 \n",
      "(GPU: 0, epoch: 11, iters: 60256, time: 0.008) nll: 4805.007812 \n",
      "(GPU: 0, epoch: 11, iters: 61056, time: 0.008) nll: 5663.866699 \n",
      "(GPU: 0, epoch: 11, iters: 61856, time: 0.008) nll: 3568.810059 \n",
      "(GPU: 0, epoch: 11, iters: 62656, time: 0.008) nll: 4048.164551 \n",
      "(GPU: 0, epoch: 11, iters: 63456, time: 0.008) nll: 4397.374023 \n",
      "(GPU: 0, epoch: 11, iters: 64256, time: 0.008) nll: 3508.602295 \n",
      "(GPU: 0, epoch: 11, iters: 65056, time: 0.008) nll: 3804.215088 \n",
      "(GPU: 0, epoch: 11, iters: 65856, time: 0.008) nll: 4950.140137 \n",
      "(GPU: 0, epoch: 11, iters: 66656, time: 0.007) nll: 3692.252197 \n",
      "(GPU: 0, epoch: 11, iters: 67456, time: 0.008) nll: 4320.809082 \n",
      "(GPU: 0, epoch: 11, iters: 68256, time: 0.007) nll: 3523.023682 \n",
      "(GPU: 0, epoch: 11, iters: 69056, time: 0.008) nll: 4257.305664 \n",
      "(GPU: 0, epoch: 11, iters: 69856, time: 0.007) nll: 3384.331543 \n",
      "(GPU: 0, epoch: 11, iters: 70656, time: 0.008) nll: 4294.081543 \n",
      "(GPU: 0, epoch: 11, iters: 71456, time: 0.008) nll: 4313.441406 \n",
      "(GPU: 0, epoch: 11, iters: 72256, time: 0.008) nll: 3640.355957 \n",
      "saving the latest model (epoch 11, total_steps 1620000)\n",
      "(GPU: 0, epoch: 11, iters: 73056, time: 0.007) nll: 4180.140137 \n",
      "(GPU: 0, epoch: 11, iters: 73856, time: 0.008) nll: 5537.867188 \n",
      "(GPU: 0, epoch: 11, iters: 74656, time: 0.007) nll: 4766.842773 \n",
      "(GPU: 0, epoch: 11, iters: 75456, time: 0.008) nll: 4175.171875 \n",
      "(GPU: 0, epoch: 11, iters: 76256, time: 0.008) nll: 3732.023682 \n",
      "(GPU: 0, epoch: 11, iters: 77056, time: 0.008) nll: 3931.669678 \n",
      "(GPU: 0, epoch: 11, iters: 77856, time: 0.008) nll: 4379.239258 \n",
      "(GPU: 0, epoch: 11, iters: 78656, time: 0.008) nll: 4601.025391 \n",
      "(GPU: 0, epoch: 11, iters: 79456, time: 0.007) nll: 3791.694824 \n",
      "(GPU: 0, epoch: 11, iters: 80256, time: 0.008) nll: 2960.555420 \n",
      "(GPU: 0, epoch: 11, iters: 81056, time: 0.008) nll: 2324.840332 \n",
      "(GPU: 0, epoch: 11, iters: 81856, time: 0.008) nll: 3424.302246 \n",
      "(GPU: 0, epoch: 11, iters: 82656, time: 0.008) nll: 3913.329590 \n",
      "(GPU: 0, epoch: 11, iters: 83456, time: 0.008) nll: 3485.401123 \n",
      "(GPU: 0, epoch: 11, iters: 84256, time: 0.007) nll: 4270.524902 \n",
      "(GPU: 0, epoch: 11, iters: 84256, time: 0.012) nll: 4198.926270 \n",
      "(GPU: 0, epoch: 11, iters: 84256, time: 0.012) nll: 4520.199219 \n",
      "(GPU: 0, epoch: 11, iters: 85056, time: 0.008) nll: 4413.843750 \n",
      "(GPU: 0, epoch: 11, iters: 85856, time: 0.008) nll: 5754.043457 \n",
      "(GPU: 0, epoch: 11, iters: 86656, time: 0.008) nll: 3733.194824 \n",
      "(GPU: 0, epoch: 11, iters: 87456, time: 0.008) nll: 3888.350830 \n",
      "(GPU: 0, epoch: 11, iters: 88256, time: 0.008) nll: 3777.655762 \n",
      "(GPU: 0, epoch: 11, iters: 89056, time: 0.008) nll: 4762.607910 \n",
      "(GPU: 0, epoch: 11, iters: 89856, time: 0.008) nll: 4364.213867 \n",
      "(GPU: 0, epoch: 11, iters: 90656, time: 0.008) nll: 3346.689697 \n",
      "(GPU: 0, epoch: 11, iters: 91456, time: 0.008) nll: 2801.956299 \n",
      "(GPU: 0, epoch: 11, iters: 92256, time: 0.007) nll: 3194.138184 \n",
      "saving the latest model (epoch 11, total_steps 1640000)\n",
      "(GPU: 0, epoch: 11, iters: 93056, time: 0.008) nll: 2932.844238 \n",
      "(GPU: 0, epoch: 11, iters: 93856, time: 0.008) nll: 4510.058594 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4397/4397 [20:33<00:00,  3.57it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 11, iters: 94656, time: 0.008) nll: 2490.835449 \n",
      "(GPU: 0, epoch: 11, iters: 95456, time: 0.008) nll: 3356.585449 \n",
      "(GPU: 0, epoch: 11, iters: 96256, time: 0.008) nll: 3607.739990 \n",
      "(GPU: 0, epoch: 11, iters: 97056, time: 0.008) nll: 2870.132568 \n",
      "(GPU: 0, epoch: 11, iters: 97856, time: 0.008) nll: 2953.782959 \n",
      "(GPU: 0, epoch: 11, iters: 98656, time: 0.007) nll: 2984.209717 \n",
      "(GPU: 0, epoch: 11, iters: 99456, time: 0.008) nll: 3273.606445 \n",
      "(GPU: 0, epoch: 11, iters: 100256, time: 0.008) nll: 2481.538574 \n",
      "(GPU: 0, epoch: 11, iters: 101056, time: 0.008) nll: 4555.521973 \n",
      "(GPU: 0, epoch: 11, iters: 101856, time: 0.008) nll: 3605.379883 \n",
      "(GPU: 0, epoch: 11, iters: 102656, time: 0.008) nll: 4123.984375 \n",
      "(GPU: 0, epoch: 11, iters: 103456, time: 0.008) nll: 3482.688965 \n",
      "(GPU: 0, epoch: 11, iters: 104256, time: 0.008) nll: 5110.548828 \n",
      "(GPU: 0, epoch: 11, iters: 105056, time: 0.007) nll: 5245.264648 \n",
      "(GPU: 0, epoch: 11, iters: 105856, time: 0.008) nll: 4533.736328 \n",
      "(GPU: 0, epoch: 11, iters: 106656, time: 0.007) nll: 3918.137939 \n",
      "(GPU: 0, epoch: 11, iters: 107456, time: 0.008) nll: 5007.633789 \n",
      "(GPU: 0, epoch: 11, iters: 108256, time: 0.008) nll: 2672.570557 \n",
      "(GPU: 0, epoch: 11, iters: 109056, time: 0.008) nll: 4043.172363 \n",
      "(GPU: 0, epoch: 11, iters: 109856, time: 0.008) nll: 2914.511719 \n",
      "(GPU: 0, epoch: 11, iters: 110656, time: 0.008) nll: 3889.244629 \n",
      "(GPU: 0, epoch: 11, iters: 111456, time: 0.008) nll: 4843.442871 \n",
      "(GPU: 0, epoch: 11, iters: 112256, time: 0.008) nll: 3777.126953 \n",
      "saving the latest model (epoch 11, total_steps 1660000)\n",
      "(GPU: 0, epoch: 11, iters: 113056, time: 0.007) nll: 3725.461426 \n",
      "(GPU: 0, epoch: 11, iters: 113856, time: 0.008) nll: 3475.686523 \n",
      "(GPU: 0, epoch: 11, iters: 114656, time: 0.008) nll: 2913.302734 \n",
      "(GPU: 0, epoch: 11, iters: 115456, time: 0.008) nll: 3067.905762 \n",
      "(GPU: 0, epoch: 11, iters: 116256, time: 0.008) nll: 3445.292725 \n",
      "(GPU: 0, epoch: 11, iters: 117056, time: 0.008) nll: 3256.714844 \n",
      "(GPU: 0, epoch: 11, iters: 117856, time: 0.008) nll: 4184.790527 \n",
      "(GPU: 0, epoch: 11, iters: 118656, time: 0.008) nll: 5284.348633 \n",
      "(GPU: 0, epoch: 11, iters: 119456, time: 0.008) nll: 5445.110352 \n",
      "(GPU: 0, epoch: 11, iters: 120256, time: 0.008) nll: 3236.791016 \n",
      "(GPU: 0, epoch: 11, iters: 121056, time: 0.008) nll: 3411.458252 \n",
      "(GPU: 0, epoch: 11, iters: 121856, time: 0.008) nll: 3400.835938 \n",
      "(GPU: 0, epoch: 11, iters: 122656, time: 0.008) nll: 3779.224121 \n",
      "(GPU: 0, epoch: 11, iters: 123456, time: 0.008) nll: 5360.735352 \n",
      "(GPU: 0, epoch: 11, iters: 124256, time: 0.008) nll: 3248.608398 \n",
      "(GPU: 0, epoch: 11, iters: 125056, time: 0.008) nll: 4904.393555 \n",
      "(GPU: 0, epoch: 11, iters: 125856, time: 0.008) nll: 2378.609375 \n",
      "(GPU: 0, epoch: 11, iters: 126656, time: 0.008) nll: 4226.481934 \n",
      "(GPU: 0, epoch: 11, iters: 127456, time: 0.008) nll: 3207.913574 \n",
      "(GPU: 0, epoch: 11, iters: 128256, time: 0.008) nll: 4629.742188 \n",
      "(GPU: 0, epoch: 11, iters: 129056, time: 0.008) nll: 3904.588867 \n",
      "(GPU: 0, epoch: 11, iters: 129856, time: 0.008) nll: 2772.392090 \n",
      "(GPU: 0, epoch: 11, iters: 130656, time: 0.008) nll: 4256.667969 \n",
      "(GPU: 0, epoch: 11, iters: 131456, time: 0.008) nll: 3211.406006 \n",
      "(GPU: 0, epoch: 11, iters: 132256, time: 0.008) nll: 3433.302734 \n",
      "saving the latest model (epoch 11, total_steps 1680000)\n",
      "(GPU: 0, epoch: 11, iters: 133056, time: 0.008) nll: 3732.403564 \n",
      "(GPU: 0, epoch: 11, iters: 133856, time: 0.007) nll: 3253.503906 \n",
      "(GPU: 0, epoch: 11, iters: 134656, time: 0.008) nll: 3355.610840 \n",
      "(GPU: 0, epoch: 11, iters: 135456, time: 0.007) nll: 4110.393066 \n",
      "(GPU: 0, epoch: 11, iters: 136256, time: 0.008) nll: 3403.163086 \n",
      "(GPU: 0, epoch: 11, iters: 137056, time: 0.007) nll: 4161.925293 \n",
      "(GPU: 0, epoch: 11, iters: 137856, time: 0.008) nll: 4645.006836 \n",
      "(GPU: 0, epoch: 11, iters: 138656, time: 0.008) nll: 4934.936035 \n",
      "(GPU: 0, epoch: 11, iters: 139456, time: 0.008) nll: 3852.451172 \n",
      "(GPU: 0, epoch: 11, iters: 140256, time: 0.008) nll: 2224.581055 \n",
      "[*] End of epoch 11 / 25 \t Time Taken: 1234 sec \n",
      "/cluster/54/streakfull/ADL4CV/Project/src/plz-autosdf/logs/bert2vqsc-text2shape-seq-LR1e-4-new-bert-2-prev_z_shape_input-try-2\n",
      "[*] learning rate = 0.0000913\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2960/4397 [13:52<06:10,  3.88it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 12, iters: 32, time: 0.004) nll: 4312.609863 \n",
      "(GPU: 0, epoch: 12, iters: 32, time: 0.004) nll: 3751.151367 \n",
      "(GPU: 0, epoch: 12, iters: 352, time: 0.007) nll: 4185.856445 \n",
      "(GPU: 0, epoch: 12, iters: 1152, time: 0.008) nll: 4380.615234 \n",
      "(GPU: 0, epoch: 12, iters: 1952, time: 0.008) nll: 4007.654053 \n",
      "(GPU: 0, epoch: 12, iters: 2752, time: 0.008) nll: 3843.536133 \n",
      "(GPU: 0, epoch: 12, iters: 3552, time: 0.008) nll: 6165.939453 \n",
      "(GPU: 0, epoch: 12, iters: 4352, time: 0.008) nll: 4126.461914 \n",
      "(GPU: 0, epoch: 12, iters: 5152, time: 0.008) nll: 3971.281738 \n",
      "(GPU: 0, epoch: 12, iters: 5952, time: 0.008) nll: 2033.831543 \n",
      "(GPU: 0, epoch: 12, iters: 6752, time: 0.008) nll: 4582.572266 \n",
      "(GPU: 0, epoch: 12, iters: 7552, time: 0.008) nll: 3248.318848 \n",
      "(GPU: 0, epoch: 12, iters: 8352, time: 0.008) nll: 3605.021973 \n",
      "(GPU: 0, epoch: 12, iters: 9152, time: 0.008) nll: 3973.843018 \n",
      "(GPU: 0, epoch: 12, iters: 9952, time: 0.008) nll: 3542.667969 \n",
      "(GPU: 0, epoch: 12, iters: 10752, time: 0.008) nll: 3559.197754 \n",
      "(GPU: 0, epoch: 12, iters: 11552, time: 0.008) nll: 4105.442383 \n",
      "saving the latest model (epoch 12, total_steps 1700000)\n",
      "(GPU: 0, epoch: 12, iters: 12352, time: 0.008) nll: 3687.900391 \n",
      "(GPU: 0, epoch: 12, iters: 13152, time: 0.007) nll: 3691.729248 \n",
      "(GPU: 0, epoch: 12, iters: 13952, time: 0.008) nll: 2987.442871 \n",
      "(GPU: 0, epoch: 12, iters: 14752, time: 0.008) nll: 3529.900146 \n",
      "(GPU: 0, epoch: 12, iters: 15552, time: 0.008) nll: 4991.185547 \n",
      "(GPU: 0, epoch: 12, iters: 16352, time: 0.008) nll: 3132.730469 \n",
      "(GPU: 0, epoch: 12, iters: 17152, time: 0.008) nll: 4481.414062 \n",
      "(GPU: 0, epoch: 12, iters: 17952, time: 0.007) nll: 4328.983398 \n",
      "(GPU: 0, epoch: 12, iters: 18752, time: 0.008) nll: 4622.705078 \n",
      "(GPU: 0, epoch: 12, iters: 19552, time: 0.008) nll: 4380.358887 \n",
      "(GPU: 0, epoch: 12, iters: 20352, time: 0.008) nll: 4184.770508 \n",
      "(GPU: 0, epoch: 12, iters: 21152, time: 0.008) nll: 5098.833008 \n",
      "(GPU: 0, epoch: 12, iters: 21952, time: 0.008) nll: 5069.627441 \n",
      "(GPU: 0, epoch: 12, iters: 22752, time: 0.007) nll: 3968.297363 \n",
      "(GPU: 0, epoch: 12, iters: 23552, time: 0.008) nll: 2839.453613 \n",
      "(GPU: 0, epoch: 12, iters: 24352, time: 0.008) nll: 3935.236816 \n",
      "(GPU: 0, epoch: 12, iters: 25152, time: 0.008) nll: 4249.426758 \n",
      "(GPU: 0, epoch: 12, iters: 25952, time: 0.008) nll: 4193.904297 \n",
      "(GPU: 0, epoch: 12, iters: 26752, time: 0.008) nll: 4541.484375 \n",
      "(GPU: 0, epoch: 12, iters: 27552, time: 0.008) nll: 4496.233398 \n",
      "(GPU: 0, epoch: 12, iters: 28352, time: 0.008) nll: 3371.580078 \n",
      "(GPU: 0, epoch: 12, iters: 29152, time: 0.008) nll: 4373.333984 \n",
      "(GPU: 0, epoch: 12, iters: 29952, time: 0.008) nll: 3686.025635 \n",
      "(GPU: 0, epoch: 12, iters: 30752, time: 0.008) nll: 4927.679688 \n",
      "(GPU: 0, epoch: 12, iters: 31552, time: 0.008) nll: 5444.520020 \n",
      "saving the latest model (epoch 12, total_steps 1720000)\n",
      "(GPU: 0, epoch: 12, iters: 32352, time: 0.008) nll: 5074.485352 \n",
      "(GPU: 0, epoch: 12, iters: 33152, time: 0.008) nll: 4879.768555 \n",
      "(GPU: 0, epoch: 12, iters: 33952, time: 0.007) nll: 4602.895996 \n",
      "(GPU: 0, epoch: 12, iters: 34752, time: 0.008) nll: 3915.882324 \n",
      "(GPU: 0, epoch: 12, iters: 35552, time: 0.007) nll: 2956.295898 \n",
      "(GPU: 0, epoch: 12, iters: 36352, time: 0.008) nll: 3775.854004 \n",
      "(GPU: 0, epoch: 12, iters: 37152, time: 0.008) nll: 4831.646484 \n",
      "(GPU: 0, epoch: 12, iters: 37952, time: 0.008) nll: 4347.301270 \n",
      "(GPU: 0, epoch: 12, iters: 38752, time: 0.008) nll: 4039.972168 \n",
      "(GPU: 0, epoch: 12, iters: 39552, time: 0.008) nll: 5233.038574 \n",
      "(GPU: 0, epoch: 12, iters: 39552, time: 0.013) nll: 5186.696289 \n",
      "(GPU: 0, epoch: 12, iters: 39552, time: 0.013) nll: 3702.875732 \n",
      "(GPU: 0, epoch: 12, iters: 40352, time: 0.008) nll: 4241.136230 \n",
      "(GPU: 0, epoch: 12, iters: 41152, time: 0.008) nll: 4540.348633 \n",
      "(GPU: 0, epoch: 12, iters: 41952, time: 0.007) nll: 2654.788574 \n",
      "(GPU: 0, epoch: 12, iters: 42752, time: 0.008) nll: 4035.692383 \n",
      "(GPU: 0, epoch: 12, iters: 43552, time: 0.008) nll: 3876.684814 \n",
      "(GPU: 0, epoch: 12, iters: 44352, time: 0.008) nll: 3500.916992 \n",
      "(GPU: 0, epoch: 12, iters: 45152, time: 0.007) nll: 4417.959473 \n",
      "(GPU: 0, epoch: 12, iters: 45952, time: 0.008) nll: 4160.950684 \n",
      "(GPU: 0, epoch: 12, iters: 46752, time: 0.008) nll: 5217.934570 \n",
      "(GPU: 0, epoch: 12, iters: 47552, time: 0.008) nll: 3941.994141 \n",
      "(GPU: 0, epoch: 12, iters: 48352, time: 0.008) nll: 3700.000488 \n",
      "(GPU: 0, epoch: 12, iters: 49152, time: 0.008) nll: 3278.813232 \n",
      "(GPU: 0, epoch: 12, iters: 49952, time: 0.008) nll: 3738.185303 \n",
      "(GPU: 0, epoch: 12, iters: 50752, time: 0.008) nll: 3420.088867 \n",
      "(GPU: 0, epoch: 12, iters: 51552, time: 0.008) nll: 3912.701660 \n",
      "saving the latest model (epoch 12, total_steps 1740000)\n",
      "(GPU: 0, epoch: 12, iters: 52352, time: 0.008) nll: 4018.915039 \n",
      "(GPU: 0, epoch: 12, iters: 53152, time: 0.007) nll: 3379.593262 \n",
      "(GPU: 0, epoch: 12, iters: 53952, time: 0.008) nll: 3306.942383 \n",
      "(GPU: 0, epoch: 12, iters: 54752, time: 0.008) nll: 5552.346680 \n",
      "(GPU: 0, epoch: 12, iters: 55552, time: 0.008) nll: 3325.378418 \n",
      "(GPU: 0, epoch: 12, iters: 56352, time: 0.008) nll: 3739.224609 \n",
      "(GPU: 0, epoch: 12, iters: 57152, time: 0.008) nll: 3432.042480 \n",
      "(GPU: 0, epoch: 12, iters: 57952, time: 0.007) nll: 5028.971191 \n",
      "(GPU: 0, epoch: 12, iters: 58752, time: 0.008) nll: 3490.781738 \n",
      "(GPU: 0, epoch: 12, iters: 59552, time: 0.008) nll: 5844.251465 \n",
      "(GPU: 0, epoch: 12, iters: 60352, time: 0.008) nll: 4748.517578 \n",
      "(GPU: 0, epoch: 12, iters: 61152, time: 0.008) nll: 4054.499023 \n",
      "(GPU: 0, epoch: 12, iters: 61952, time: 0.008) nll: 3414.123779 \n",
      "(GPU: 0, epoch: 12, iters: 62752, time: 0.007) nll: 3995.957520 \n",
      "(GPU: 0, epoch: 12, iters: 63552, time: 0.008) nll: 4931.777344 \n",
      "(GPU: 0, epoch: 12, iters: 64352, time: 0.008) nll: 3364.940674 \n",
      "(GPU: 0, epoch: 12, iters: 65152, time: 0.008) nll: 3603.839355 \n",
      "(GPU: 0, epoch: 12, iters: 65952, time: 0.008) nll: 2991.233154 \n",
      "(GPU: 0, epoch: 12, iters: 66752, time: 0.008) nll: 4802.670898 \n",
      "(GPU: 0, epoch: 12, iters: 67552, time: 0.008) nll: 5061.193848 \n",
      "(GPU: 0, epoch: 12, iters: 68352, time: 0.008) nll: 2950.193359 \n",
      "(GPU: 0, epoch: 12, iters: 69152, time: 0.007) nll: 4691.850586 \n",
      "(GPU: 0, epoch: 12, iters: 69952, time: 0.008) nll: 3716.195312 \n",
      "(GPU: 0, epoch: 12, iters: 70752, time: 0.008) nll: 4458.312012 \n",
      "(GPU: 0, epoch: 12, iters: 71552, time: 0.008) nll: 3296.604980 \n",
      "saving the latest model (epoch 12, total_steps 1760000)\n",
      "(GPU: 0, epoch: 12, iters: 72352, time: 0.008) nll: 4147.288086 \n",
      "(GPU: 0, epoch: 12, iters: 73152, time: 0.008) nll: 3416.778076 \n",
      "(GPU: 0, epoch: 12, iters: 73952, time: 0.008) nll: 3532.622070 \n",
      "(GPU: 0, epoch: 12, iters: 74752, time: 0.008) nll: 2089.645996 \n",
      "(GPU: 0, epoch: 12, iters: 75552, time: 0.008) nll: 4051.275146 \n",
      "(GPU: 0, epoch: 12, iters: 76352, time: 0.008) nll: 5561.758301 \n",
      "(GPU: 0, epoch: 12, iters: 77152, time: 0.008) nll: 3239.181396 \n",
      "(GPU: 0, epoch: 12, iters: 77952, time: 0.008) nll: 5059.484863 \n",
      "(GPU: 0, epoch: 12, iters: 78752, time: 0.008) nll: 3308.969727 \n",
      "(GPU: 0, epoch: 12, iters: 79552, time: 0.008) nll: 5056.479492 \n",
      "(GPU: 0, epoch: 12, iters: 80352, time: 0.008) nll: 4117.833984 \n",
      "(GPU: 0, epoch: 12, iters: 81152, time: 0.008) nll: 4151.750000 \n",
      "(GPU: 0, epoch: 12, iters: 81952, time: 0.007) nll: 4551.026367 \n",
      "(GPU: 0, epoch: 12, iters: 82752, time: 0.008) nll: 4682.993652 \n",
      "(GPU: 0, epoch: 12, iters: 83552, time: 0.008) nll: 3078.717773 \n",
      "(GPU: 0, epoch: 12, iters: 84352, time: 0.008) nll: 3343.486084 \n",
      "(GPU: 0, epoch: 12, iters: 85152, time: 0.008) nll: 4653.669434 \n",
      "(GPU: 0, epoch: 12, iters: 85952, time: 0.008) nll: 3795.011719 \n",
      "(GPU: 0, epoch: 12, iters: 86752, time: 0.007) nll: 3400.623047 \n",
      "(GPU: 0, epoch: 12, iters: 87552, time: 0.008) nll: 2586.355957 \n",
      "(GPU: 0, epoch: 12, iters: 88352, time: 0.008) nll: 3248.138672 \n",
      "(GPU: 0, epoch: 12, iters: 89152, time: 0.008) nll: 4197.478516 \n",
      "(GPU: 0, epoch: 12, iters: 89952, time: 0.007) nll: 3639.846680 \n",
      "(GPU: 0, epoch: 12, iters: 90752, time: 0.008) nll: 3389.058594 \n",
      "(GPU: 0, epoch: 12, iters: 91552, time: 0.007) nll: 3577.449951 \n",
      "saving the latest model (epoch 12, total_steps 1780000)\n",
      "(GPU: 0, epoch: 12, iters: 92352, time: 0.008) nll: 4343.529297 \n",
      "(GPU: 0, epoch: 12, iters: 93152, time: 0.007) nll: 4276.299316 \n",
      "(GPU: 0, epoch: 12, iters: 93952, time: 0.008) nll: 4036.179688 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4397/4397 [20:31<00:00,  3.57it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 12, iters: 94752, time: 0.008) nll: 3533.705078 \n",
      "(GPU: 0, epoch: 12, iters: 95552, time: 0.008) nll: 2776.111572 \n",
      "(GPU: 0, epoch: 12, iters: 96352, time: 0.007) nll: 3426.311279 \n",
      "(GPU: 0, epoch: 12, iters: 97152, time: 0.008) nll: 3885.530762 \n",
      "(GPU: 0, epoch: 12, iters: 97952, time: 0.008) nll: 4037.250488 \n",
      "(GPU: 0, epoch: 12, iters: 98752, time: 0.008) nll: 3593.419189 \n",
      "(GPU: 0, epoch: 12, iters: 99552, time: 0.008) nll: 3079.710205 \n",
      "(GPU: 0, epoch: 12, iters: 100352, time: 0.008) nll: 5162.277344 \n",
      "(GPU: 0, epoch: 12, iters: 101152, time: 0.007) nll: 3236.473389 \n",
      "(GPU: 0, epoch: 12, iters: 101952, time: 0.008) nll: 4526.367188 \n",
      "(GPU: 0, epoch: 12, iters: 102752, time: 0.008) nll: 3243.532715 \n",
      "(GPU: 0, epoch: 12, iters: 103552, time: 0.008) nll: 4174.113281 \n",
      "(GPU: 0, epoch: 12, iters: 104352, time: 0.008) nll: 3722.471680 \n",
      "(GPU: 0, epoch: 12, iters: 105152, time: 0.008) nll: 4554.050293 \n",
      "(GPU: 0, epoch: 12, iters: 105952, time: 0.007) nll: 3724.861328 \n",
      "(GPU: 0, epoch: 12, iters: 106752, time: 0.008) nll: 3924.046875 \n",
      "(GPU: 0, epoch: 12, iters: 107552, time: 0.008) nll: 3613.083984 \n",
      "(GPU: 0, epoch: 12, iters: 108352, time: 0.008) nll: 4411.833496 \n",
      "(GPU: 0, epoch: 12, iters: 109152, time: 0.007) nll: 4543.034180 \n",
      "(GPU: 0, epoch: 12, iters: 109952, time: 0.008) nll: 3436.867188 \n",
      "(GPU: 0, epoch: 12, iters: 110752, time: 0.008) nll: 4858.774414 \n",
      "(GPU: 0, epoch: 12, iters: 111552, time: 0.008) nll: 5197.550781 \n",
      "saving the latest model (epoch 12, total_steps 1800000)\n",
      "(GPU: 0, epoch: 12, iters: 112352, time: 0.007) nll: 3064.512451 \n",
      "(GPU: 0, epoch: 12, iters: 113152, time: 0.008) nll: 3689.275635 \n",
      "(GPU: 0, epoch: 12, iters: 113952, time: 0.008) nll: 3161.968750 \n",
      "(GPU: 0, epoch: 12, iters: 114752, time: 0.008) nll: 2989.933350 \n",
      "(GPU: 0, epoch: 12, iters: 115552, time: 0.008) nll: 3721.392090 \n",
      "(GPU: 0, epoch: 12, iters: 116352, time: 0.008) nll: 2971.361816 \n",
      "(GPU: 0, epoch: 12, iters: 117152, time: 0.008) nll: 2653.050781 \n",
      "(GPU: 0, epoch: 12, iters: 117952, time: 0.008) nll: 4567.498535 \n",
      "(GPU: 0, epoch: 12, iters: 118752, time: 0.008) nll: 3698.968262 \n",
      "(GPU: 0, epoch: 12, iters: 119552, time: 0.008) nll: 4190.713867 \n",
      "(GPU: 0, epoch: 12, iters: 120352, time: 0.007) nll: 3535.324951 \n",
      "(GPU: 0, epoch: 12, iters: 121152, time: 0.008) nll: 4167.180176 \n",
      "(GPU: 0, epoch: 12, iters: 121952, time: 0.008) nll: 3313.921875 \n",
      "(GPU: 0, epoch: 12, iters: 122752, time: 0.008) nll: 3097.440674 \n",
      "(GPU: 0, epoch: 12, iters: 123552, time: 0.007) nll: 5285.527832 \n",
      "(GPU: 0, epoch: 12, iters: 124352, time: 0.008) nll: 3639.196533 \n",
      "(GPU: 0, epoch: 12, iters: 125152, time: 0.008) nll: 2532.840820 \n",
      "(GPU: 0, epoch: 12, iters: 125952, time: 0.008) nll: 5043.873047 \n",
      "(GPU: 0, epoch: 12, iters: 126752, time: 0.007) nll: 3299.607422 \n",
      "(GPU: 0, epoch: 12, iters: 127552, time: 0.008) nll: 2701.575684 \n",
      "(GPU: 0, epoch: 12, iters: 128352, time: 0.007) nll: 4159.547363 \n",
      "(GPU: 0, epoch: 12, iters: 129152, time: 0.008) nll: 4309.159180 \n",
      "(GPU: 0, epoch: 12, iters: 129952, time: 0.007) nll: 5083.687500 \n",
      "(GPU: 0, epoch: 12, iters: 130752, time: 0.008) nll: 4416.389160 \n",
      "(GPU: 0, epoch: 12, iters: 131552, time: 0.008) nll: 4759.062500 \n",
      "saving the latest model (epoch 12, total_steps 1820000)\n",
      "(GPU: 0, epoch: 12, iters: 132352, time: 0.008) nll: 3279.616211 \n",
      "(GPU: 0, epoch: 12, iters: 133152, time: 0.007) nll: 4209.167969 \n",
      "(GPU: 0, epoch: 12, iters: 133952, time: 0.008) nll: 3056.512939 \n",
      "(GPU: 0, epoch: 12, iters: 134752, time: 0.007) nll: 4592.208008 \n",
      "(GPU: 0, epoch: 12, iters: 135552, time: 0.008) nll: 3636.721680 \n",
      "(GPU: 0, epoch: 12, iters: 135552, time: 0.013) nll: 3576.137207 \n",
      "(GPU: 0, epoch: 12, iters: 135552, time: 0.013) nll: 4473.395508 \n",
      "(GPU: 0, epoch: 12, iters: 136352, time: 0.008) nll: 3902.479004 \n",
      "(GPU: 0, epoch: 12, iters: 137152, time: 0.008) nll: 4841.051758 \n",
      "(GPU: 0, epoch: 12, iters: 137952, time: 0.008) nll: 3532.183350 \n",
      "(GPU: 0, epoch: 12, iters: 138752, time: 0.008) nll: 4131.320312 \n",
      "(GPU: 0, epoch: 12, iters: 139552, time: 0.008) nll: 4557.917969 \n",
      "(GPU: 0, epoch: 12, iters: 140352, time: 0.008) nll: 3597.932617 \n",
      "saving the model at the end of epoch 12, iters 1829152\n",
      "([test] GPU: 0, epoch: 12) \n",
      "OrderedDict()\n",
      "[*] End of epoch 12 / 25 \t Time Taken: 1258 sec \n",
      "/cluster/54/streakfull/ADL4CV/Project/src/plz-autosdf/logs/bert2vqsc-text2shape-seq-LR1e-4-new-bert-2-prev_z_shape_input-try-2\n",
      "[*] learning rate = 0.0000877\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2963/4397 [13:51<06:08,  3.89it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 13, iters: 32, time: 0.004) nll: 3761.392822 \n",
      "(GPU: 0, epoch: 13, iters: 32, time: 0.004) nll: 3522.392090 \n",
      "(GPU: 0, epoch: 13, iters: 448, time: 0.008) nll: 2601.737793 \n",
      "(GPU: 0, epoch: 13, iters: 1248, time: 0.008) nll: 3282.300781 \n",
      "(GPU: 0, epoch: 13, iters: 2048, time: 0.008) nll: 4163.822754 \n",
      "(GPU: 0, epoch: 13, iters: 2848, time: 0.008) nll: 4845.444336 \n",
      "(GPU: 0, epoch: 13, iters: 3648, time: 0.008) nll: 2373.440918 \n",
      "(GPU: 0, epoch: 13, iters: 4448, time: 0.008) nll: 3843.314941 \n",
      "(GPU: 0, epoch: 13, iters: 5248, time: 0.008) nll: 3748.837646 \n",
      "(GPU: 0, epoch: 13, iters: 6048, time: 0.008) nll: 2896.247803 \n",
      "(GPU: 0, epoch: 13, iters: 6848, time: 0.008) nll: 4120.883789 \n",
      "(GPU: 0, epoch: 13, iters: 7648, time: 0.008) nll: 3861.744629 \n",
      "(GPU: 0, epoch: 13, iters: 8448, time: 0.008) nll: 4345.618652 \n",
      "(GPU: 0, epoch: 13, iters: 9248, time: 0.008) nll: 4635.989258 \n",
      "(GPU: 0, epoch: 13, iters: 10048, time: 0.008) nll: 4255.402344 \n",
      "(GPU: 0, epoch: 13, iters: 10848, time: 0.008) nll: 5242.945312 \n",
      "saving the latest model (epoch 13, total_steps 1840000)\n",
      "(GPU: 0, epoch: 13, iters: 11648, time: 0.008) nll: 4930.404297 \n",
      "(GPU: 0, epoch: 13, iters: 12448, time: 0.008) nll: 4257.530762 \n",
      "(GPU: 0, epoch: 13, iters: 13248, time: 0.008) nll: 5324.886719 \n",
      "(GPU: 0, epoch: 13, iters: 14048, time: 0.008) nll: 2980.450195 \n",
      "(GPU: 0, epoch: 13, iters: 14848, time: 0.008) nll: 3786.589111 \n",
      "(GPU: 0, epoch: 13, iters: 15648, time: 0.007) nll: 4549.643555 \n",
      "(GPU: 0, epoch: 13, iters: 16448, time: 0.008) nll: 3314.394531 \n",
      "(GPU: 0, epoch: 13, iters: 17248, time: 0.007) nll: 4929.035156 \n",
      "(GPU: 0, epoch: 13, iters: 18048, time: 0.008) nll: 4329.627930 \n",
      "(GPU: 0, epoch: 13, iters: 18848, time: 0.008) nll: 4271.958984 \n",
      "(GPU: 0, epoch: 13, iters: 19648, time: 0.008) nll: 3462.550537 \n",
      "(GPU: 0, epoch: 13, iters: 20448, time: 0.008) nll: 4009.936523 \n",
      "(GPU: 0, epoch: 13, iters: 21248, time: 0.008) nll: 3973.275635 \n",
      "(GPU: 0, epoch: 13, iters: 22048, time: 0.008) nll: 2921.871094 \n",
      "(GPU: 0, epoch: 13, iters: 22848, time: 0.008) nll: 4011.860352 \n",
      "(GPU: 0, epoch: 13, iters: 23648, time: 0.008) nll: 3715.169922 \n",
      "(GPU: 0, epoch: 13, iters: 24448, time: 0.008) nll: 3419.201172 \n",
      "(GPU: 0, epoch: 13, iters: 25248, time: 0.008) nll: 4758.733398 \n",
      "(GPU: 0, epoch: 13, iters: 26048, time: 0.008) nll: 5505.617188 \n",
      "(GPU: 0, epoch: 13, iters: 26848, time: 0.008) nll: 3292.737305 \n",
      "(GPU: 0, epoch: 13, iters: 27648, time: 0.008) nll: 3148.753418 \n",
      "(GPU: 0, epoch: 13, iters: 28448, time: 0.007) nll: 2991.421387 \n",
      "(GPU: 0, epoch: 13, iters: 29248, time: 0.008) nll: 5296.040039 \n",
      "(GPU: 0, epoch: 13, iters: 30048, time: 0.007) nll: 4063.396729 \n",
      "(GPU: 0, epoch: 13, iters: 30848, time: 0.008) nll: 3132.103760 \n",
      "saving the latest model (epoch 13, total_steps 1860000)\n",
      "(GPU: 0, epoch: 13, iters: 31648, time: 0.007) nll: 4785.034668 \n",
      "(GPU: 0, epoch: 13, iters: 32448, time: 0.008) nll: 3420.990967 \n",
      "(GPU: 0, epoch: 13, iters: 33248, time: 0.008) nll: 3836.237793 \n",
      "(GPU: 0, epoch: 13, iters: 34048, time: 0.008) nll: 3787.008545 \n",
      "(GPU: 0, epoch: 13, iters: 34848, time: 0.008) nll: 3634.768555 \n",
      "(GPU: 0, epoch: 13, iters: 35648, time: 0.008) nll: 4172.722168 \n",
      "(GPU: 0, epoch: 13, iters: 36448, time: 0.008) nll: 3793.027100 \n",
      "(GPU: 0, epoch: 13, iters: 37248, time: 0.008) nll: 3780.739014 \n",
      "(GPU: 0, epoch: 13, iters: 38048, time: 0.008) nll: 4516.877930 \n",
      "(GPU: 0, epoch: 13, iters: 38848, time: 0.008) nll: 4718.208008 \n",
      "(GPU: 0, epoch: 13, iters: 39648, time: 0.008) nll: 4545.928711 \n",
      "(GPU: 0, epoch: 13, iters: 40448, time: 0.008) nll: 4902.871094 \n",
      "(GPU: 0, epoch: 13, iters: 41248, time: 0.008) nll: 4687.322754 \n",
      "(GPU: 0, epoch: 13, iters: 42048, time: 0.008) nll: 4407.659180 \n",
      "(GPU: 0, epoch: 13, iters: 42848, time: 0.007) nll: 4443.944336 \n",
      "(GPU: 0, epoch: 13, iters: 43648, time: 0.008) nll: 4113.611328 \n",
      "(GPU: 0, epoch: 13, iters: 44448, time: 0.007) nll: 4477.870117 \n",
      "(GPU: 0, epoch: 13, iters: 45248, time: 0.008) nll: 3932.325195 \n",
      "(GPU: 0, epoch: 13, iters: 46048, time: 0.008) nll: 3844.479980 \n",
      "(GPU: 0, epoch: 13, iters: 46848, time: 0.008) nll: 3618.130859 \n",
      "(GPU: 0, epoch: 13, iters: 47648, time: 0.008) nll: 4529.799805 \n",
      "(GPU: 0, epoch: 13, iters: 48448, time: 0.008) nll: 3994.411621 \n",
      "(GPU: 0, epoch: 13, iters: 49248, time: 0.008) nll: 5569.791992 \n",
      "(GPU: 0, epoch: 13, iters: 50048, time: 0.008) nll: 4170.381348 \n",
      "(GPU: 0, epoch: 13, iters: 50848, time: 0.008) nll: 4121.438477 \n",
      "saving the latest model (epoch 13, total_steps 1880000)\n",
      "(GPU: 0, epoch: 13, iters: 51648, time: 0.008) nll: 3963.365234 \n",
      "(GPU: 0, epoch: 13, iters: 52448, time: 0.008) nll: 3881.532715 \n",
      "(GPU: 0, epoch: 13, iters: 53248, time: 0.008) nll: 3526.297363 \n",
      "(GPU: 0, epoch: 13, iters: 54048, time: 0.007) nll: 4192.444824 \n",
      "(GPU: 0, epoch: 13, iters: 54848, time: 0.008) nll: 2705.276123 \n",
      "(GPU: 0, epoch: 13, iters: 55648, time: 0.008) nll: 4821.105957 \n",
      "(GPU: 0, epoch: 13, iters: 56448, time: 0.008) nll: 4056.018799 \n",
      "(GPU: 0, epoch: 13, iters: 57248, time: 0.008) nll: 2542.521973 \n",
      "(GPU: 0, epoch: 13, iters: 58048, time: 0.008) nll: 3418.966309 \n",
      "(GPU: 0, epoch: 13, iters: 58848, time: 0.008) nll: 3424.179932 \n",
      "(GPU: 0, epoch: 13, iters: 59648, time: 0.008) nll: 3774.480469 \n",
      "(GPU: 0, epoch: 13, iters: 60448, time: 0.008) nll: 3537.651611 \n",
      "(GPU: 0, epoch: 13, iters: 61248, time: 0.008) nll: 4209.159180 \n",
      "(GPU: 0, epoch: 13, iters: 62048, time: 0.008) nll: 3412.709473 \n",
      "(GPU: 0, epoch: 13, iters: 62848, time: 0.008) nll: 4409.222656 \n",
      "(GPU: 0, epoch: 13, iters: 63648, time: 0.008) nll: 3054.677490 \n",
      "(GPU: 0, epoch: 13, iters: 64448, time: 0.008) nll: 4011.061523 \n",
      "(GPU: 0, epoch: 13, iters: 65248, time: 0.007) nll: 4347.787109 \n",
      "(GPU: 0, epoch: 13, iters: 66048, time: 0.008) nll: 3737.294189 \n",
      "(GPU: 0, epoch: 13, iters: 66848, time: 0.008) nll: 3077.614014 \n",
      "(GPU: 0, epoch: 13, iters: 67648, time: 0.008) nll: 3525.494385 \n",
      "(GPU: 0, epoch: 13, iters: 68448, time: 0.008) nll: 3377.400391 \n",
      "(GPU: 0, epoch: 13, iters: 69248, time: 0.008) nll: 4248.461914 \n",
      "(GPU: 0, epoch: 13, iters: 70048, time: 0.007) nll: 3558.207031 \n",
      "(GPU: 0, epoch: 13, iters: 70848, time: 0.008) nll: 6313.991699 \n",
      "saving the latest model (epoch 13, total_steps 1900000)\n",
      "(GPU: 0, epoch: 13, iters: 71648, time: 0.008) nll: 5371.043945 \n",
      "(GPU: 0, epoch: 13, iters: 72448, time: 0.008) nll: 4808.253906 \n",
      "(GPU: 0, epoch: 13, iters: 73248, time: 0.007) nll: 2309.023682 \n",
      "(GPU: 0, epoch: 13, iters: 74048, time: 0.008) nll: 4026.794922 \n",
      "(GPU: 0, epoch: 13, iters: 74848, time: 0.008) nll: 3958.580566 \n",
      "(GPU: 0, epoch: 13, iters: 75648, time: 0.008) nll: 4080.740479 \n",
      "(GPU: 0, epoch: 13, iters: 76448, time: 0.008) nll: 3758.825439 \n",
      "(GPU: 0, epoch: 13, iters: 77248, time: 0.008) nll: 3912.559082 \n",
      "(GPU: 0, epoch: 13, iters: 78048, time: 0.008) nll: 4142.158203 \n",
      "(GPU: 0, epoch: 13, iters: 78848, time: 0.008) nll: 2984.292480 \n",
      "(GPU: 0, epoch: 13, iters: 79648, time: 0.007) nll: 4900.602539 \n",
      "(GPU: 0, epoch: 13, iters: 80448, time: 0.008) nll: 4413.423828 \n",
      "(GPU: 0, epoch: 13, iters: 81248, time: 0.007) nll: 3861.150146 \n",
      "(GPU: 0, epoch: 13, iters: 82048, time: 0.008) nll: 4565.280273 \n",
      "(GPU: 0, epoch: 13, iters: 82848, time: 0.008) nll: 3589.877441 \n",
      "(GPU: 0, epoch: 13, iters: 83648, time: 0.008) nll: 3556.030029 \n",
      "(GPU: 0, epoch: 13, iters: 84448, time: 0.008) nll: 4033.083984 \n",
      "(GPU: 0, epoch: 13, iters: 85248, time: 0.008) nll: 4866.548828 \n",
      "(GPU: 0, epoch: 13, iters: 86048, time: 0.008) nll: 3907.264648 \n",
      "(GPU: 0, epoch: 13, iters: 86848, time: 0.008) nll: 2855.527344 \n",
      "(GPU: 0, epoch: 13, iters: 87648, time: 0.008) nll: 5156.924316 \n",
      "(GPU: 0, epoch: 13, iters: 88448, time: 0.008) nll: 4421.496582 \n",
      "(GPU: 0, epoch: 13, iters: 89248, time: 0.008) nll: 3197.150635 \n",
      "(GPU: 0, epoch: 13, iters: 90048, time: 0.008) nll: 3985.719971 \n",
      "(GPU: 0, epoch: 13, iters: 90848, time: 0.007) nll: 2727.923828 \n",
      "(GPU: 0, epoch: 13, iters: 90848, time: 0.012) nll: 2679.061279 \n",
      "(GPU: 0, epoch: 13, iters: 90848, time: 0.012) nll: 4251.718750 \n",
      "saving the latest model (epoch 13, total_steps 1920000)\n",
      "(GPU: 0, epoch: 13, iters: 91648, time: 0.008) nll: 3542.684082 \n",
      "(GPU: 0, epoch: 13, iters: 92448, time: 0.008) nll: 4789.825195 \n",
      "(GPU: 0, epoch: 13, iters: 93248, time: 0.008) nll: 4150.442383 \n",
      "(GPU: 0, epoch: 13, iters: 94048, time: 0.008) nll: 5316.368164 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4397/4397 [20:29<00:00,  3.58it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 13, iters: 94848, time: 0.008) nll: 4310.196289 \n",
      "(GPU: 0, epoch: 13, iters: 95648, time: 0.007) nll: 3071.210449 \n",
      "(GPU: 0, epoch: 13, iters: 96448, time: 0.008) nll: 5537.319336 \n",
      "(GPU: 0, epoch: 13, iters: 97248, time: 0.007) nll: 5054.393066 \n",
      "(GPU: 0, epoch: 13, iters: 98048, time: 0.008) nll: 4277.600586 \n",
      "(GPU: 0, epoch: 13, iters: 98848, time: 0.008) nll: 4100.553711 \n",
      "(GPU: 0, epoch: 13, iters: 99648, time: 0.008) nll: 3090.853271 \n",
      "(GPU: 0, epoch: 13, iters: 100448, time: 0.008) nll: 4108.153320 \n",
      "(GPU: 0, epoch: 13, iters: 101248, time: 0.008) nll: 3843.272949 \n",
      "(GPU: 0, epoch: 13, iters: 102048, time: 0.007) nll: 4271.767578 \n",
      "(GPU: 0, epoch: 13, iters: 102848, time: 0.008) nll: 3969.079102 \n",
      "(GPU: 0, epoch: 13, iters: 103648, time: 0.008) nll: 2579.692383 \n",
      "(GPU: 0, epoch: 13, iters: 104448, time: 0.008) nll: 4295.250488 \n",
      "(GPU: 0, epoch: 13, iters: 105248, time: 0.008) nll: 3635.288330 \n",
      "(GPU: 0, epoch: 13, iters: 106048, time: 0.008) nll: 4202.198730 \n",
      "(GPU: 0, epoch: 13, iters: 106848, time: 0.008) nll: 3021.830566 \n",
      "(GPU: 0, epoch: 13, iters: 107648, time: 0.008) nll: 4329.842285 \n",
      "(GPU: 0, epoch: 13, iters: 108448, time: 0.008) nll: 3824.621826 \n",
      "(GPU: 0, epoch: 13, iters: 109248, time: 0.008) nll: 4546.874023 \n",
      "(GPU: 0, epoch: 13, iters: 110048, time: 0.008) nll: 3546.133789 \n",
      "(GPU: 0, epoch: 13, iters: 110848, time: 0.008) nll: 4086.019775 \n",
      "saving the latest model (epoch 13, total_steps 1940000)\n",
      "(GPU: 0, epoch: 13, iters: 111648, time: 0.008) nll: 3101.330078 \n",
      "(GPU: 0, epoch: 13, iters: 112448, time: 0.008) nll: 3288.584473 \n",
      "(GPU: 0, epoch: 13, iters: 113248, time: 0.007) nll: 1844.713867 \n",
      "(GPU: 0, epoch: 13, iters: 114048, time: 0.008) nll: 4726.914551 \n",
      "(GPU: 0, epoch: 13, iters: 114848, time: 0.008) nll: 4762.225586 \n",
      "(GPU: 0, epoch: 13, iters: 115648, time: 0.008) nll: 3194.724121 \n",
      "(GPU: 0, epoch: 13, iters: 116448, time: 0.008) nll: 3739.767090 \n",
      "(GPU: 0, epoch: 13, iters: 117248, time: 0.008) nll: 4866.108398 \n",
      "(GPU: 0, epoch: 13, iters: 118048, time: 0.008) nll: 3371.144043 \n",
      "(GPU: 0, epoch: 13, iters: 118848, time: 0.008) nll: 3437.669434 \n",
      "(GPU: 0, epoch: 13, iters: 119648, time: 0.008) nll: 3838.526611 \n",
      "(GPU: 0, epoch: 13, iters: 120448, time: 0.008) nll: 3684.683105 \n",
      "(GPU: 0, epoch: 13, iters: 121248, time: 0.008) nll: 4403.124023 \n",
      "(GPU: 0, epoch: 13, iters: 122048, time: 0.008) nll: 3259.013184 \n",
      "(GPU: 0, epoch: 13, iters: 122848, time: 0.008) nll: 4607.967285 \n",
      "(GPU: 0, epoch: 13, iters: 123648, time: 0.008) nll: 3986.125977 \n",
      "(GPU: 0, epoch: 13, iters: 124448, time: 0.008) nll: 3105.102051 \n",
      "(GPU: 0, epoch: 13, iters: 125248, time: 0.008) nll: 5160.650391 \n",
      "(GPU: 0, epoch: 13, iters: 126048, time: 0.008) nll: 4113.838379 \n",
      "(GPU: 0, epoch: 13, iters: 126848, time: 0.008) nll: 4790.849121 \n",
      "(GPU: 0, epoch: 13, iters: 127648, time: 0.008) nll: 2744.642090 \n",
      "(GPU: 0, epoch: 13, iters: 128448, time: 0.008) nll: 4044.906494 \n",
      "(GPU: 0, epoch: 13, iters: 129248, time: 0.008) nll: 3660.427490 \n",
      "(GPU: 0, epoch: 13, iters: 130048, time: 0.008) nll: 4167.636719 \n",
      "(GPU: 0, epoch: 13, iters: 130848, time: 0.008) nll: 5213.923828 \n",
      "saving the latest model (epoch 13, total_steps 1960000)\n",
      "(GPU: 0, epoch: 13, iters: 131648, time: 0.008) nll: 4047.387695 \n",
      "(GPU: 0, epoch: 13, iters: 132448, time: 0.008) nll: 5036.398438 \n",
      "(GPU: 0, epoch: 13, iters: 133248, time: 0.008) nll: 3440.963867 \n",
      "(GPU: 0, epoch: 13, iters: 134048, time: 0.008) nll: 3819.839600 \n",
      "(GPU: 0, epoch: 13, iters: 134848, time: 0.008) nll: 3781.997559 \n",
      "(GPU: 0, epoch: 13, iters: 135648, time: 0.007) nll: 5434.845703 \n",
      "(GPU: 0, epoch: 13, iters: 136448, time: 0.008) nll: 2871.574219 \n",
      "(GPU: 0, epoch: 13, iters: 137248, time: 0.008) nll: 2121.413330 \n",
      "(GPU: 0, epoch: 13, iters: 138048, time: 0.008) nll: 3156.925049 \n",
      "(GPU: 0, epoch: 13, iters: 138848, time: 0.008) nll: 4316.151855 \n",
      "(GPU: 0, epoch: 13, iters: 139648, time: 0.008) nll: 2963.635742 \n",
      "(GPU: 0, epoch: 13, iters: 140448, time: 0.008) nll: 3606.395508 \n",
      "[*] End of epoch 13 / 25 \t Time Taken: 1230 sec \n",
      "/cluster/54/streakfull/ADL4CV/Project/src/plz-autosdf/logs/bert2vqsc-text2shape-seq-LR1e-4-new-bert-2-prev_z_shape_input-try-2\n",
      "[*] learning rate = 0.0000845\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2966/4397 [13:51<06:06,  3.90it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 14, iters: 32, time: 0.004) nll: 3905.078125 \n",
      "(GPU: 0, epoch: 14, iters: 32, time: 0.004) nll: 3419.849609 \n",
      "(GPU: 0, epoch: 14, iters: 544, time: 0.008) nll: 2692.110840 \n",
      "(GPU: 0, epoch: 14, iters: 1344, time: 0.008) nll: 4702.138672 \n",
      "(GPU: 0, epoch: 14, iters: 2144, time: 0.008) nll: 4129.315430 \n",
      "(GPU: 0, epoch: 14, iters: 2944, time: 0.008) nll: 2676.723633 \n",
      "(GPU: 0, epoch: 14, iters: 3744, time: 0.008) nll: 4794.603027 \n",
      "(GPU: 0, epoch: 14, iters: 4544, time: 0.008) nll: 3734.256104 \n",
      "(GPU: 0, epoch: 14, iters: 5344, time: 0.008) nll: 2609.516113 \n",
      "(GPU: 0, epoch: 14, iters: 6144, time: 0.008) nll: 4970.574707 \n",
      "(GPU: 0, epoch: 14, iters: 6944, time: 0.007) nll: 4280.563477 \n",
      "(GPU: 0, epoch: 14, iters: 7744, time: 0.008) nll: 3251.469238 \n",
      "(GPU: 0, epoch: 14, iters: 8544, time: 0.008) nll: 3873.053711 \n",
      "(GPU: 0, epoch: 14, iters: 9344, time: 0.008) nll: 4564.646484 \n",
      "(GPU: 0, epoch: 14, iters: 10144, time: 0.007) nll: 3254.376465 \n",
      "saving the latest model (epoch 14, total_steps 1980000)\n",
      "(GPU: 0, epoch: 14, iters: 10944, time: 0.008) nll: 3738.807617 \n",
      "(GPU: 0, epoch: 14, iters: 11744, time: 0.008) nll: 3664.402344 \n",
      "(GPU: 0, epoch: 14, iters: 12544, time: 0.008) nll: 3445.223633 \n",
      "(GPU: 0, epoch: 14, iters: 13344, time: 0.008) nll: 4303.800781 \n",
      "(GPU: 0, epoch: 14, iters: 14144, time: 0.008) nll: 2650.849609 \n",
      "(GPU: 0, epoch: 14, iters: 14944, time: 0.008) nll: 3283.807617 \n",
      "(GPU: 0, epoch: 14, iters: 15744, time: 0.008) nll: 6061.876953 \n",
      "(GPU: 0, epoch: 14, iters: 16544, time: 0.008) nll: 4907.690430 \n",
      "(GPU: 0, epoch: 14, iters: 17344, time: 0.008) nll: 4233.856445 \n",
      "(GPU: 0, epoch: 14, iters: 18144, time: 0.008) nll: 3430.235596 \n",
      "(GPU: 0, epoch: 14, iters: 18944, time: 0.008) nll: 4838.766602 \n",
      "(GPU: 0, epoch: 14, iters: 19744, time: 0.008) nll: 3681.740723 \n",
      "(GPU: 0, epoch: 14, iters: 20544, time: 0.008) nll: 3059.943359 \n",
      "(GPU: 0, epoch: 14, iters: 21344, time: 0.007) nll: 3726.514160 \n",
      "(GPU: 0, epoch: 14, iters: 22144, time: 0.008) nll: 3252.267334 \n",
      "(GPU: 0, epoch: 14, iters: 22944, time: 0.008) nll: 3239.208496 \n",
      "(GPU: 0, epoch: 14, iters: 23744, time: 0.008) nll: 3718.619141 \n",
      "(GPU: 0, epoch: 14, iters: 24544, time: 0.008) nll: 3730.476318 \n",
      "(GPU: 0, epoch: 14, iters: 25344, time: 0.008) nll: 2665.482910 \n",
      "(GPU: 0, epoch: 14, iters: 26144, time: 0.008) nll: 3651.338867 \n",
      "(GPU: 0, epoch: 14, iters: 26944, time: 0.008) nll: 2933.811035 \n",
      "(GPU: 0, epoch: 14, iters: 27744, time: 0.008) nll: 2923.457520 \n",
      "(GPU: 0, epoch: 14, iters: 28544, time: 0.008) nll: 4007.109863 \n",
      "(GPU: 0, epoch: 14, iters: 29344, time: 0.008) nll: 6116.450195 \n",
      "(GPU: 0, epoch: 14, iters: 30144, time: 0.008) nll: 3248.559082 \n",
      "saving the latest model (epoch 14, total_steps 2000000)\n",
      "(GPU: 0, epoch: 14, iters: 30944, time: 0.008) nll: 4894.793945 \n",
      "(GPU: 0, epoch: 14, iters: 31744, time: 0.008) nll: 3929.226562 \n",
      "(GPU: 0, epoch: 14, iters: 32544, time: 0.008) nll: 4906.093750 \n",
      "(GPU: 0, epoch: 14, iters: 33344, time: 0.008) nll: 3418.106445 \n",
      "(GPU: 0, epoch: 14, iters: 34144, time: 0.008) nll: 3545.401855 \n",
      "(GPU: 0, epoch: 14, iters: 34944, time: 0.008) nll: 3363.374512 \n",
      "(GPU: 0, epoch: 14, iters: 35744, time: 0.007) nll: 3868.154785 \n",
      "(GPU: 0, epoch: 14, iters: 36544, time: 0.008) nll: 3409.215332 \n",
      "(GPU: 0, epoch: 14, iters: 37344, time: 0.008) nll: 3310.536377 \n",
      "(GPU: 0, epoch: 14, iters: 38144, time: 0.008) nll: 2566.616455 \n",
      "(GPU: 0, epoch: 14, iters: 38944, time: 0.008) nll: 4802.881836 \n",
      "(GPU: 0, epoch: 14, iters: 39744, time: 0.008) nll: 4482.779297 \n",
      "(GPU: 0, epoch: 14, iters: 40544, time: 0.008) nll: 3806.403076 \n",
      "(GPU: 0, epoch: 14, iters: 41344, time: 0.008) nll: 3934.625977 \n",
      "(GPU: 0, epoch: 14, iters: 42144, time: 0.007) nll: 3959.624023 \n",
      "(GPU: 0, epoch: 14, iters: 42944, time: 0.008) nll: 4701.781250 \n",
      "(GPU: 0, epoch: 14, iters: 43744, time: 0.008) nll: 3957.384766 \n",
      "(GPU: 0, epoch: 14, iters: 44544, time: 0.008) nll: 4508.631836 \n",
      "(GPU: 0, epoch: 14, iters: 45344, time: 0.008) nll: 3433.074219 \n",
      "(GPU: 0, epoch: 14, iters: 46144, time: 0.008) nll: 3375.102051 \n",
      "(GPU: 0, epoch: 14, iters: 46144, time: 0.013) nll: 3316.061279 \n",
      "(GPU: 0, epoch: 14, iters: 46144, time: 0.013) nll: 3057.345703 \n",
      "(GPU: 0, epoch: 14, iters: 46944, time: 0.008) nll: 3425.372070 \n",
      "(GPU: 0, epoch: 14, iters: 47744, time: 0.008) nll: 5129.609375 \n",
      "(GPU: 0, epoch: 14, iters: 48544, time: 0.008) nll: 4627.021484 \n",
      "(GPU: 0, epoch: 14, iters: 49344, time: 0.008) nll: 4839.115234 \n",
      "(GPU: 0, epoch: 14, iters: 50144, time: 0.008) nll: 4373.778320 \n",
      "saving the latest model (epoch 14, total_steps 2020000)\n",
      "(GPU: 0, epoch: 14, iters: 50944, time: 0.008) nll: 4945.068848 \n",
      "(GPU: 0, epoch: 14, iters: 51744, time: 0.008) nll: 2362.823242 \n",
      "(GPU: 0, epoch: 14, iters: 52544, time: 0.008) nll: 3675.291992 \n",
      "(GPU: 0, epoch: 14, iters: 53344, time: 0.007) nll: 2576.641602 \n",
      "(GPU: 0, epoch: 14, iters: 54144, time: 0.008) nll: 4442.073242 \n",
      "(GPU: 0, epoch: 14, iters: 54944, time: 0.008) nll: 3860.893555 \n",
      "(GPU: 0, epoch: 14, iters: 55744, time: 0.008) nll: 2243.616943 \n",
      "(GPU: 0, epoch: 14, iters: 56544, time: 0.008) nll: 3782.210449 \n",
      "(GPU: 0, epoch: 14, iters: 57344, time: 0.008) nll: 2665.763672 \n",
      "(GPU: 0, epoch: 14, iters: 58144, time: 0.008) nll: 3732.334473 \n",
      "(GPU: 0, epoch: 14, iters: 58944, time: 0.008) nll: 4346.638672 \n",
      "(GPU: 0, epoch: 14, iters: 59744, time: 0.008) nll: 4030.376221 \n",
      "(GPU: 0, epoch: 14, iters: 60544, time: 0.008) nll: 3692.278320 \n",
      "(GPU: 0, epoch: 14, iters: 61344, time: 0.008) nll: 4433.660156 \n",
      "(GPU: 0, epoch: 14, iters: 62144, time: 0.008) nll: 2861.545898 \n",
      "(GPU: 0, epoch: 14, iters: 62944, time: 0.008) nll: 4615.272461 \n",
      "(GPU: 0, epoch: 14, iters: 63744, time: 0.008) nll: 4110.455078 \n",
      "(GPU: 0, epoch: 14, iters: 64544, time: 0.008) nll: 4865.744141 \n",
      "(GPU: 0, epoch: 14, iters: 65344, time: 0.008) nll: 3890.910645 \n",
      "(GPU: 0, epoch: 14, iters: 66144, time: 0.008) nll: 4824.492188 \n",
      "(GPU: 0, epoch: 14, iters: 66944, time: 0.008) nll: 4040.614502 \n",
      "(GPU: 0, epoch: 14, iters: 67744, time: 0.008) nll: 4386.165039 \n",
      "(GPU: 0, epoch: 14, iters: 68544, time: 0.008) nll: 4528.536621 \n",
      "(GPU: 0, epoch: 14, iters: 69344, time: 0.008) nll: 2867.846924 \n",
      "(GPU: 0, epoch: 14, iters: 70144, time: 0.008) nll: 3470.192383 \n",
      "saving the latest model (epoch 14, total_steps 2040000)\n",
      "(GPU: 0, epoch: 14, iters: 70944, time: 0.008) nll: 4638.993164 \n",
      "(GPU: 0, epoch: 14, iters: 71744, time: 0.008) nll: 3507.679688 \n",
      "(GPU: 0, epoch: 14, iters: 72544, time: 0.007) nll: 5837.943359 \n",
      "(GPU: 0, epoch: 14, iters: 73344, time: 0.008) nll: 2987.953613 \n",
      "(GPU: 0, epoch: 14, iters: 74144, time: 0.008) nll: 3660.337402 \n",
      "(GPU: 0, epoch: 14, iters: 74944, time: 0.008) nll: 3498.421387 \n",
      "(GPU: 0, epoch: 14, iters: 75744, time: 0.008) nll: 4181.051758 \n",
      "(GPU: 0, epoch: 14, iters: 76544, time: 0.008) nll: 3751.175049 \n",
      "(GPU: 0, epoch: 14, iters: 77344, time: 0.008) nll: 4702.918945 \n",
      "(GPU: 0, epoch: 14, iters: 78144, time: 0.008) nll: 3448.956055 \n",
      "(GPU: 0, epoch: 14, iters: 78944, time: 0.007) nll: 2728.333740 \n",
      "(GPU: 0, epoch: 14, iters: 79744, time: 0.008) nll: 5379.334961 \n",
      "(GPU: 0, epoch: 14, iters: 80544, time: 0.008) nll: 2960.849609 \n",
      "(GPU: 0, epoch: 14, iters: 81344, time: 0.008) nll: 4181.026855 \n",
      "(GPU: 0, epoch: 14, iters: 82144, time: 0.008) nll: 4021.152588 \n",
      "(GPU: 0, epoch: 14, iters: 82944, time: 0.008) nll: 5567.881836 \n",
      "(GPU: 0, epoch: 14, iters: 83744, time: 0.007) nll: 3453.201904 \n",
      "(GPU: 0, epoch: 14, iters: 84544, time: 0.008) nll: 5142.214844 \n",
      "(GPU: 0, epoch: 14, iters: 85344, time: 0.008) nll: 3938.339355 \n",
      "(GPU: 0, epoch: 14, iters: 86144, time: 0.008) nll: 3557.063721 \n",
      "(GPU: 0, epoch: 14, iters: 86944, time: 0.008) nll: 3854.553955 \n",
      "(GPU: 0, epoch: 14, iters: 87744, time: 0.008) nll: 3615.027344 \n",
      "(GPU: 0, epoch: 14, iters: 88544, time: 0.008) nll: 3049.281738 \n",
      "(GPU: 0, epoch: 14, iters: 89344, time: 0.008) nll: 2968.948730 \n",
      "(GPU: 0, epoch: 14, iters: 90144, time: 0.008) nll: 3864.375488 \n",
      "saving the latest model (epoch 14, total_steps 2060000)\n",
      "(GPU: 0, epoch: 14, iters: 90944, time: 0.008) nll: 4304.157227 \n",
      "(GPU: 0, epoch: 14, iters: 91744, time: 0.008) nll: 3916.993652 \n",
      "(GPU: 0, epoch: 14, iters: 92544, time: 0.008) nll: 3175.435303 \n",
      "(GPU: 0, epoch: 14, iters: 93344, time: 0.007) nll: 4070.512451 \n",
      "(GPU: 0, epoch: 14, iters: 94144, time: 0.008) nll: 3409.423828 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4397/4397 [20:27<00:00,  3.58it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 14, iters: 94944, time: 0.008) nll: 4602.569824 \n",
      "(GPU: 0, epoch: 14, iters: 95744, time: 0.008) nll: 3547.771973 \n",
      "(GPU: 0, epoch: 14, iters: 96544, time: 0.008) nll: 4456.423828 \n",
      "(GPU: 0, epoch: 14, iters: 97344, time: 0.008) nll: 3455.898926 \n",
      "(GPU: 0, epoch: 14, iters: 98144, time: 0.007) nll: 4335.562500 \n",
      "(GPU: 0, epoch: 14, iters: 98944, time: 0.008) nll: 4331.338867 \n",
      "(GPU: 0, epoch: 14, iters: 99744, time: 0.008) nll: 4907.116211 \n",
      "(GPU: 0, epoch: 14, iters: 100544, time: 0.008) nll: 3572.339844 \n",
      "(GPU: 0, epoch: 14, iters: 101344, time: 0.008) nll: 4371.817383 \n",
      "(GPU: 0, epoch: 14, iters: 102144, time: 0.008) nll: 3153.267090 \n",
      "(GPU: 0, epoch: 14, iters: 102944, time: 0.008) nll: 3854.010742 \n",
      "(GPU: 0, epoch: 14, iters: 103744, time: 0.008) nll: 4500.724609 \n",
      "(GPU: 0, epoch: 14, iters: 104544, time: 0.008) nll: 2899.245605 \n",
      "(GPU: 0, epoch: 14, iters: 105344, time: 0.008) nll: 6039.516602 \n",
      "(GPU: 0, epoch: 14, iters: 106144, time: 0.008) nll: 4250.168945 \n",
      "(GPU: 0, epoch: 14, iters: 106944, time: 0.008) nll: 4610.452148 \n",
      "(GPU: 0, epoch: 14, iters: 107744, time: 0.008) nll: 3503.025391 \n",
      "(GPU: 0, epoch: 14, iters: 108544, time: 0.008) nll: 4870.512695 \n",
      "(GPU: 0, epoch: 14, iters: 109344, time: 0.008) nll: 3909.100342 \n",
      "(GPU: 0, epoch: 14, iters: 110144, time: 0.008) nll: 4420.621094 \n",
      "saving the latest model (epoch 14, total_steps 2080000)\n",
      "(GPU: 0, epoch: 14, iters: 110944, time: 0.008) nll: 4172.277344 \n",
      "(GPU: 0, epoch: 14, iters: 111744, time: 0.008) nll: 3463.680176 \n",
      "(GPU: 0, epoch: 14, iters: 112544, time: 0.008) nll: 4101.227051 \n",
      "(GPU: 0, epoch: 14, iters: 113344, time: 0.008) nll: 3828.352783 \n",
      "(GPU: 0, epoch: 14, iters: 114144, time: 0.007) nll: 2533.935547 \n",
      "(GPU: 0, epoch: 14, iters: 114944, time: 0.008) nll: 4259.791016 \n",
      "(GPU: 0, epoch: 14, iters: 115744, time: 0.008) nll: 3811.102783 \n",
      "(GPU: 0, epoch: 14, iters: 116544, time: 0.008) nll: 3795.286621 \n",
      "(GPU: 0, epoch: 14, iters: 117344, time: 0.008) nll: 4026.461914 \n",
      "(GPU: 0, epoch: 14, iters: 118144, time: 0.008) nll: 4011.189941 \n",
      "(GPU: 0, epoch: 14, iters: 118944, time: 0.008) nll: 4322.598633 \n",
      "(GPU: 0, epoch: 14, iters: 119744, time: 0.008) nll: 3841.613281 \n",
      "(GPU: 0, epoch: 14, iters: 120544, time: 0.008) nll: 3775.940186 \n",
      "(GPU: 0, epoch: 14, iters: 121344, time: 0.008) nll: 2697.588135 \n",
      "(GPU: 0, epoch: 14, iters: 122144, time: 0.007) nll: 3119.654785 \n",
      "(GPU: 0, epoch: 14, iters: 122944, time: 0.008) nll: 3740.527344 \n",
      "(GPU: 0, epoch: 14, iters: 123744, time: 0.008) nll: 3853.489014 \n",
      "(GPU: 0, epoch: 14, iters: 124544, time: 0.008) nll: 2613.277344 \n",
      "(GPU: 0, epoch: 14, iters: 125344, time: 0.007) nll: 3704.257324 \n",
      "(GPU: 0, epoch: 14, iters: 126144, time: 0.008) nll: 4826.797852 \n",
      "(GPU: 0, epoch: 14, iters: 126944, time: 0.007) nll: 4197.520508 \n",
      "(GPU: 0, epoch: 14, iters: 127744, time: 0.008) nll: 3421.464844 \n",
      "(GPU: 0, epoch: 14, iters: 128544, time: 0.008) nll: 4432.576660 \n",
      "(GPU: 0, epoch: 14, iters: 129344, time: 0.008) nll: 5581.363281 \n",
      "(GPU: 0, epoch: 14, iters: 130144, time: 0.008) nll: 3530.317139 \n",
      "saving the latest model (epoch 14, total_steps 2100000)\n",
      "(GPU: 0, epoch: 14, iters: 130944, time: 0.008) nll: 4263.647949 \n",
      "(GPU: 0, epoch: 14, iters: 131744, time: 0.008) nll: 3712.276367 \n",
      "(GPU: 0, epoch: 14, iters: 132544, time: 0.008) nll: 3309.749023 \n",
      "(GPU: 0, epoch: 14, iters: 133344, time: 0.008) nll: 3437.338867 \n",
      "(GPU: 0, epoch: 14, iters: 134144, time: 0.008) nll: 4082.296387 \n",
      "(GPU: 0, epoch: 14, iters: 134944, time: 0.008) nll: 4740.028320 \n",
      "(GPU: 0, epoch: 14, iters: 135744, time: 0.008) nll: 4069.100098 \n",
      "(GPU: 0, epoch: 14, iters: 136544, time: 0.008) nll: 3474.723877 \n",
      "(GPU: 0, epoch: 14, iters: 137344, time: 0.008) nll: 3517.263672 \n",
      "(GPU: 0, epoch: 14, iters: 138144, time: 0.008) nll: 5373.733398 \n",
      "(GPU: 0, epoch: 14, iters: 138944, time: 0.008) nll: 3568.870605 \n",
      "(GPU: 0, epoch: 14, iters: 139744, time: 0.008) nll: 3712.646484 \n",
      "(GPU: 0, epoch: 14, iters: 140544, time: 0.008) nll: 3863.063965 \n",
      "[*] End of epoch 14 / 25 \t Time Taken: 1228 sec \n",
      "/cluster/54/streakfull/ADL4CV/Project/src/plz-autosdf/logs/bert2vqsc-text2shape-seq-LR1e-4-new-bert-2-prev_z_shape_input-try-2\n",
      "[*] learning rate = 0.0000816\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 2969/4397 [13:53<06:08,  3.88it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 15, iters: 32, time: 0.004) nll: 3080.394775 \n",
      "(GPU: 0, epoch: 15, iters: 32, time: 0.004) nll: 3978.980957 \n",
      "(GPU: 0, epoch: 15, iters: 640, time: 0.008) nll: 1323.324707 \n",
      "(GPU: 0, epoch: 15, iters: 1440, time: 0.008) nll: 4621.335449 \n",
      "(GPU: 0, epoch: 15, iters: 1440, time: 0.012) nll: 4565.540039 \n",
      "(GPU: 0, epoch: 15, iters: 1440, time: 0.012) nll: 3097.145020 \n",
      "(GPU: 0, epoch: 15, iters: 2240, time: 0.008) nll: 4529.052246 \n",
      "(GPU: 0, epoch: 15, iters: 3040, time: 0.008) nll: 4406.600098 \n",
      "(GPU: 0, epoch: 15, iters: 3840, time: 0.008) nll: 3772.895996 \n",
      "(GPU: 0, epoch: 15, iters: 4640, time: 0.008) nll: 3689.227051 \n",
      "(GPU: 0, epoch: 15, iters: 5440, time: 0.008) nll: 4290.416016 \n",
      "(GPU: 0, epoch: 15, iters: 6240, time: 0.008) nll: 3204.046143 \n",
      "(GPU: 0, epoch: 15, iters: 7040, time: 0.008) nll: 4436.633789 \n",
      "(GPU: 0, epoch: 15, iters: 7840, time: 0.008) nll: 2717.916016 \n",
      "(GPU: 0, epoch: 15, iters: 8640, time: 0.008) nll: 3952.255371 \n",
      "(GPU: 0, epoch: 15, iters: 9440, time: 0.008) nll: 4533.686035 \n",
      "saving the latest model (epoch 15, total_steps 2120000)\n",
      "(GPU: 0, epoch: 15, iters: 10240, time: 0.008) nll: 4845.898438 \n",
      "(GPU: 0, epoch: 15, iters: 11040, time: 0.007) nll: 3987.298340 \n",
      "(GPU: 0, epoch: 15, iters: 11840, time: 0.008) nll: 4105.655273 \n",
      "(GPU: 0, epoch: 15, iters: 12640, time: 0.008) nll: 4682.762207 \n",
      "(GPU: 0, epoch: 15, iters: 13440, time: 0.008) nll: 3980.812988 \n",
      "(GPU: 0, epoch: 15, iters: 14240, time: 0.008) nll: 5523.176270 \n",
      "(GPU: 0, epoch: 15, iters: 15040, time: 0.008) nll: 3604.708984 \n",
      "(GPU: 0, epoch: 15, iters: 15840, time: 0.008) nll: 3802.356445 \n",
      "(GPU: 0, epoch: 15, iters: 16640, time: 0.008) nll: 3309.801514 \n",
      "(GPU: 0, epoch: 15, iters: 17440, time: 0.008) nll: 3326.131836 \n",
      "(GPU: 0, epoch: 15, iters: 18240, time: 0.008) nll: 3622.810547 \n",
      "(GPU: 0, epoch: 15, iters: 19040, time: 0.008) nll: 3966.741211 \n",
      "(GPU: 0, epoch: 15, iters: 19840, time: 0.008) nll: 3168.521973 \n",
      "(GPU: 0, epoch: 15, iters: 20640, time: 0.008) nll: 4584.068359 \n",
      "(GPU: 0, epoch: 15, iters: 21440, time: 0.008) nll: 4536.712891 \n",
      "(GPU: 0, epoch: 15, iters: 22240, time: 0.008) nll: 3397.383057 \n",
      "(GPU: 0, epoch: 15, iters: 23040, time: 0.008) nll: 4193.583008 \n",
      "(GPU: 0, epoch: 15, iters: 23840, time: 0.008) nll: 3669.148682 \n",
      "(GPU: 0, epoch: 15, iters: 24640, time: 0.008) nll: 2428.364258 \n",
      "(GPU: 0, epoch: 15, iters: 25440, time: 0.008) nll: 4035.100586 \n",
      "(GPU: 0, epoch: 15, iters: 26240, time: 0.008) nll: 3817.350830 \n",
      "(GPU: 0, epoch: 15, iters: 27040, time: 0.008) nll: 3933.395020 \n",
      "(GPU: 0, epoch: 15, iters: 27840, time: 0.008) nll: 4520.711426 \n",
      "(GPU: 0, epoch: 15, iters: 28640, time: 0.008) nll: 3307.183105 \n",
      "(GPU: 0, epoch: 15, iters: 29440, time: 0.008) nll: 3938.379395 \n",
      "saving the latest model (epoch 15, total_steps 2140000)\n",
      "(GPU: 0, epoch: 15, iters: 30240, time: 0.008) nll: 3572.504395 \n",
      "(GPU: 0, epoch: 15, iters: 31040, time: 0.008) nll: 3425.979492 \n",
      "(GPU: 0, epoch: 15, iters: 31840, time: 0.007) nll: 3758.799316 \n",
      "(GPU: 0, epoch: 15, iters: 32640, time: 0.008) nll: 3064.261719 \n",
      "(GPU: 0, epoch: 15, iters: 33440, time: 0.008) nll: 2769.374268 \n",
      "(GPU: 0, epoch: 15, iters: 34240, time: 0.008) nll: 3939.884033 \n",
      "(GPU: 0, epoch: 15, iters: 35040, time: 0.008) nll: 4717.759766 \n",
      "(GPU: 0, epoch: 15, iters: 35840, time: 0.008) nll: 2876.630127 \n",
      "(GPU: 0, epoch: 15, iters: 36640, time: 0.008) nll: 4494.268555 \n",
      "(GPU: 0, epoch: 15, iters: 37440, time: 0.008) nll: 4198.124023 \n",
      "(GPU: 0, epoch: 15, iters: 38240, time: 0.008) nll: 3340.504395 \n",
      "(GPU: 0, epoch: 15, iters: 39040, time: 0.008) nll: 2808.767334 \n",
      "(GPU: 0, epoch: 15, iters: 39840, time: 0.008) nll: 3875.508301 \n",
      "(GPU: 0, epoch: 15, iters: 40640, time: 0.008) nll: 3516.238770 \n",
      "(GPU: 0, epoch: 15, iters: 41440, time: 0.007) nll: 4626.998047 \n",
      "(GPU: 0, epoch: 15, iters: 42240, time: 0.008) nll: 2667.003174 \n",
      "(GPU: 0, epoch: 15, iters: 43040, time: 0.008) nll: 2743.086914 \n",
      "(GPU: 0, epoch: 15, iters: 43840, time: 0.008) nll: 2940.319580 \n",
      "(GPU: 0, epoch: 15, iters: 44640, time: 0.007) nll: 4424.185547 \n",
      "(GPU: 0, epoch: 15, iters: 45440, time: 0.008) nll: 3647.105469 \n",
      "(GPU: 0, epoch: 15, iters: 46240, time: 0.008) nll: 3558.964844 \n",
      "(GPU: 0, epoch: 15, iters: 47040, time: 0.008) nll: 4060.540527 \n",
      "(GPU: 0, epoch: 15, iters: 47840, time: 0.008) nll: 3787.278076 \n",
      "(GPU: 0, epoch: 15, iters: 48640, time: 0.008) nll: 5183.943359 \n",
      "(GPU: 0, epoch: 15, iters: 49440, time: 0.008) nll: 4200.121582 \n",
      "saving the latest model (epoch 15, total_steps 2160000)\n",
      "(GPU: 0, epoch: 15, iters: 50240, time: 0.008) nll: 4125.114258 \n",
      "(GPU: 0, epoch: 15, iters: 51040, time: 0.007) nll: 4625.037598 \n",
      "(GPU: 0, epoch: 15, iters: 51840, time: 0.008) nll: 3047.107910 \n",
      "(GPU: 0, epoch: 15, iters: 52640, time: 0.008) nll: 4313.175781 \n",
      "(GPU: 0, epoch: 15, iters: 53440, time: 0.008) nll: 4704.239258 \n",
      "(GPU: 0, epoch: 15, iters: 54240, time: 0.008) nll: 3185.897461 \n",
      "(GPU: 0, epoch: 15, iters: 55040, time: 0.008) nll: 3719.765625 \n",
      "(GPU: 0, epoch: 15, iters: 55840, time: 0.008) nll: 4057.360840 \n",
      "(GPU: 0, epoch: 15, iters: 56640, time: 0.008) nll: 2998.050537 \n",
      "(GPU: 0, epoch: 15, iters: 57440, time: 0.008) nll: 3154.934814 \n",
      "(GPU: 0, epoch: 15, iters: 58240, time: 0.008) nll: 6104.419922 \n",
      "(GPU: 0, epoch: 15, iters: 59040, time: 0.008) nll: 5861.190918 \n",
      "(GPU: 0, epoch: 15, iters: 59840, time: 0.008) nll: 4960.217773 \n",
      "(GPU: 0, epoch: 15, iters: 60640, time: 0.008) nll: 3144.163086 \n",
      "(GPU: 0, epoch: 15, iters: 61440, time: 0.008) nll: 4088.860107 \n",
      "(GPU: 0, epoch: 15, iters: 62240, time: 0.007) nll: 4919.351562 \n",
      "(GPU: 0, epoch: 15, iters: 63040, time: 0.008) nll: 3079.457031 \n",
      "(GPU: 0, epoch: 15, iters: 63840, time: 0.008) nll: 3984.790527 \n",
      "(GPU: 0, epoch: 15, iters: 64640, time: 0.008) nll: 4217.492188 \n",
      "(GPU: 0, epoch: 15, iters: 65440, time: 0.008) nll: 4003.660889 \n",
      "(GPU: 0, epoch: 15, iters: 66240, time: 0.008) nll: 2745.401367 \n",
      "(GPU: 0, epoch: 15, iters: 67040, time: 0.007) nll: 2018.226074 \n",
      "(GPU: 0, epoch: 15, iters: 67840, time: 0.008) nll: 3238.892334 \n",
      "(GPU: 0, epoch: 15, iters: 68640, time: 0.008) nll: 4562.365234 \n",
      "(GPU: 0, epoch: 15, iters: 69440, time: 0.008) nll: 3740.600586 \n",
      "saving the latest model (epoch 15, total_steps 2180000)\n",
      "(GPU: 0, epoch: 15, iters: 70240, time: 0.007) nll: 3951.744141 \n",
      "(GPU: 0, epoch: 15, iters: 71040, time: 0.008) nll: 4281.773926 \n",
      "(GPU: 0, epoch: 15, iters: 71840, time: 0.007) nll: 3407.403320 \n",
      "(GPU: 0, epoch: 15, iters: 72640, time: 0.008) nll: 4124.105469 \n",
      "(GPU: 0, epoch: 15, iters: 73440, time: 0.008) nll: 3672.751709 \n",
      "(GPU: 0, epoch: 15, iters: 74240, time: 0.008) nll: 4562.564453 \n",
      "(GPU: 0, epoch: 15, iters: 75040, time: 0.008) nll: 3479.216553 \n",
      "(GPU: 0, epoch: 15, iters: 75840, time: 0.008) nll: 3465.395264 \n",
      "(GPU: 0, epoch: 15, iters: 76640, time: 0.008) nll: 3870.420654 \n",
      "(GPU: 0, epoch: 15, iters: 77440, time: 0.008) nll: 3416.872070 \n",
      "(GPU: 0, epoch: 15, iters: 78240, time: 0.007) nll: 3221.045898 \n",
      "(GPU: 0, epoch: 15, iters: 79040, time: 0.008) nll: 4185.282227 \n",
      "(GPU: 0, epoch: 15, iters: 79840, time: 0.008) nll: 2440.565918 \n",
      "(GPU: 0, epoch: 15, iters: 80640, time: 0.008) nll: 3509.285400 \n",
      "(GPU: 0, epoch: 15, iters: 81440, time: 0.008) nll: 4039.787842 \n",
      "(GPU: 0, epoch: 15, iters: 82240, time: 0.008) nll: 2990.963867 \n",
      "(GPU: 0, epoch: 15, iters: 83040, time: 0.008) nll: 3843.663818 \n",
      "(GPU: 0, epoch: 15, iters: 83840, time: 0.008) nll: 4596.683594 \n",
      "(GPU: 0, epoch: 15, iters: 84640, time: 0.008) nll: 3403.656738 \n",
      "(GPU: 0, epoch: 15, iters: 85440, time: 0.008) nll: 3935.665771 \n",
      "(GPU: 0, epoch: 15, iters: 86240, time: 0.007) nll: 4006.955322 \n",
      "(GPU: 0, epoch: 15, iters: 87040, time: 0.008) nll: 3411.377686 \n",
      "(GPU: 0, epoch: 15, iters: 87840, time: 0.008) nll: 3087.273926 \n",
      "(GPU: 0, epoch: 15, iters: 88640, time: 0.008) nll: 3535.890625 \n",
      "(GPU: 0, epoch: 15, iters: 89440, time: 0.008) nll: 4404.376953 \n",
      "saving the latest model (epoch 15, total_steps 2200000)\n",
      "(GPU: 0, epoch: 15, iters: 90240, time: 0.008) nll: 4423.372070 \n",
      "(GPU: 0, epoch: 15, iters: 91040, time: 0.008) nll: 4287.578125 \n",
      "(GPU: 0, epoch: 15, iters: 91840, time: 0.008) nll: 4129.984375 \n",
      "(GPU: 0, epoch: 15, iters: 92640, time: 0.007) nll: 3863.130371 \n",
      "(GPU: 0, epoch: 15, iters: 93440, time: 0.008) nll: 3302.324707 \n",
      "(GPU: 0, epoch: 15, iters: 94240, time: 0.008) nll: 4327.780273 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4397/4397 [20:29<00:00,  3.58it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 15, iters: 95040, time: 0.008) nll: 3619.194580 \n",
      "(GPU: 0, epoch: 15, iters: 95840, time: 0.008) nll: 4412.147949 \n",
      "(GPU: 0, epoch: 15, iters: 96640, time: 0.008) nll: 3124.676758 \n",
      "(GPU: 0, epoch: 15, iters: 97440, time: 0.008) nll: 3724.968750 \n",
      "(GPU: 0, epoch: 15, iters: 97440, time: 0.013) nll: 3663.333740 \n",
      "(GPU: 0, epoch: 15, iters: 97440, time: 0.013) nll: 4246.274414 \n",
      "(GPU: 0, epoch: 15, iters: 98240, time: 0.008) nll: 3933.216309 \n",
      "(GPU: 0, epoch: 15, iters: 99040, time: 0.008) nll: 2781.692383 \n",
      "(GPU: 0, epoch: 15, iters: 99840, time: 0.008) nll: 3817.143311 \n",
      "(GPU: 0, epoch: 15, iters: 100640, time: 0.008) nll: 3227.320312 \n",
      "(GPU: 0, epoch: 15, iters: 101440, time: 0.008) nll: 3141.549316 \n",
      "(GPU: 0, epoch: 15, iters: 102240, time: 0.008) nll: 4870.390625 \n",
      "(GPU: 0, epoch: 15, iters: 103040, time: 0.008) nll: 3981.125488 \n",
      "(GPU: 0, epoch: 15, iters: 103840, time: 0.008) nll: 4816.925781 \n",
      "(GPU: 0, epoch: 15, iters: 104640, time: 0.008) nll: 5088.305664 \n",
      "(GPU: 0, epoch: 15, iters: 105440, time: 0.008) nll: 5992.329102 \n",
      "(GPU: 0, epoch: 15, iters: 106240, time: 0.008) nll: 3996.258545 \n",
      "(GPU: 0, epoch: 15, iters: 107040, time: 0.008) nll: 5071.457031 \n",
      "(GPU: 0, epoch: 15, iters: 107840, time: 0.008) nll: 4947.279297 \n",
      "(GPU: 0, epoch: 15, iters: 108640, time: 0.008) nll: 4135.816406 \n",
      "(GPU: 0, epoch: 15, iters: 109440, time: 0.008) nll: 4872.610352 \n",
      "saving the latest model (epoch 15, total_steps 2220000)\n",
      "(GPU: 0, epoch: 15, iters: 110240, time: 0.008) nll: 4327.166016 \n",
      "(GPU: 0, epoch: 15, iters: 111040, time: 0.008) nll: 3879.846680 \n",
      "(GPU: 0, epoch: 15, iters: 111840, time: 0.007) nll: 4332.864746 \n",
      "(GPU: 0, epoch: 15, iters: 112640, time: 0.008) nll: 3178.684082 \n",
      "(GPU: 0, epoch: 15, iters: 113440, time: 0.008) nll: 2925.409668 \n",
      "(GPU: 0, epoch: 15, iters: 114240, time: 0.008) nll: 3707.189941 \n",
      "(GPU: 0, epoch: 15, iters: 115040, time: 0.007) nll: 3637.336426 \n",
      "(GPU: 0, epoch: 15, iters: 115840, time: 0.008) nll: 4001.287109 \n",
      "(GPU: 0, epoch: 15, iters: 116640, time: 0.008) nll: 3329.452148 \n",
      "(GPU: 0, epoch: 15, iters: 117440, time: 0.008) nll: 4197.726562 \n",
      "(GPU: 0, epoch: 15, iters: 118240, time: 0.007) nll: 5283.103516 \n",
      "(GPU: 0, epoch: 15, iters: 119040, time: 0.008) nll: 4468.895020 \n",
      "(GPU: 0, epoch: 15, iters: 119840, time: 0.008) nll: 4071.680176 \n",
      "(GPU: 0, epoch: 15, iters: 120640, time: 0.008) nll: 2948.224609 \n",
      "(GPU: 0, epoch: 15, iters: 121440, time: 0.007) nll: 4409.229492 \n",
      "(GPU: 0, epoch: 15, iters: 122240, time: 0.008) nll: 3610.132324 \n",
      "(GPU: 0, epoch: 15, iters: 123040, time: 0.008) nll: 3928.839600 \n",
      "(GPU: 0, epoch: 15, iters: 123840, time: 0.008) nll: 2820.701172 \n",
      "(GPU: 0, epoch: 15, iters: 124640, time: 0.008) nll: 4518.840820 \n",
      "(GPU: 0, epoch: 15, iters: 125440, time: 0.008) nll: 4560.483398 \n",
      "(GPU: 0, epoch: 15, iters: 126240, time: 0.008) nll: 4397.101562 \n",
      "(GPU: 0, epoch: 15, iters: 127040, time: 0.008) nll: 4036.793945 \n",
      "(GPU: 0, epoch: 15, iters: 127840, time: 0.007) nll: 3013.803711 \n",
      "(GPU: 0, epoch: 15, iters: 128640, time: 0.008) nll: 3446.419922 \n",
      "(GPU: 0, epoch: 15, iters: 129440, time: 0.008) nll: 3786.342041 \n",
      "saving the latest model (epoch 15, total_steps 2240000)\n",
      "(GPU: 0, epoch: 15, iters: 130240, time: 0.008) nll: 3842.046387 \n",
      "(GPU: 0, epoch: 15, iters: 131040, time: 0.008) nll: 2001.026123 \n",
      "(GPU: 0, epoch: 15, iters: 131840, time: 0.008) nll: 3541.878662 \n",
      "(GPU: 0, epoch: 15, iters: 132640, time: 0.007) nll: 4286.138672 \n",
      "(GPU: 0, epoch: 15, iters: 133440, time: 0.008) nll: 4354.933594 \n",
      "(GPU: 0, epoch: 15, iters: 134240, time: 0.007) nll: 3891.615723 \n",
      "(GPU: 0, epoch: 15, iters: 135040, time: 0.008) nll: 6083.093750 \n",
      "(GPU: 0, epoch: 15, iters: 135840, time: 0.007) nll: 3037.032959 \n",
      "(GPU: 0, epoch: 15, iters: 136640, time: 0.008) nll: 3980.521484 \n",
      "(GPU: 0, epoch: 15, iters: 137440, time: 0.007) nll: 2545.934814 \n",
      "(GPU: 0, epoch: 15, iters: 138240, time: 0.008) nll: 3685.598145 \n",
      "(GPU: 0, epoch: 15, iters: 139040, time: 0.008) nll: 4295.074707 \n",
      "(GPU: 0, epoch: 15, iters: 139840, time: 0.008) nll: 2686.913086 \n",
      "(GPU: 0, epoch: 15, iters: 140640, time: 0.008) nll: 3047.284424 \n",
      "saving the model at the end of epoch 15, iters 2251264\n",
      "([test] GPU: 0, epoch: 15) \n",
      "OrderedDict()\n",
      "[*] End of epoch 15 / 25 \t Time Taken: 1255 sec \n",
      "/cluster/54/streakfull/ADL4CV/Project/src/plz-autosdf/logs/bert2vqsc-text2shape-seq-LR1e-4-new-bert-2-prev_z_shape_input-try-2\n",
      "[*] learning rate = 0.0000791\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 2972/4397 [13:54<06:10,  3.85it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 16, iters: 32, time: 0.004) nll: 3749.295166 \n",
      "(GPU: 0, epoch: 16, iters: 32, time: 0.004) nll: 3990.479004 \n",
      "(GPU: 0, epoch: 16, iters: 736, time: 0.007) nll: 2940.436035 \n",
      "(GPU: 0, epoch: 16, iters: 1536, time: 0.008) nll: 4017.533203 \n",
      "(GPU: 0, epoch: 16, iters: 2336, time: 0.008) nll: 3834.712402 \n",
      "(GPU: 0, epoch: 16, iters: 3136, time: 0.008) nll: 3667.994629 \n",
      "(GPU: 0, epoch: 16, iters: 3936, time: 0.008) nll: 3280.178223 \n",
      "(GPU: 0, epoch: 16, iters: 4736, time: 0.008) nll: 4034.731689 \n",
      "(GPU: 0, epoch: 16, iters: 5536, time: 0.008) nll: 3816.036377 \n",
      "(GPU: 0, epoch: 16, iters: 6336, time: 0.008) nll: 4178.203125 \n",
      "(GPU: 0, epoch: 16, iters: 7136, time: 0.008) nll: 5087.334961 \n",
      "(GPU: 0, epoch: 16, iters: 7936, time: 0.008) nll: 3574.175293 \n",
      "(GPU: 0, epoch: 16, iters: 8736, time: 0.007) nll: 1846.432983 \n",
      "saving the latest model (epoch 16, total_steps 2260000)\n",
      "(GPU: 0, epoch: 16, iters: 9536, time: 0.008) nll: 2757.159424 \n",
      "(GPU: 0, epoch: 16, iters: 10336, time: 0.008) nll: 4338.354492 \n",
      "(GPU: 0, epoch: 16, iters: 11136, time: 0.008) nll: 3861.903809 \n",
      "(GPU: 0, epoch: 16, iters: 11936, time: 0.008) nll: 4283.644531 \n",
      "(GPU: 0, epoch: 16, iters: 12736, time: 0.008) nll: 3320.282715 \n",
      "(GPU: 0, epoch: 16, iters: 13536, time: 0.008) nll: 4570.724121 \n",
      "(GPU: 0, epoch: 16, iters: 14336, time: 0.008) nll: 4727.404297 \n",
      "(GPU: 0, epoch: 16, iters: 15136, time: 0.008) nll: 5895.211914 \n",
      "(GPU: 0, epoch: 16, iters: 15936, time: 0.008) nll: 4274.916504 \n",
      "(GPU: 0, epoch: 16, iters: 16736, time: 0.008) nll: 3148.556885 \n",
      "(GPU: 0, epoch: 16, iters: 17536, time: 0.008) nll: 4448.672363 \n",
      "(GPU: 0, epoch: 16, iters: 18336, time: 0.008) nll: 2969.649170 \n",
      "(GPU: 0, epoch: 16, iters: 19136, time: 0.008) nll: 3180.067383 \n",
      "(GPU: 0, epoch: 16, iters: 19936, time: 0.008) nll: 3566.750244 \n",
      "(GPU: 0, epoch: 16, iters: 20736, time: 0.008) nll: 5173.041992 \n",
      "(GPU: 0, epoch: 16, iters: 21536, time: 0.008) nll: 4071.284180 \n",
      "(GPU: 0, epoch: 16, iters: 22336, time: 0.008) nll: 3252.818359 \n",
      "(GPU: 0, epoch: 16, iters: 23136, time: 0.008) nll: 3497.414551 \n",
      "(GPU: 0, epoch: 16, iters: 23936, time: 0.008) nll: 3674.277588 \n",
      "(GPU: 0, epoch: 16, iters: 24736, time: 0.008) nll: 4798.931641 \n",
      "(GPU: 0, epoch: 16, iters: 25536, time: 0.008) nll: 3145.674805 \n",
      "(GPU: 0, epoch: 16, iters: 26336, time: 0.008) nll: 4376.613281 \n",
      "(GPU: 0, epoch: 16, iters: 27136, time: 0.008) nll: 4039.015869 \n",
      "(GPU: 0, epoch: 16, iters: 27936, time: 0.008) nll: 4532.706055 \n",
      "(GPU: 0, epoch: 16, iters: 28736, time: 0.008) nll: 3130.757324 \n",
      "saving the latest model (epoch 16, total_steps 2280000)\n",
      "(GPU: 0, epoch: 16, iters: 29536, time: 0.007) nll: 4017.133301 \n",
      "(GPU: 0, epoch: 16, iters: 30336, time: 0.008) nll: 5042.903320 \n",
      "(GPU: 0, epoch: 16, iters: 31136, time: 0.008) nll: 5042.040039 \n",
      "(GPU: 0, epoch: 16, iters: 31936, time: 0.008) nll: 5211.598633 \n",
      "(GPU: 0, epoch: 16, iters: 32736, time: 0.008) nll: 2934.893066 \n",
      "(GPU: 0, epoch: 16, iters: 33536, time: 0.008) nll: 3341.387939 \n",
      "(GPU: 0, epoch: 16, iters: 34336, time: 0.008) nll: 3451.276855 \n",
      "(GPU: 0, epoch: 16, iters: 35136, time: 0.008) nll: 4863.092773 \n",
      "(GPU: 0, epoch: 16, iters: 35936, time: 0.007) nll: 3966.505859 \n",
      "(GPU: 0, epoch: 16, iters: 36736, time: 0.008) nll: 3681.257324 \n",
      "(GPU: 0, epoch: 16, iters: 37536, time: 0.008) nll: 5076.934570 \n",
      "(GPU: 0, epoch: 16, iters: 38336, time: 0.008) nll: 3975.650879 \n",
      "(GPU: 0, epoch: 16, iters: 39136, time: 0.008) nll: 4572.944336 \n",
      "(GPU: 0, epoch: 16, iters: 39936, time: 0.008) nll: 4604.644531 \n",
      "(GPU: 0, epoch: 16, iters: 40736, time: 0.008) nll: 2805.361328 \n",
      "(GPU: 0, epoch: 16, iters: 41536, time: 0.008) nll: 4052.113770 \n",
      "(GPU: 0, epoch: 16, iters: 42336, time: 0.007) nll: 4002.143311 \n",
      "(GPU: 0, epoch: 16, iters: 43136, time: 0.008) nll: 2599.487793 \n",
      "(GPU: 0, epoch: 16, iters: 43936, time: 0.007) nll: 3908.079590 \n",
      "(GPU: 0, epoch: 16, iters: 44736, time: 0.008) nll: 3688.636719 \n",
      "(GPU: 0, epoch: 16, iters: 45536, time: 0.008) nll: 3381.018799 \n",
      "(GPU: 0, epoch: 16, iters: 46336, time: 0.008) nll: 3167.364990 \n",
      "(GPU: 0, epoch: 16, iters: 47136, time: 0.008) nll: 4416.057617 \n",
      "(GPU: 0, epoch: 16, iters: 47936, time: 0.008) nll: 4236.662109 \n",
      "(GPU: 0, epoch: 16, iters: 48736, time: 0.008) nll: 3958.412354 \n",
      "saving the latest model (epoch 16, total_steps 2300000)\n",
      "(GPU: 0, epoch: 16, iters: 49536, time: 0.008) nll: 4720.694336 \n",
      "(GPU: 0, epoch: 16, iters: 50336, time: 0.008) nll: 3934.207031 \n",
      "(GPU: 0, epoch: 16, iters: 51136, time: 0.008) nll: 4047.088379 \n",
      "(GPU: 0, epoch: 16, iters: 51936, time: 0.008) nll: 3326.412598 \n",
      "(GPU: 0, epoch: 16, iters: 52736, time: 0.008) nll: 3202.915039 \n",
      "(GPU: 0, epoch: 16, iters: 52736, time: 0.013) nll: 3165.431396 \n",
      "(GPU: 0, epoch: 16, iters: 52736, time: 0.013) nll: 3906.033691 \n",
      "(GPU: 0, epoch: 16, iters: 53536, time: 0.008) nll: 2785.224854 \n",
      "(GPU: 0, epoch: 16, iters: 54336, time: 0.008) nll: 4482.997070 \n",
      "(GPU: 0, epoch: 16, iters: 55136, time: 0.008) nll: 4095.672363 \n",
      "(GPU: 0, epoch: 16, iters: 55936, time: 0.008) nll: 5066.185059 \n",
      "(GPU: 0, epoch: 16, iters: 56736, time: 0.008) nll: 4405.443359 \n",
      "(GPU: 0, epoch: 16, iters: 57536, time: 0.008) nll: 4133.182129 \n",
      "(GPU: 0, epoch: 16, iters: 58336, time: 0.008) nll: 1960.041870 \n",
      "(GPU: 0, epoch: 16, iters: 59136, time: 0.008) nll: 3240.660645 \n",
      "(GPU: 0, epoch: 16, iters: 59936, time: 0.007) nll: 2817.446045 \n",
      "(GPU: 0, epoch: 16, iters: 60736, time: 0.008) nll: 4058.638428 \n",
      "(GPU: 0, epoch: 16, iters: 61536, time: 0.008) nll: 3025.529053 \n",
      "(GPU: 0, epoch: 16, iters: 62336, time: 0.008) nll: 4464.443359 \n",
      "(GPU: 0, epoch: 16, iters: 63136, time: 0.008) nll: 4151.407227 \n",
      "(GPU: 0, epoch: 16, iters: 63936, time: 0.008) nll: 3995.413818 \n",
      "(GPU: 0, epoch: 16, iters: 64736, time: 0.007) nll: 3730.346680 \n",
      "(GPU: 0, epoch: 16, iters: 65536, time: 0.008) nll: 2598.206543 \n",
      "(GPU: 0, epoch: 16, iters: 66336, time: 0.007) nll: 4582.751953 \n",
      "(GPU: 0, epoch: 16, iters: 67136, time: 0.008) nll: 3497.540039 \n",
      "(GPU: 0, epoch: 16, iters: 67936, time: 0.007) nll: 3340.085449 \n",
      "(GPU: 0, epoch: 16, iters: 68736, time: 0.008) nll: 3287.212646 \n",
      "saving the latest model (epoch 16, total_steps 2320000)\n",
      "(GPU: 0, epoch: 16, iters: 69536, time: 0.007) nll: 3876.271240 \n",
      "(GPU: 0, epoch: 16, iters: 70336, time: 0.008) nll: 4209.428711 \n",
      "(GPU: 0, epoch: 16, iters: 71136, time: 0.008) nll: 4089.703613 \n",
      "(GPU: 0, epoch: 16, iters: 71936, time: 0.008) nll: 3009.299561 \n",
      "(GPU: 0, epoch: 16, iters: 72736, time: 0.007) nll: 4071.183105 \n",
      "(GPU: 0, epoch: 16, iters: 73536, time: 0.008) nll: 2545.456055 \n",
      "(GPU: 0, epoch: 16, iters: 74336, time: 0.007) nll: 3459.000488 \n",
      "(GPU: 0, epoch: 16, iters: 75136, time: 0.008) nll: 4430.105469 \n",
      "(GPU: 0, epoch: 16, iters: 75936, time: 0.008) nll: 4775.094727 \n",
      "(GPU: 0, epoch: 16, iters: 76736, time: 0.008) nll: 3338.767090 \n",
      "(GPU: 0, epoch: 16, iters: 77536, time: 0.008) nll: 3763.568848 \n",
      "(GPU: 0, epoch: 16, iters: 78336, time: 0.008) nll: 2930.197510 \n",
      "(GPU: 0, epoch: 16, iters: 79136, time: 0.008) nll: 3826.118652 \n",
      "(GPU: 0, epoch: 16, iters: 79936, time: 0.008) nll: 4298.586914 \n",
      "(GPU: 0, epoch: 16, iters: 80736, time: 0.008) nll: 3270.246826 \n",
      "(GPU: 0, epoch: 16, iters: 81536, time: 0.008) nll: 3854.863770 \n",
      "(GPU: 0, epoch: 16, iters: 82336, time: 0.008) nll: 3018.008057 \n",
      "(GPU: 0, epoch: 16, iters: 83136, time: 0.008) nll: 4543.422363 \n",
      "(GPU: 0, epoch: 16, iters: 83936, time: 0.008) nll: 4257.499023 \n",
      "(GPU: 0, epoch: 16, iters: 84736, time: 0.008) nll: 4200.918945 \n",
      "(GPU: 0, epoch: 16, iters: 85536, time: 0.008) nll: 3687.645508 \n",
      "(GPU: 0, epoch: 16, iters: 86336, time: 0.008) nll: 3925.570312 \n",
      "(GPU: 0, epoch: 16, iters: 87136, time: 0.008) nll: 4156.971680 \n",
      "(GPU: 0, epoch: 16, iters: 87936, time: 0.008) nll: 5293.863281 \n",
      "(GPU: 0, epoch: 16, iters: 88736, time: 0.008) nll: 4431.997070 \n",
      "saving the latest model (epoch 16, total_steps 2340000)\n",
      "(GPU: 0, epoch: 16, iters: 89536, time: 0.008) nll: 3028.954590 \n",
      "(GPU: 0, epoch: 16, iters: 90336, time: 0.008) nll: 2651.413086 \n",
      "(GPU: 0, epoch: 16, iters: 91136, time: 0.008) nll: 4899.803711 \n",
      "(GPU: 0, epoch: 16, iters: 91936, time: 0.007) nll: 4674.090820 \n",
      "(GPU: 0, epoch: 16, iters: 92736, time: 0.008) nll: 4764.845703 \n",
      "(GPU: 0, epoch: 16, iters: 93536, time: 0.007) nll: 4991.824219 \n",
      "(GPU: 0, epoch: 16, iters: 94336, time: 0.008) nll: 4969.734375 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4397/4397 [20:30<00:00,  3.57it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 16, iters: 95136, time: 0.007) nll: 2676.820312 \n",
      "(GPU: 0, epoch: 16, iters: 95936, time: 0.008) nll: 4319.953125 \n",
      "(GPU: 0, epoch: 16, iters: 96736, time: 0.008) nll: 4079.735352 \n",
      "(GPU: 0, epoch: 16, iters: 97536, time: 0.008) nll: 3641.457031 \n",
      "(GPU: 0, epoch: 16, iters: 98336, time: 0.008) nll: 3435.590820 \n",
      "(GPU: 0, epoch: 16, iters: 99136, time: 0.008) nll: 3673.978271 \n",
      "(GPU: 0, epoch: 16, iters: 99936, time: 0.008) nll: 3443.071777 \n",
      "(GPU: 0, epoch: 16, iters: 100736, time: 0.008) nll: 3482.779541 \n",
      "(GPU: 0, epoch: 16, iters: 101536, time: 0.008) nll: 3096.409424 \n",
      "(GPU: 0, epoch: 16, iters: 102336, time: 0.008) nll: 4787.875977 \n",
      "(GPU: 0, epoch: 16, iters: 103136, time: 0.007) nll: 4940.549805 \n",
      "(GPU: 0, epoch: 16, iters: 103936, time: 0.008) nll: 5030.423828 \n",
      "(GPU: 0, epoch: 16, iters: 104736, time: 0.007) nll: 2822.328125 \n",
      "(GPU: 0, epoch: 16, iters: 105536, time: 0.008) nll: 4124.910156 \n",
      "(GPU: 0, epoch: 16, iters: 106336, time: 0.008) nll: 4075.458496 \n",
      "(GPU: 0, epoch: 16, iters: 107136, time: 0.008) nll: 4463.763672 \n",
      "(GPU: 0, epoch: 16, iters: 107936, time: 0.008) nll: 3183.945312 \n",
      "(GPU: 0, epoch: 16, iters: 108736, time: 0.008) nll: 3171.413330 \n",
      "saving the latest model (epoch 16, total_steps 2360000)\n",
      "(GPU: 0, epoch: 16, iters: 109536, time: 0.008) nll: 4085.446289 \n",
      "(GPU: 0, epoch: 16, iters: 110336, time: 0.008) nll: 3782.532959 \n",
      "(GPU: 0, epoch: 16, iters: 111136, time: 0.008) nll: 5863.190430 \n",
      "(GPU: 0, epoch: 16, iters: 111936, time: 0.008) nll: 4677.388672 \n",
      "(GPU: 0, epoch: 16, iters: 112736, time: 0.008) nll: 3456.386719 \n",
      "(GPU: 0, epoch: 16, iters: 113536, time: 0.008) nll: 4595.047852 \n",
      "(GPU: 0, epoch: 16, iters: 114336, time: 0.008) nll: 3326.137695 \n",
      "(GPU: 0, epoch: 16, iters: 115136, time: 0.008) nll: 3824.275635 \n",
      "(GPU: 0, epoch: 16, iters: 115936, time: 0.008) nll: 3626.475586 \n",
      "(GPU: 0, epoch: 16, iters: 116736, time: 0.008) nll: 4274.282715 \n",
      "(GPU: 0, epoch: 16, iters: 117536, time: 0.008) nll: 4681.191406 \n",
      "(GPU: 0, epoch: 16, iters: 118336, time: 0.008) nll: 3256.062500 \n",
      "(GPU: 0, epoch: 16, iters: 119136, time: 0.008) nll: 3210.453125 \n",
      "(GPU: 0, epoch: 16, iters: 119936, time: 0.008) nll: 2978.016357 \n",
      "(GPU: 0, epoch: 16, iters: 120736, time: 0.008) nll: 3177.917236 \n",
      "(GPU: 0, epoch: 16, iters: 121536, time: 0.008) nll: 3005.685547 \n",
      "(GPU: 0, epoch: 16, iters: 122336, time: 0.008) nll: 3922.352539 \n",
      "(GPU: 0, epoch: 16, iters: 123136, time: 0.008) nll: 3698.105957 \n",
      "(GPU: 0, epoch: 16, iters: 123936, time: 0.008) nll: 3855.516602 \n",
      "(GPU: 0, epoch: 16, iters: 124736, time: 0.008) nll: 4894.187988 \n",
      "(GPU: 0, epoch: 16, iters: 125536, time: 0.007) nll: 4422.748535 \n",
      "(GPU: 0, epoch: 16, iters: 126336, time: 0.008) nll: 2898.010254 \n",
      "(GPU: 0, epoch: 16, iters: 127136, time: 0.008) nll: 3601.170410 \n",
      "(GPU: 0, epoch: 16, iters: 127936, time: 0.008) nll: 4398.073730 \n",
      "(GPU: 0, epoch: 16, iters: 128736, time: 0.007) nll: 3997.464355 \n",
      "saving the latest model (epoch 16, total_steps 2380000)\n",
      "(GPU: 0, epoch: 16, iters: 129536, time: 0.008) nll: 3936.679199 \n",
      "(GPU: 0, epoch: 16, iters: 130336, time: 0.008) nll: 3218.229004 \n",
      "(GPU: 0, epoch: 16, iters: 131136, time: 0.008) nll: 2654.848389 \n",
      "(GPU: 0, epoch: 16, iters: 131936, time: 0.008) nll: 5716.444824 \n",
      "(GPU: 0, epoch: 16, iters: 132736, time: 0.008) nll: 5276.837402 \n",
      "(GPU: 0, epoch: 16, iters: 133536, time: 0.008) nll: 2760.668457 \n",
      "(GPU: 0, epoch: 16, iters: 134336, time: 0.008) nll: 4432.567383 \n",
      "(GPU: 0, epoch: 16, iters: 135136, time: 0.008) nll: 3893.955811 \n",
      "(GPU: 0, epoch: 16, iters: 135936, time: 0.008) nll: 3538.347168 \n",
      "(GPU: 0, epoch: 16, iters: 136736, time: 0.008) nll: 4070.171143 \n",
      "(GPU: 0, epoch: 16, iters: 137536, time: 0.008) nll: 3160.469727 \n",
      "(GPU: 0, epoch: 16, iters: 138336, time: 0.008) nll: 2748.487305 \n",
      "(GPU: 0, epoch: 16, iters: 139136, time: 0.008) nll: 4936.799805 \n",
      "(GPU: 0, epoch: 16, iters: 139936, time: 0.008) nll: 3957.134521 \n",
      "[*] End of epoch 16 / 25 \t Time Taken: 1231 sec \n",
      "/cluster/54/streakfull/ADL4CV/Project/src/plz-autosdf/logs/bert2vqsc-text2shape-seq-LR1e-4-new-bert-2-prev_z_shape_input-try-2\n",
      "[*] learning rate = 0.0000767\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2950/4397 [13:50<06:12,  3.88it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 17, iters: 32, time: 0.004) nll: 3489.705078 \n",
      "(GPU: 0, epoch: 17, iters: 32, time: 0.009) nll: 3434.071289 \n",
      "(GPU: 0, epoch: 17, iters: 32, time: 0.009) nll: 4372.616211 \n",
      "(GPU: 0, epoch: 17, iters: 832, time: 0.008) nll: 2761.530273 \n",
      "(GPU: 0, epoch: 17, iters: 1632, time: 0.008) nll: 4104.768555 \n",
      "(GPU: 0, epoch: 17, iters: 2432, time: 0.008) nll: 3477.235107 \n",
      "(GPU: 0, epoch: 17, iters: 3232, time: 0.008) nll: 4040.131836 \n",
      "(GPU: 0, epoch: 17, iters: 4032, time: 0.008) nll: 3861.796387 \n",
      "(GPU: 0, epoch: 17, iters: 4832, time: 0.008) nll: 3200.195801 \n",
      "(GPU: 0, epoch: 17, iters: 5632, time: 0.008) nll: 3063.984375 \n",
      "(GPU: 0, epoch: 17, iters: 6432, time: 0.008) nll: 5343.062012 \n",
      "(GPU: 0, epoch: 17, iters: 7232, time: 0.008) nll: 3816.371338 \n",
      "(GPU: 0, epoch: 17, iters: 8032, time: 0.008) nll: 3679.773438 \n",
      "(GPU: 0, epoch: 17, iters: 8032, time: 0.012) nll: 3651.815918 \n",
      "(GPU: 0, epoch: 17, iters: 8032, time: 0.012) nll: 3796.581543 \n",
      "saving the latest model (epoch 17, total_steps 2400000)\n",
      "(GPU: 0, epoch: 17, iters: 8832, time: 0.008) nll: 3494.517578 \n",
      "(GPU: 0, epoch: 17, iters: 9632, time: 0.008) nll: 3993.237305 \n",
      "(GPU: 0, epoch: 17, iters: 10432, time: 0.008) nll: 4114.232910 \n",
      "(GPU: 0, epoch: 17, iters: 11232, time: 0.008) nll: 3485.980469 \n",
      "(GPU: 0, epoch: 17, iters: 12032, time: 0.008) nll: 3295.567383 \n",
      "(GPU: 0, epoch: 17, iters: 12832, time: 0.008) nll: 3433.116211 \n",
      "(GPU: 0, epoch: 17, iters: 13632, time: 0.008) nll: 4439.250488 \n",
      "(GPU: 0, epoch: 17, iters: 14432, time: 0.008) nll: 3841.223389 \n",
      "(GPU: 0, epoch: 17, iters: 15232, time: 0.008) nll: 4853.613281 \n",
      "(GPU: 0, epoch: 17, iters: 16032, time: 0.008) nll: 3299.178711 \n",
      "(GPU: 0, epoch: 17, iters: 16832, time: 0.008) nll: 3825.224609 \n",
      "(GPU: 0, epoch: 17, iters: 17632, time: 0.008) nll: 3083.012207 \n",
      "(GPU: 0, epoch: 17, iters: 18432, time: 0.008) nll: 3863.584473 \n",
      "(GPU: 0, epoch: 17, iters: 19232, time: 0.008) nll: 4566.010742 \n",
      "(GPU: 0, epoch: 17, iters: 20032, time: 0.008) nll: 4059.947754 \n",
      "(GPU: 0, epoch: 17, iters: 20832, time: 0.008) nll: 4102.972168 \n",
      "(GPU: 0, epoch: 17, iters: 21632, time: 0.008) nll: 4019.850586 \n",
      "(GPU: 0, epoch: 17, iters: 22432, time: 0.008) nll: 5341.694824 \n",
      "(GPU: 0, epoch: 17, iters: 23232, time: 0.008) nll: 3017.178711 \n",
      "(GPU: 0, epoch: 17, iters: 24032, time: 0.007) nll: 2662.827148 \n",
      "(GPU: 0, epoch: 17, iters: 24832, time: 0.008) nll: 4012.986572 \n",
      "(GPU: 0, epoch: 17, iters: 25632, time: 0.008) nll: 2862.581543 \n",
      "(GPU: 0, epoch: 17, iters: 26432, time: 0.008) nll: 4008.977539 \n",
      "(GPU: 0, epoch: 17, iters: 27232, time: 0.007) nll: 4066.689453 \n",
      "(GPU: 0, epoch: 17, iters: 28032, time: 0.008) nll: 3006.565918 \n",
      "saving the latest model (epoch 17, total_steps 2420000)\n",
      "(GPU: 0, epoch: 17, iters: 28832, time: 0.008) nll: 2827.745605 \n",
      "(GPU: 0, epoch: 17, iters: 29632, time: 0.008) nll: 5163.451660 \n",
      "(GPU: 0, epoch: 17, iters: 30432, time: 0.008) nll: 3667.665527 \n",
      "(GPU: 0, epoch: 17, iters: 31232, time: 0.008) nll: 2824.036621 \n",
      "(GPU: 0, epoch: 17, iters: 32032, time: 0.008) nll: 3345.126465 \n",
      "(GPU: 0, epoch: 17, iters: 32832, time: 0.008) nll: 3980.558594 \n",
      "(GPU: 0, epoch: 17, iters: 33632, time: 0.008) nll: 2792.083496 \n",
      "(GPU: 0, epoch: 17, iters: 34432, time: 0.008) nll: 4160.752930 \n",
      "(GPU: 0, epoch: 17, iters: 35232, time: 0.008) nll: 3537.860352 \n",
      "(GPU: 0, epoch: 17, iters: 36032, time: 0.008) nll: 4010.757812 \n",
      "(GPU: 0, epoch: 17, iters: 36832, time: 0.008) nll: 2140.365234 \n",
      "(GPU: 0, epoch: 17, iters: 37632, time: 0.008) nll: 3845.707275 \n",
      "(GPU: 0, epoch: 17, iters: 38432, time: 0.008) nll: 3345.793945 \n",
      "(GPU: 0, epoch: 17, iters: 39232, time: 0.008) nll: 3211.726318 \n",
      "(GPU: 0, epoch: 17, iters: 40032, time: 0.008) nll: 4321.676758 \n",
      "(GPU: 0, epoch: 17, iters: 40832, time: 0.008) nll: 4294.135742 \n",
      "(GPU: 0, epoch: 17, iters: 41632, time: 0.008) nll: 3838.178223 \n",
      "(GPU: 0, epoch: 17, iters: 42432, time: 0.008) nll: 3749.419922 \n",
      "(GPU: 0, epoch: 17, iters: 43232, time: 0.008) nll: 4195.322266 \n",
      "(GPU: 0, epoch: 17, iters: 44032, time: 0.008) nll: 4379.020508 \n",
      "(GPU: 0, epoch: 17, iters: 44832, time: 0.008) nll: 4726.997070 \n",
      "(GPU: 0, epoch: 17, iters: 45632, time: 0.008) nll: 3364.034668 \n",
      "(GPU: 0, epoch: 17, iters: 46432, time: 0.008) nll: 3022.658203 \n",
      "(GPU: 0, epoch: 17, iters: 47232, time: 0.008) nll: 5228.614746 \n",
      "(GPU: 0, epoch: 17, iters: 48032, time: 0.008) nll: 3686.862549 \n",
      "saving the latest model (epoch 17, total_steps 2440000)\n",
      "(GPU: 0, epoch: 17, iters: 48832, time: 0.008) nll: 3087.580566 \n",
      "(GPU: 0, epoch: 17, iters: 49632, time: 0.007) nll: 3956.753906 \n",
      "(GPU: 0, epoch: 17, iters: 50432, time: 0.008) nll: 3642.156006 \n",
      "(GPU: 0, epoch: 17, iters: 51232, time: 0.008) nll: 3846.124268 \n",
      "(GPU: 0, epoch: 17, iters: 52032, time: 0.008) nll: 2704.753906 \n",
      "(GPU: 0, epoch: 17, iters: 52832, time: 0.008) nll: 3728.561035 \n",
      "(GPU: 0, epoch: 17, iters: 53632, time: 0.008) nll: 4248.174316 \n",
      "(GPU: 0, epoch: 17, iters: 54432, time: 0.007) nll: 4179.154297 \n",
      "(GPU: 0, epoch: 17, iters: 55232, time: 0.008) nll: 3273.234375 \n",
      "(GPU: 0, epoch: 17, iters: 56032, time: 0.008) nll: 4222.477539 \n",
      "(GPU: 0, epoch: 17, iters: 56832, time: 0.008) nll: 5171.952637 \n",
      "(GPU: 0, epoch: 17, iters: 57632, time: 0.008) nll: 3727.894775 \n",
      "(GPU: 0, epoch: 17, iters: 58432, time: 0.008) nll: 5043.134766 \n",
      "(GPU: 0, epoch: 17, iters: 59232, time: 0.008) nll: 3785.804199 \n",
      "(GPU: 0, epoch: 17, iters: 60032, time: 0.008) nll: 2783.625488 \n",
      "(GPU: 0, epoch: 17, iters: 60832, time: 0.008) nll: 2824.315430 \n",
      "(GPU: 0, epoch: 17, iters: 61632, time: 0.008) nll: 2530.407715 \n",
      "(GPU: 0, epoch: 17, iters: 62432, time: 0.008) nll: 3665.392090 \n",
      "(GPU: 0, epoch: 17, iters: 63232, time: 0.008) nll: 3683.716309 \n",
      "(GPU: 0, epoch: 17, iters: 64032, time: 0.008) nll: 3767.665039 \n",
      "(GPU: 0, epoch: 17, iters: 64832, time: 0.008) nll: 3133.664551 \n",
      "(GPU: 0, epoch: 17, iters: 65632, time: 0.008) nll: 3366.290039 \n",
      "(GPU: 0, epoch: 17, iters: 66432, time: 0.008) nll: 4378.696289 \n",
      "(GPU: 0, epoch: 17, iters: 67232, time: 0.008) nll: 2864.630859 \n",
      "(GPU: 0, epoch: 17, iters: 68032, time: 0.008) nll: 3562.256348 \n",
      "saving the latest model (epoch 17, total_steps 2460000)\n",
      "(GPU: 0, epoch: 17, iters: 68832, time: 0.007) nll: 2679.452148 \n",
      "(GPU: 0, epoch: 17, iters: 69632, time: 0.008) nll: 3913.338379 \n",
      "(GPU: 0, epoch: 17, iters: 70432, time: 0.007) nll: 5326.479004 \n",
      "(GPU: 0, epoch: 17, iters: 71232, time: 0.008) nll: 2466.854736 \n",
      "(GPU: 0, epoch: 17, iters: 72032, time: 0.007) nll: 3959.813965 \n",
      "(GPU: 0, epoch: 17, iters: 72832, time: 0.008) nll: 4132.474609 \n",
      "(GPU: 0, epoch: 17, iters: 73632, time: 0.008) nll: 3514.271484 \n",
      "(GPU: 0, epoch: 17, iters: 74432, time: 0.008) nll: 4025.413574 \n",
      "(GPU: 0, epoch: 17, iters: 75232, time: 0.008) nll: 3610.910645 \n",
      "(GPU: 0, epoch: 17, iters: 76032, time: 0.008) nll: 2528.406738 \n",
      "(GPU: 0, epoch: 17, iters: 76832, time: 0.008) nll: 3747.508301 \n",
      "(GPU: 0, epoch: 17, iters: 77632, time: 0.008) nll: 4146.653320 \n",
      "(GPU: 0, epoch: 17, iters: 78432, time: 0.008) nll: 4381.898926 \n",
      "(GPU: 0, epoch: 17, iters: 79232, time: 0.008) nll: 4089.720215 \n",
      "(GPU: 0, epoch: 17, iters: 80032, time: 0.008) nll: 3993.026855 \n",
      "(GPU: 0, epoch: 17, iters: 80832, time: 0.008) nll: 2531.391602 \n",
      "(GPU: 0, epoch: 17, iters: 81632, time: 0.008) nll: 4633.083008 \n",
      "(GPU: 0, epoch: 17, iters: 82432, time: 0.008) nll: 2901.478027 \n",
      "(GPU: 0, epoch: 17, iters: 83232, time: 0.008) nll: 2724.248779 \n",
      "(GPU: 0, epoch: 17, iters: 84032, time: 0.008) nll: 2932.866211 \n",
      "(GPU: 0, epoch: 17, iters: 84832, time: 0.008) nll: 3517.884277 \n",
      "(GPU: 0, epoch: 17, iters: 85632, time: 0.008) nll: 3126.168701 \n",
      "(GPU: 0, epoch: 17, iters: 86432, time: 0.008) nll: 3962.591309 \n",
      "(GPU: 0, epoch: 17, iters: 87232, time: 0.008) nll: 3979.820312 \n",
      "(GPU: 0, epoch: 17, iters: 88032, time: 0.008) nll: 3819.303711 \n",
      "saving the latest model (epoch 17, total_steps 2480000)\n",
      "(GPU: 0, epoch: 17, iters: 88832, time: 0.008) nll: 3579.047852 \n",
      "(GPU: 0, epoch: 17, iters: 89632, time: 0.008) nll: 4483.980469 \n",
      "(GPU: 0, epoch: 17, iters: 90432, time: 0.008) nll: 3776.047119 \n",
      "(GPU: 0, epoch: 17, iters: 91232, time: 0.008) nll: 2868.128906 \n",
      "(GPU: 0, epoch: 17, iters: 92032, time: 0.008) nll: 5574.330078 \n",
      "(GPU: 0, epoch: 17, iters: 92832, time: 0.008) nll: 2963.614258 \n",
      "(GPU: 0, epoch: 17, iters: 93632, time: 0.008) nll: 2618.045410 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4397/4397 [20:31<00:00,  3.57it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 17, iters: 94432, time: 0.008) nll: 2973.500000 \n",
      "(GPU: 0, epoch: 17, iters: 95232, time: 0.008) nll: 3467.110352 \n",
      "(GPU: 0, epoch: 17, iters: 96032, time: 0.008) nll: 3848.513916 \n",
      "(GPU: 0, epoch: 17, iters: 96832, time: 0.008) nll: 5663.056641 \n",
      "(GPU: 0, epoch: 17, iters: 97632, time: 0.007) nll: 3478.849609 \n",
      "(GPU: 0, epoch: 17, iters: 98432, time: 0.008) nll: 4300.345703 \n",
      "(GPU: 0, epoch: 17, iters: 99232, time: 0.008) nll: 3974.689941 \n",
      "(GPU: 0, epoch: 17, iters: 100032, time: 0.008) nll: 4068.047607 \n",
      "(GPU: 0, epoch: 17, iters: 100832, time: 0.007) nll: 3752.996582 \n",
      "(GPU: 0, epoch: 17, iters: 101632, time: 0.008) nll: 4133.036133 \n",
      "(GPU: 0, epoch: 17, iters: 102432, time: 0.008) nll: 3718.607422 \n",
      "(GPU: 0, epoch: 17, iters: 103232, time: 0.008) nll: 4522.493652 \n",
      "(GPU: 0, epoch: 17, iters: 104032, time: 0.008) nll: 3736.518066 \n",
      "(GPU: 0, epoch: 17, iters: 104032, time: 0.013) nll: 3696.019531 \n",
      "(GPU: 0, epoch: 17, iters: 104032, time: 0.013) nll: 5212.546875 \n",
      "(GPU: 0, epoch: 17, iters: 104832, time: 0.008) nll: 2528.998535 \n",
      "(GPU: 0, epoch: 17, iters: 105632, time: 0.008) nll: 3778.075684 \n",
      "(GPU: 0, epoch: 17, iters: 106432, time: 0.008) nll: 3489.062988 \n",
      "(GPU: 0, epoch: 17, iters: 107232, time: 0.008) nll: 3936.976318 \n",
      "(GPU: 0, epoch: 17, iters: 108032, time: 0.008) nll: 3024.524414 \n",
      "saving the latest model (epoch 17, total_steps 2500000)\n",
      "(GPU: 0, epoch: 17, iters: 108832, time: 0.008) nll: 4648.163086 \n",
      "(GPU: 0, epoch: 17, iters: 109632, time: 0.008) nll: 4313.996094 \n",
      "(GPU: 0, epoch: 17, iters: 110432, time: 0.007) nll: 3767.656494 \n",
      "(GPU: 0, epoch: 17, iters: 111232, time: 0.008) nll: 3258.266113 \n",
      "(GPU: 0, epoch: 17, iters: 112032, time: 0.008) nll: 2825.802734 \n",
      "(GPU: 0, epoch: 17, iters: 112832, time: 0.008) nll: 5368.492188 \n",
      "(GPU: 0, epoch: 17, iters: 113632, time: 0.007) nll: 2786.738281 \n",
      "(GPU: 0, epoch: 17, iters: 114432, time: 0.008) nll: 3302.411133 \n",
      "(GPU: 0, epoch: 17, iters: 115232, time: 0.008) nll: 4387.822754 \n",
      "(GPU: 0, epoch: 17, iters: 116032, time: 0.008) nll: 4067.317139 \n",
      "(GPU: 0, epoch: 17, iters: 116832, time: 0.008) nll: 3736.521729 \n",
      "(GPU: 0, epoch: 17, iters: 117632, time: 0.008) nll: 3677.101807 \n",
      "(GPU: 0, epoch: 17, iters: 118432, time: 0.008) nll: 3269.030762 \n",
      "(GPU: 0, epoch: 17, iters: 119232, time: 0.008) nll: 3287.108398 \n",
      "(GPU: 0, epoch: 17, iters: 120032, time: 0.008) nll: 2232.317383 \n",
      "(GPU: 0, epoch: 17, iters: 120832, time: 0.008) nll: 4692.369141 \n",
      "(GPU: 0, epoch: 17, iters: 121632, time: 0.008) nll: 4350.799316 \n",
      "(GPU: 0, epoch: 17, iters: 122432, time: 0.008) nll: 2859.867676 \n",
      "(GPU: 0, epoch: 17, iters: 123232, time: 0.008) nll: 3039.934082 \n",
      "(GPU: 0, epoch: 17, iters: 124032, time: 0.008) nll: 3974.926758 \n",
      "(GPU: 0, epoch: 17, iters: 124832, time: 0.008) nll: 3615.241699 \n",
      "(GPU: 0, epoch: 17, iters: 125632, time: 0.008) nll: 3640.941162 \n",
      "(GPU: 0, epoch: 17, iters: 126432, time: 0.008) nll: 3797.823242 \n",
      "(GPU: 0, epoch: 17, iters: 127232, time: 0.008) nll: 2709.313721 \n",
      "(GPU: 0, epoch: 17, iters: 128032, time: 0.008) nll: 3907.861816 \n",
      "saving the latest model (epoch 17, total_steps 2520000)\n",
      "(GPU: 0, epoch: 17, iters: 128832, time: 0.008) nll: 4046.508789 \n",
      "(GPU: 0, epoch: 17, iters: 129632, time: 0.008) nll: 3552.343506 \n",
      "(GPU: 0, epoch: 17, iters: 130432, time: 0.008) nll: 2860.853516 \n",
      "(GPU: 0, epoch: 17, iters: 131232, time: 0.008) nll: 3021.444336 \n",
      "(GPU: 0, epoch: 17, iters: 132032, time: 0.008) nll: 3652.512695 \n",
      "(GPU: 0, epoch: 17, iters: 132832, time: 0.008) nll: 3539.629395 \n",
      "(GPU: 0, epoch: 17, iters: 133632, time: 0.008) nll: 3172.712891 \n",
      "(GPU: 0, epoch: 17, iters: 134432, time: 0.008) nll: 4883.970703 \n",
      "(GPU: 0, epoch: 17, iters: 135232, time: 0.008) nll: 3659.041504 \n",
      "(GPU: 0, epoch: 17, iters: 136032, time: 0.008) nll: 3511.337891 \n",
      "(GPU: 0, epoch: 17, iters: 136832, time: 0.008) nll: 4264.585938 \n",
      "(GPU: 0, epoch: 17, iters: 137632, time: 0.008) nll: 2587.043945 \n",
      "(GPU: 0, epoch: 17, iters: 138432, time: 0.008) nll: 2870.758301 \n",
      "(GPU: 0, epoch: 17, iters: 139232, time: 0.008) nll: 3378.070312 \n",
      "(GPU: 0, epoch: 17, iters: 140032, time: 0.008) nll: 3677.316162 \n",
      "[*] End of epoch 17 / 25 \t Time Taken: 1232 sec \n",
      "/cluster/54/streakfull/ADL4CV/Project/src/plz-autosdf/logs/bert2vqsc-text2shape-seq-LR1e-4-new-bert-2-prev_z_shape_input-try-2\n",
      "[*] learning rate = 0.0000745\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2953/4397 [13:52<06:13,  3.87it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 18, iters: 32, time: 0.004) nll: 4206.989746 \n",
      "(GPU: 0, epoch: 18, iters: 32, time: 0.004) nll: 3845.361328 \n",
      "(GPU: 0, epoch: 18, iters: 128, time: 0.008) nll: 2924.981445 \n",
      "(GPU: 0, epoch: 18, iters: 928, time: 0.008) nll: 3075.679688 \n",
      "(GPU: 0, epoch: 18, iters: 1728, time: 0.008) nll: 2446.595215 \n",
      "(GPU: 0, epoch: 18, iters: 2528, time: 0.008) nll: 4267.708984 \n",
      "(GPU: 0, epoch: 18, iters: 3328, time: 0.008) nll: 3428.433350 \n",
      "(GPU: 0, epoch: 18, iters: 4128, time: 0.008) nll: 3686.967529 \n",
      "(GPU: 0, epoch: 18, iters: 4928, time: 0.008) nll: 3343.146973 \n",
      "(GPU: 0, epoch: 18, iters: 5728, time: 0.008) nll: 3283.947266 \n",
      "(GPU: 0, epoch: 18, iters: 6528, time: 0.008) nll: 2391.046875 \n",
      "(GPU: 0, epoch: 18, iters: 7328, time: 0.007) nll: 3602.422119 \n",
      "saving the latest model (epoch 18, total_steps 2540000)\n",
      "(GPU: 0, epoch: 18, iters: 8128, time: 0.008) nll: 4478.501953 \n",
      "(GPU: 0, epoch: 18, iters: 8928, time: 0.007) nll: 3528.535645 \n",
      "(GPU: 0, epoch: 18, iters: 9728, time: 0.008) nll: 4708.191406 \n",
      "(GPU: 0, epoch: 18, iters: 10528, time: 0.008) nll: 3806.554199 \n",
      "(GPU: 0, epoch: 18, iters: 11328, time: 0.008) nll: 3694.512695 \n",
      "(GPU: 0, epoch: 18, iters: 12128, time: 0.008) nll: 3161.085938 \n",
      "(GPU: 0, epoch: 18, iters: 12928, time: 0.008) nll: 4780.054688 \n",
      "(GPU: 0, epoch: 18, iters: 13728, time: 0.008) nll: 3728.715576 \n",
      "(GPU: 0, epoch: 18, iters: 14528, time: 0.008) nll: 4989.993164 \n",
      "(GPU: 0, epoch: 18, iters: 15328, time: 0.008) nll: 3372.623047 \n",
      "(GPU: 0, epoch: 18, iters: 16128, time: 0.008) nll: 3275.829102 \n",
      "(GPU: 0, epoch: 18, iters: 16928, time: 0.007) nll: 3453.282715 \n",
      "(GPU: 0, epoch: 18, iters: 17728, time: 0.008) nll: 3318.149658 \n",
      "(GPU: 0, epoch: 18, iters: 18528, time: 0.007) nll: 4085.377930 \n",
      "(GPU: 0, epoch: 18, iters: 19328, time: 0.008) nll: 3702.114258 \n",
      "(GPU: 0, epoch: 18, iters: 20128, time: 0.008) nll: 4044.212402 \n",
      "(GPU: 0, epoch: 18, iters: 20928, time: 0.008) nll: 1572.137207 \n",
      "(GPU: 0, epoch: 18, iters: 21728, time: 0.008) nll: 4610.012695 \n",
      "(GPU: 0, epoch: 18, iters: 22528, time: 0.008) nll: 4813.271973 \n",
      "(GPU: 0, epoch: 18, iters: 23328, time: 0.008) nll: 3434.481934 \n",
      "(GPU: 0, epoch: 18, iters: 24128, time: 0.008) nll: 3021.975830 \n",
      "(GPU: 0, epoch: 18, iters: 24928, time: 0.008) nll: 2690.849609 \n",
      "(GPU: 0, epoch: 18, iters: 25728, time: 0.008) nll: 2972.019775 \n",
      "(GPU: 0, epoch: 18, iters: 26528, time: 0.008) nll: 4582.366211 \n",
      "(GPU: 0, epoch: 18, iters: 27328, time: 0.008) nll: 2723.344971 \n",
      "saving the latest model (epoch 18, total_steps 2560000)\n",
      "(GPU: 0, epoch: 18, iters: 28128, time: 0.008) nll: 3702.771973 \n",
      "(GPU: 0, epoch: 18, iters: 28928, time: 0.008) nll: 4840.802734 \n",
      "(GPU: 0, epoch: 18, iters: 29728, time: 0.007) nll: 3128.770996 \n",
      "(GPU: 0, epoch: 18, iters: 30528, time: 0.008) nll: 3965.460938 \n",
      "(GPU: 0, epoch: 18, iters: 31328, time: 0.007) nll: 3946.308105 \n",
      "(GPU: 0, epoch: 18, iters: 32128, time: 0.008) nll: 3666.843262 \n",
      "(GPU: 0, epoch: 18, iters: 32928, time: 0.008) nll: 4200.320312 \n",
      "(GPU: 0, epoch: 18, iters: 33728, time: 0.008) nll: 4009.116699 \n",
      "(GPU: 0, epoch: 18, iters: 34528, time: 0.008) nll: 4406.918945 \n",
      "(GPU: 0, epoch: 18, iters: 35328, time: 0.008) nll: 3543.616943 \n",
      "(GPU: 0, epoch: 18, iters: 36128, time: 0.008) nll: 5306.516602 \n",
      "(GPU: 0, epoch: 18, iters: 36928, time: 0.008) nll: 4190.676758 \n",
      "(GPU: 0, epoch: 18, iters: 37728, time: 0.008) nll: 3327.392822 \n",
      "(GPU: 0, epoch: 18, iters: 38528, time: 0.008) nll: 4570.743164 \n",
      "(GPU: 0, epoch: 18, iters: 39328, time: 0.008) nll: 2928.734375 \n",
      "(GPU: 0, epoch: 18, iters: 40128, time: 0.008) nll: 5253.974121 \n",
      "(GPU: 0, epoch: 18, iters: 40928, time: 0.008) nll: 3041.103760 \n",
      "(GPU: 0, epoch: 18, iters: 41728, time: 0.008) nll: 4247.293457 \n",
      "(GPU: 0, epoch: 18, iters: 42528, time: 0.008) nll: 4590.948242 \n",
      "(GPU: 0, epoch: 18, iters: 43328, time: 0.008) nll: 4229.579590 \n",
      "(GPU: 0, epoch: 18, iters: 44128, time: 0.008) nll: 3932.661621 \n",
      "(GPU: 0, epoch: 18, iters: 44928, time: 0.008) nll: 4220.253418 \n",
      "(GPU: 0, epoch: 18, iters: 45728, time: 0.008) nll: 4269.712891 \n",
      "(GPU: 0, epoch: 18, iters: 46528, time: 0.008) nll: 4537.999023 \n",
      "(GPU: 0, epoch: 18, iters: 47328, time: 0.008) nll: 4507.194824 \n",
      "saving the latest model (epoch 18, total_steps 2580000)\n",
      "(GPU: 0, epoch: 18, iters: 48128, time: 0.008) nll: 4906.121094 \n",
      "(GPU: 0, epoch: 18, iters: 48928, time: 0.008) nll: 3973.821289 \n",
      "(GPU: 0, epoch: 18, iters: 49728, time: 0.008) nll: 4399.772461 \n",
      "(GPU: 0, epoch: 18, iters: 50528, time: 0.008) nll: 3528.743652 \n",
      "(GPU: 0, epoch: 18, iters: 51328, time: 0.008) nll: 3545.453613 \n",
      "(GPU: 0, epoch: 18, iters: 52128, time: 0.007) nll: 2325.549561 \n",
      "(GPU: 0, epoch: 18, iters: 52928, time: 0.008) nll: 3743.490234 \n",
      "(GPU: 0, epoch: 18, iters: 53728, time: 0.007) nll: 3276.828857 \n",
      "(GPU: 0, epoch: 18, iters: 54528, time: 0.008) nll: 3570.626465 \n",
      "(GPU: 0, epoch: 18, iters: 55328, time: 0.007) nll: 5038.647461 \n",
      "(GPU: 0, epoch: 18, iters: 56128, time: 0.008) nll: 2958.229248 \n",
      "(GPU: 0, epoch: 18, iters: 56928, time: 0.008) nll: 3664.247070 \n",
      "(GPU: 0, epoch: 18, iters: 57728, time: 0.008) nll: 3513.426758 \n",
      "(GPU: 0, epoch: 18, iters: 58528, time: 0.007) nll: 3891.523926 \n",
      "(GPU: 0, epoch: 18, iters: 59328, time: 0.008) nll: 3843.727051 \n",
      "(GPU: 0, epoch: 18, iters: 59328, time: 0.013) nll: 3811.415527 \n",
      "(GPU: 0, epoch: 18, iters: 59328, time: 0.013) nll: 4092.248047 \n",
      "(GPU: 0, epoch: 18, iters: 60128, time: 0.008) nll: 4092.914062 \n",
      "(GPU: 0, epoch: 18, iters: 60928, time: 0.008) nll: 3931.791504 \n",
      "(GPU: 0, epoch: 18, iters: 61728, time: 0.008) nll: 4587.173340 \n",
      "(GPU: 0, epoch: 18, iters: 62528, time: 0.008) nll: 3932.355957 \n",
      "(GPU: 0, epoch: 18, iters: 63328, time: 0.008) nll: 2991.031250 \n",
      "(GPU: 0, epoch: 18, iters: 64128, time: 0.008) nll: 2885.936523 \n",
      "(GPU: 0, epoch: 18, iters: 64928, time: 0.007) nll: 4323.562500 \n",
      "(GPU: 0, epoch: 18, iters: 65728, time: 0.008) nll: 3705.108643 \n",
      "(GPU: 0, epoch: 18, iters: 66528, time: 0.008) nll: 4776.903809 \n",
      "(GPU: 0, epoch: 18, iters: 67328, time: 0.008) nll: 3468.677734 \n",
      "saving the latest model (epoch 18, total_steps 2600000)\n",
      "(GPU: 0, epoch: 18, iters: 68128, time: 0.008) nll: 3206.748291 \n",
      "(GPU: 0, epoch: 18, iters: 68928, time: 0.008) nll: 3807.995605 \n",
      "(GPU: 0, epoch: 18, iters: 69728, time: 0.008) nll: 2395.708740 \n",
      "(GPU: 0, epoch: 18, iters: 70528, time: 0.008) nll: 2521.432861 \n",
      "(GPU: 0, epoch: 18, iters: 71328, time: 0.008) nll: 3421.989746 \n",
      "(GPU: 0, epoch: 18, iters: 72128, time: 0.008) nll: 2621.860596 \n",
      "(GPU: 0, epoch: 18, iters: 72928, time: 0.007) nll: 2986.286377 \n",
      "(GPU: 0, epoch: 18, iters: 73728, time: 0.008) nll: 3816.789795 \n",
      "(GPU: 0, epoch: 18, iters: 74528, time: 0.007) nll: 3993.755371 \n",
      "(GPU: 0, epoch: 18, iters: 75328, time: 0.008) nll: 3422.186279 \n",
      "(GPU: 0, epoch: 18, iters: 76128, time: 0.008) nll: 2904.722656 \n",
      "(GPU: 0, epoch: 18, iters: 76928, time: 0.008) nll: 2645.876465 \n",
      "(GPU: 0, epoch: 18, iters: 77728, time: 0.008) nll: 3174.807861 \n",
      "(GPU: 0, epoch: 18, iters: 78528, time: 0.008) nll: 3285.957764 \n",
      "(GPU: 0, epoch: 18, iters: 79328, time: 0.008) nll: 5377.064453 \n",
      "(GPU: 0, epoch: 18, iters: 80128, time: 0.008) nll: 3676.049072 \n",
      "(GPU: 0, epoch: 18, iters: 80928, time: 0.008) nll: 5456.886719 \n",
      "(GPU: 0, epoch: 18, iters: 81728, time: 0.008) nll: 4268.589844 \n",
      "(GPU: 0, epoch: 18, iters: 82528, time: 0.008) nll: 3529.627686 \n",
      "(GPU: 0, epoch: 18, iters: 83328, time: 0.008) nll: 4543.200684 \n",
      "(GPU: 0, epoch: 18, iters: 84128, time: 0.008) nll: 3015.666992 \n",
      "(GPU: 0, epoch: 18, iters: 84928, time: 0.008) nll: 4041.956055 \n",
      "(GPU: 0, epoch: 18, iters: 85728, time: 0.008) nll: 4209.049805 \n",
      "(GPU: 0, epoch: 18, iters: 86528, time: 0.008) nll: 3694.736816 \n",
      "(GPU: 0, epoch: 18, iters: 87328, time: 0.008) nll: 4406.274414 \n",
      "saving the latest model (epoch 18, total_steps 2620000)\n",
      "(GPU: 0, epoch: 18, iters: 88128, time: 0.008) nll: 3164.244385 \n",
      "(GPU: 0, epoch: 18, iters: 88928, time: 0.008) nll: 2477.240723 \n",
      "(GPU: 0, epoch: 18, iters: 89728, time: 0.008) nll: 4158.775391 \n",
      "(GPU: 0, epoch: 18, iters: 90528, time: 0.008) nll: 3611.179688 \n",
      "(GPU: 0, epoch: 18, iters: 91328, time: 0.008) nll: 4048.522705 \n",
      "(GPU: 0, epoch: 18, iters: 92128, time: 0.008) nll: 4221.634766 \n",
      "(GPU: 0, epoch: 18, iters: 92928, time: 0.008) nll: 3417.677002 \n",
      "(GPU: 0, epoch: 18, iters: 93728, time: 0.008) nll: 3733.461670 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4397/4397 [20:32<00:00,  3.57it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 18, iters: 94528, time: 0.008) nll: 3753.953613 \n",
      "(GPU: 0, epoch: 18, iters: 95328, time: 0.008) nll: 4662.618164 \n",
      "(GPU: 0, epoch: 18, iters: 96128, time: 0.008) nll: 3721.559814 \n",
      "(GPU: 0, epoch: 18, iters: 96928, time: 0.008) nll: 3619.463867 \n",
      "(GPU: 0, epoch: 18, iters: 97728, time: 0.008) nll: 3877.391113 \n",
      "(GPU: 0, epoch: 18, iters: 98528, time: 0.008) nll: 3434.161133 \n",
      "(GPU: 0, epoch: 18, iters: 99328, time: 0.008) nll: 4501.699707 \n",
      "(GPU: 0, epoch: 18, iters: 100128, time: 0.007) nll: 2853.442139 \n",
      "(GPU: 0, epoch: 18, iters: 100928, time: 0.008) nll: 5517.946289 \n",
      "(GPU: 0, epoch: 18, iters: 101728, time: 0.008) nll: 3141.869629 \n",
      "(GPU: 0, epoch: 18, iters: 102528, time: 0.008) nll: 2772.983643 \n",
      "(GPU: 0, epoch: 18, iters: 103328, time: 0.008) nll: 3428.783936 \n",
      "(GPU: 0, epoch: 18, iters: 104128, time: 0.008) nll: 3880.065430 \n",
      "(GPU: 0, epoch: 18, iters: 104928, time: 0.008) nll: 4839.777344 \n",
      "(GPU: 0, epoch: 18, iters: 105728, time: 0.008) nll: 4396.038086 \n",
      "(GPU: 0, epoch: 18, iters: 106528, time: 0.008) nll: 3583.802002 \n",
      "(GPU: 0, epoch: 18, iters: 107328, time: 0.008) nll: 3293.647949 \n",
      "saving the latest model (epoch 18, total_steps 2640000)\n",
      "(GPU: 0, epoch: 18, iters: 108128, time: 0.008) nll: 4281.735352 \n",
      "(GPU: 0, epoch: 18, iters: 108928, time: 0.008) nll: 3341.259766 \n",
      "(GPU: 0, epoch: 18, iters: 109728, time: 0.008) nll: 3197.277832 \n",
      "(GPU: 0, epoch: 18, iters: 110528, time: 0.008) nll: 3793.002930 \n",
      "(GPU: 0, epoch: 18, iters: 111328, time: 0.007) nll: 4658.054688 \n",
      "(GPU: 0, epoch: 18, iters: 112128, time: 0.008) nll: 2992.729736 \n",
      "(GPU: 0, epoch: 18, iters: 112928, time: 0.008) nll: 3618.256836 \n",
      "(GPU: 0, epoch: 18, iters: 113728, time: 0.008) nll: 3447.760986 \n",
      "(GPU: 0, epoch: 18, iters: 114528, time: 0.008) nll: 4905.607422 \n",
      "(GPU: 0, epoch: 18, iters: 115328, time: 0.008) nll: 4872.874023 \n",
      "(GPU: 0, epoch: 18, iters: 116128, time: 0.008) nll: 3073.042236 \n",
      "(GPU: 0, epoch: 18, iters: 116928, time: 0.008) nll: 3174.716064 \n",
      "(GPU: 0, epoch: 18, iters: 117728, time: 0.007) nll: 2966.133545 \n",
      "(GPU: 0, epoch: 18, iters: 118528, time: 0.008) nll: 3159.422363 \n",
      "(GPU: 0, epoch: 18, iters: 119328, time: 0.008) nll: 3788.584473 \n",
      "(GPU: 0, epoch: 18, iters: 120128, time: 0.008) nll: 3433.521729 \n",
      "(GPU: 0, epoch: 18, iters: 120928, time: 0.008) nll: 3179.906494 \n",
      "(GPU: 0, epoch: 18, iters: 121728, time: 0.008) nll: 3623.580078 \n",
      "(GPU: 0, epoch: 18, iters: 122528, time: 0.008) nll: 2935.051270 \n",
      "(GPU: 0, epoch: 18, iters: 123328, time: 0.008) nll: 3266.384766 \n",
      "(GPU: 0, epoch: 18, iters: 124128, time: 0.007) nll: 2943.994141 \n",
      "(GPU: 0, epoch: 18, iters: 124928, time: 0.008) nll: 3509.081543 \n",
      "(GPU: 0, epoch: 18, iters: 125728, time: 0.008) nll: 3981.862793 \n",
      "(GPU: 0, epoch: 18, iters: 126528, time: 0.008) nll: 5627.744141 \n",
      "(GPU: 0, epoch: 18, iters: 127328, time: 0.008) nll: 4066.960205 \n",
      "saving the latest model (epoch 18, total_steps 2660000)\n",
      "(GPU: 0, epoch: 18, iters: 128128, time: 0.008) nll: 4643.588867 \n",
      "(GPU: 0, epoch: 18, iters: 128928, time: 0.008) nll: 3576.145020 \n",
      "(GPU: 0, epoch: 18, iters: 129728, time: 0.008) nll: 3670.484863 \n",
      "(GPU: 0, epoch: 18, iters: 130528, time: 0.008) nll: 3880.062744 \n",
      "(GPU: 0, epoch: 18, iters: 131328, time: 0.008) nll: 4755.802734 \n",
      "(GPU: 0, epoch: 18, iters: 132128, time: 0.008) nll: 2387.892090 \n",
      "(GPU: 0, epoch: 18, iters: 132928, time: 0.008) nll: 3004.229980 \n",
      "(GPU: 0, epoch: 18, iters: 133728, time: 0.008) nll: 5031.584961 \n",
      "(GPU: 0, epoch: 18, iters: 134528, time: 0.008) nll: 2973.213379 \n",
      "(GPU: 0, epoch: 18, iters: 135328, time: 0.008) nll: 3914.111328 \n",
      "(GPU: 0, epoch: 18, iters: 136128, time: 0.008) nll: 3452.527344 \n",
      "(GPU: 0, epoch: 18, iters: 136928, time: 0.008) nll: 3552.734131 \n",
      "(GPU: 0, epoch: 18, iters: 137728, time: 0.008) nll: 5224.904297 \n",
      "(GPU: 0, epoch: 18, iters: 138528, time: 0.008) nll: 3389.044434 \n",
      "(GPU: 0, epoch: 18, iters: 139328, time: 0.008) nll: 3731.176270 \n",
      "(GPU: 0, epoch: 18, iters: 140128, time: 0.008) nll: 3995.381836 \n",
      "saving the model at the end of epoch 18, iters 2673376\n",
      "([test] GPU: 0, epoch: 18) \n",
      "OrderedDict()\n",
      "[*] End of epoch 18 / 25 \t Time Taken: 1260 sec \n",
      "/cluster/54/streakfull/ADL4CV/Project/src/plz-autosdf/logs/bert2vqsc-text2shape-seq-LR1e-4-new-bert-2-prev_z_shape_input-try-2\n",
      "[*] learning rate = 0.0000725\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2956/4397 [13:52<06:11,  3.88it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 19, iters: 32, time: 0.004) nll: 3540.635254 \n",
      "(GPU: 0, epoch: 19, iters: 32, time: 0.004) nll: 5266.368164 \n",
      "(GPU: 0, epoch: 19, iters: 224, time: 0.008) nll: 3513.085449 \n",
      "(GPU: 0, epoch: 19, iters: 1024, time: 0.008) nll: 3594.369141 \n",
      "(GPU: 0, epoch: 19, iters: 1824, time: 0.008) nll: 3718.614990 \n",
      "(GPU: 0, epoch: 19, iters: 2624, time: 0.008) nll: 3324.434814 \n",
      "(GPU: 0, epoch: 19, iters: 3424, time: 0.007) nll: 3603.288574 \n",
      "(GPU: 0, epoch: 19, iters: 4224, time: 0.008) nll: 3673.291504 \n",
      "(GPU: 0, epoch: 19, iters: 5024, time: 0.008) nll: 4087.064453 \n",
      "(GPU: 0, epoch: 19, iters: 5824, time: 0.008) nll: 2987.405273 \n",
      "(GPU: 0, epoch: 19, iters: 6624, time: 0.008) nll: 2054.639648 \n",
      "saving the latest model (epoch 19, total_steps 2680000)\n",
      "(GPU: 0, epoch: 19, iters: 7424, time: 0.008) nll: 3876.492676 \n",
      "(GPU: 0, epoch: 19, iters: 8224, time: 0.008) nll: 3798.358887 \n",
      "(GPU: 0, epoch: 19, iters: 9024, time: 0.008) nll: 3309.305664 \n",
      "(GPU: 0, epoch: 19, iters: 9824, time: 0.007) nll: 5130.105957 \n",
      "(GPU: 0, epoch: 19, iters: 10624, time: 0.008) nll: 4141.515137 \n",
      "(GPU: 0, epoch: 19, iters: 11424, time: 0.008) nll: 4004.697998 \n",
      "(GPU: 0, epoch: 19, iters: 12224, time: 0.008) nll: 3997.457031 \n",
      "(GPU: 0, epoch: 19, iters: 13024, time: 0.008) nll: 3538.221191 \n",
      "(GPU: 0, epoch: 19, iters: 13824, time: 0.008) nll: 3619.284180 \n",
      "(GPU: 0, epoch: 19, iters: 14624, time: 0.008) nll: 4492.820801 \n",
      "(GPU: 0, epoch: 19, iters: 14624, time: 0.013) nll: 4432.323730 \n",
      "(GPU: 0, epoch: 19, iters: 14624, time: 0.013) nll: 4092.503906 \n",
      "(GPU: 0, epoch: 19, iters: 15424, time: 0.008) nll: 5100.062012 \n",
      "(GPU: 0, epoch: 19, iters: 16224, time: 0.008) nll: 3878.106934 \n",
      "(GPU: 0, epoch: 19, iters: 17024, time: 0.008) nll: 3698.590576 \n",
      "(GPU: 0, epoch: 19, iters: 17824, time: 0.008) nll: 2242.985352 \n",
      "(GPU: 0, epoch: 19, iters: 18624, time: 0.008) nll: 4397.977539 \n",
      "(GPU: 0, epoch: 19, iters: 19424, time: 0.008) nll: 3926.544434 \n",
      "(GPU: 0, epoch: 19, iters: 20224, time: 0.008) nll: 4781.399414 \n",
      "(GPU: 0, epoch: 19, iters: 21024, time: 0.007) nll: 4483.203125 \n",
      "(GPU: 0, epoch: 19, iters: 21824, time: 0.008) nll: 4700.415039 \n",
      "(GPU: 0, epoch: 19, iters: 22624, time: 0.008) nll: 2878.197266 \n",
      "(GPU: 0, epoch: 19, iters: 23424, time: 0.008) nll: 4154.552246 \n",
      "(GPU: 0, epoch: 19, iters: 24224, time: 0.008) nll: 3946.701172 \n",
      "(GPU: 0, epoch: 19, iters: 25024, time: 0.008) nll: 3179.204590 \n",
      "(GPU: 0, epoch: 19, iters: 25824, time: 0.008) nll: 3511.207031 \n",
      "(GPU: 0, epoch: 19, iters: 26624, time: 0.008) nll: 4216.834961 \n",
      "saving the latest model (epoch 19, total_steps 2700000)\n",
      "(GPU: 0, epoch: 19, iters: 27424, time: 0.008) nll: 3898.886963 \n",
      "(GPU: 0, epoch: 19, iters: 28224, time: 0.008) nll: 4695.010742 \n",
      "(GPU: 0, epoch: 19, iters: 29024, time: 0.008) nll: 4687.237793 \n",
      "(GPU: 0, epoch: 19, iters: 29824, time: 0.008) nll: 2918.458496 \n",
      "(GPU: 0, epoch: 19, iters: 30624, time: 0.008) nll: 4061.149414 \n",
      "(GPU: 0, epoch: 19, iters: 31424, time: 0.008) nll: 3764.363281 \n",
      "(GPU: 0, epoch: 19, iters: 32224, time: 0.008) nll: 3150.444336 \n",
      "(GPU: 0, epoch: 19, iters: 33024, time: 0.008) nll: 4829.541016 \n",
      "(GPU: 0, epoch: 19, iters: 33824, time: 0.008) nll: 4865.199707 \n",
      "(GPU: 0, epoch: 19, iters: 34624, time: 0.008) nll: 4619.320312 \n",
      "(GPU: 0, epoch: 19, iters: 35424, time: 0.007) nll: 4522.744629 \n",
      "(GPU: 0, epoch: 19, iters: 36224, time: 0.008) nll: 4332.388184 \n",
      "(GPU: 0, epoch: 19, iters: 37024, time: 0.008) nll: 3161.773193 \n",
      "(GPU: 0, epoch: 19, iters: 37824, time: 0.008) nll: 5077.860840 \n",
      "(GPU: 0, epoch: 19, iters: 38624, time: 0.008) nll: 5168.245117 \n",
      "(GPU: 0, epoch: 19, iters: 39424, time: 0.008) nll: 4018.380371 \n",
      "(GPU: 0, epoch: 19, iters: 40224, time: 0.008) nll: 3070.445312 \n",
      "(GPU: 0, epoch: 19, iters: 41024, time: 0.008) nll: 3785.022949 \n",
      "(GPU: 0, epoch: 19, iters: 41824, time: 0.008) nll: 4706.975586 \n",
      "(GPU: 0, epoch: 19, iters: 42624, time: 0.008) nll: 3001.203857 \n",
      "(GPU: 0, epoch: 19, iters: 43424, time: 0.008) nll: 4215.602539 \n",
      "(GPU: 0, epoch: 19, iters: 44224, time: 0.008) nll: 2654.940918 \n",
      "(GPU: 0, epoch: 19, iters: 45024, time: 0.008) nll: 3602.153809 \n",
      "(GPU: 0, epoch: 19, iters: 45824, time: 0.008) nll: 5811.271973 \n",
      "(GPU: 0, epoch: 19, iters: 46624, time: 0.008) nll: 4707.429688 \n",
      "saving the latest model (epoch 19, total_steps 2720000)\n",
      "(GPU: 0, epoch: 19, iters: 47424, time: 0.008) nll: 3594.188721 \n",
      "(GPU: 0, epoch: 19, iters: 48224, time: 0.008) nll: 3899.573975 \n",
      "(GPU: 0, epoch: 19, iters: 49024, time: 0.008) nll: 3466.700684 \n",
      "(GPU: 0, epoch: 19, iters: 49824, time: 0.008) nll: 4476.034180 \n",
      "(GPU: 0, epoch: 19, iters: 50624, time: 0.008) nll: 2952.674561 \n",
      "(GPU: 0, epoch: 19, iters: 51424, time: 0.008) nll: 4319.956055 \n",
      "(GPU: 0, epoch: 19, iters: 52224, time: 0.008) nll: 5012.752930 \n",
      "(GPU: 0, epoch: 19, iters: 53024, time: 0.007) nll: 4486.929199 \n",
      "(GPU: 0, epoch: 19, iters: 53824, time: 0.008) nll: 4826.072754 \n",
      "(GPU: 0, epoch: 19, iters: 54624, time: 0.008) nll: 4719.527832 \n",
      "(GPU: 0, epoch: 19, iters: 55424, time: 0.008) nll: 3608.188965 \n",
      "(GPU: 0, epoch: 19, iters: 56224, time: 0.008) nll: 4937.723633 \n",
      "(GPU: 0, epoch: 19, iters: 57024, time: 0.008) nll: 3918.682861 \n",
      "(GPU: 0, epoch: 19, iters: 57824, time: 0.007) nll: 5094.958496 \n",
      "(GPU: 0, epoch: 19, iters: 58624, time: 0.008) nll: 3192.221680 \n",
      "(GPU: 0, epoch: 19, iters: 59424, time: 0.008) nll: 3098.770020 \n",
      "(GPU: 0, epoch: 19, iters: 60224, time: 0.008) nll: 3338.640625 \n",
      "(GPU: 0, epoch: 19, iters: 61024, time: 0.008) nll: 3894.093018 \n",
      "(GPU: 0, epoch: 19, iters: 61824, time: 0.008) nll: 2516.791992 \n",
      "(GPU: 0, epoch: 19, iters: 62624, time: 0.008) nll: 4153.443848 \n",
      "(GPU: 0, epoch: 19, iters: 63424, time: 0.008) nll: 2809.319336 \n",
      "(GPU: 0, epoch: 19, iters: 64224, time: 0.008) nll: 2884.216797 \n",
      "(GPU: 0, epoch: 19, iters: 65024, time: 0.008) nll: 3050.761230 \n",
      "(GPU: 0, epoch: 19, iters: 65824, time: 0.008) nll: 4222.990723 \n",
      "(GPU: 0, epoch: 19, iters: 66624, time: 0.008) nll: 4194.196289 \n",
      "saving the latest model (epoch 19, total_steps 2740000)\n",
      "(GPU: 0, epoch: 19, iters: 67424, time: 0.008) nll: 4494.914062 \n",
      "(GPU: 0, epoch: 19, iters: 68224, time: 0.008) nll: 3804.709473 \n",
      "(GPU: 0, epoch: 19, iters: 69024, time: 0.008) nll: 3323.932861 \n",
      "(GPU: 0, epoch: 19, iters: 69824, time: 0.008) nll: 3295.629883 \n",
      "(GPU: 0, epoch: 19, iters: 70624, time: 0.008) nll: 3855.376221 \n",
      "(GPU: 0, epoch: 19, iters: 71424, time: 0.008) nll: 3922.859375 \n",
      "(GPU: 0, epoch: 19, iters: 72224, time: 0.008) nll: 3123.004395 \n",
      "(GPU: 0, epoch: 19, iters: 73024, time: 0.008) nll: 5166.532227 \n",
      "(GPU: 0, epoch: 19, iters: 73824, time: 0.008) nll: 3385.506348 \n",
      "(GPU: 0, epoch: 19, iters: 74624, time: 0.008) nll: 4045.356445 \n",
      "(GPU: 0, epoch: 19, iters: 75424, time: 0.008) nll: 3239.167969 \n",
      "(GPU: 0, epoch: 19, iters: 76224, time: 0.008) nll: 4193.134277 \n",
      "(GPU: 0, epoch: 19, iters: 77024, time: 0.008) nll: 2506.517334 \n",
      "(GPU: 0, epoch: 19, iters: 77824, time: 0.008) nll: 3348.936768 \n",
      "(GPU: 0, epoch: 19, iters: 78624, time: 0.008) nll: 4514.374023 \n",
      "(GPU: 0, epoch: 19, iters: 79424, time: 0.008) nll: 3221.412109 \n",
      "(GPU: 0, epoch: 19, iters: 80224, time: 0.008) nll: 3642.797852 \n",
      "(GPU: 0, epoch: 19, iters: 81024, time: 0.008) nll: 3575.099609 \n",
      "(GPU: 0, epoch: 19, iters: 81824, time: 0.007) nll: 3429.292480 \n",
      "(GPU: 0, epoch: 19, iters: 82624, time: 0.008) nll: 6457.444336 \n",
      "(GPU: 0, epoch: 19, iters: 83424, time: 0.008) nll: 3862.719238 \n",
      "(GPU: 0, epoch: 19, iters: 84224, time: 0.008) nll: 4459.946777 \n",
      "(GPU: 0, epoch: 19, iters: 85024, time: 0.008) nll: 4528.218750 \n",
      "(GPU: 0, epoch: 19, iters: 85824, time: 0.008) nll: 4072.400635 \n",
      "(GPU: 0, epoch: 19, iters: 86624, time: 0.008) nll: 3587.372559 \n",
      "saving the latest model (epoch 19, total_steps 2760000)\n",
      "(GPU: 0, epoch: 19, iters: 87424, time: 0.008) nll: 3945.125000 \n",
      "(GPU: 0, epoch: 19, iters: 88224, time: 0.008) nll: 3966.174072 \n",
      "(GPU: 0, epoch: 19, iters: 89024, time: 0.008) nll: 4158.262207 \n",
      "(GPU: 0, epoch: 19, iters: 89824, time: 0.008) nll: 3390.875488 \n",
      "(GPU: 0, epoch: 19, iters: 90624, time: 0.008) nll: 2917.320312 \n",
      "(GPU: 0, epoch: 19, iters: 91424, time: 0.008) nll: 2824.162354 \n",
      "(GPU: 0, epoch: 19, iters: 92224, time: 0.008) nll: 3607.227539 \n",
      "(GPU: 0, epoch: 19, iters: 93024, time: 0.008) nll: 4613.300781 \n",
      "(GPU: 0, epoch: 19, iters: 93824, time: 0.008) nll: 3913.453369 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4397/4397 [20:32<00:00,  3.57it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 19, iters: 94624, time: 0.008) nll: 3541.341309 \n",
      "(GPU: 0, epoch: 19, iters: 95424, time: 0.008) nll: 5069.116211 \n",
      "(GPU: 0, epoch: 19, iters: 96224, time: 0.007) nll: 4206.159180 \n",
      "(GPU: 0, epoch: 19, iters: 97024, time: 0.008) nll: 4107.853516 \n",
      "(GPU: 0, epoch: 19, iters: 97824, time: 0.008) nll: 3874.736328 \n",
      "(GPU: 0, epoch: 19, iters: 98624, time: 0.008) nll: 4624.205078 \n",
      "(GPU: 0, epoch: 19, iters: 99424, time: 0.007) nll: 3416.586426 \n",
      "(GPU: 0, epoch: 19, iters: 100224, time: 0.008) nll: 3459.205566 \n",
      "(GPU: 0, epoch: 19, iters: 101024, time: 0.008) nll: 4122.833496 \n",
      "(GPU: 0, epoch: 19, iters: 101824, time: 0.008) nll: 3350.068848 \n",
      "(GPU: 0, epoch: 19, iters: 102624, time: 0.008) nll: 3931.879395 \n",
      "(GPU: 0, epoch: 19, iters: 103424, time: 0.008) nll: 4886.372070 \n",
      "(GPU: 0, epoch: 19, iters: 104224, time: 0.007) nll: 3647.283203 \n",
      "(GPU: 0, epoch: 19, iters: 105024, time: 0.008) nll: 3659.333740 \n",
      "(GPU: 0, epoch: 19, iters: 105824, time: 0.008) nll: 3954.701904 \n",
      "(GPU: 0, epoch: 19, iters: 106624, time: 0.008) nll: 3152.992188 \n",
      "saving the latest model (epoch 19, total_steps 2780000)\n",
      "(GPU: 0, epoch: 19, iters: 107424, time: 0.007) nll: 4363.604492 \n",
      "(GPU: 0, epoch: 19, iters: 108224, time: 0.008) nll: 2893.264893 \n",
      "(GPU: 0, epoch: 19, iters: 109024, time: 0.008) nll: 4250.884277 \n",
      "(GPU: 0, epoch: 19, iters: 109824, time: 0.008) nll: 2978.668457 \n",
      "(GPU: 0, epoch: 19, iters: 110624, time: 0.008) nll: 3472.509033 \n",
      "(GPU: 0, epoch: 19, iters: 110624, time: 0.012) nll: 3444.246582 \n",
      "(GPU: 0, epoch: 19, iters: 110624, time: 0.012) nll: 5092.355469 \n",
      "(GPU: 0, epoch: 19, iters: 111424, time: 0.008) nll: 4227.570801 \n",
      "(GPU: 0, epoch: 19, iters: 112224, time: 0.008) nll: 4017.826660 \n",
      "(GPU: 0, epoch: 19, iters: 113024, time: 0.008) nll: 4397.780273 \n",
      "(GPU: 0, epoch: 19, iters: 113824, time: 0.008) nll: 4282.621094 \n",
      "(GPU: 0, epoch: 19, iters: 114624, time: 0.008) nll: 3499.634277 \n",
      "(GPU: 0, epoch: 19, iters: 115424, time: 0.008) nll: 4449.546875 \n",
      "(GPU: 0, epoch: 19, iters: 116224, time: 0.008) nll: 4074.621582 \n",
      "(GPU: 0, epoch: 19, iters: 117024, time: 0.008) nll: 4227.846191 \n",
      "(GPU: 0, epoch: 19, iters: 117824, time: 0.008) nll: 4822.185547 \n",
      "(GPU: 0, epoch: 19, iters: 118624, time: 0.007) nll: 2977.400391 \n",
      "(GPU: 0, epoch: 19, iters: 119424, time: 0.008) nll: 4145.453125 \n",
      "(GPU: 0, epoch: 19, iters: 120224, time: 0.008) nll: 2961.535645 \n",
      "(GPU: 0, epoch: 19, iters: 121024, time: 0.008) nll: 4801.193359 \n",
      "(GPU: 0, epoch: 19, iters: 121824, time: 0.007) nll: 2696.423096 \n",
      "(GPU: 0, epoch: 19, iters: 122624, time: 0.008) nll: 3666.389648 \n",
      "(GPU: 0, epoch: 19, iters: 123424, time: 0.008) nll: 4244.719238 \n",
      "(GPU: 0, epoch: 19, iters: 124224, time: 0.008) nll: 4712.553711 \n",
      "(GPU: 0, epoch: 19, iters: 125024, time: 0.008) nll: 5632.487305 \n",
      "(GPU: 0, epoch: 19, iters: 125824, time: 0.008) nll: 3252.177246 \n",
      "(GPU: 0, epoch: 19, iters: 126624, time: 0.008) nll: 5088.474609 \n",
      "saving the latest model (epoch 19, total_steps 2800000)\n",
      "(GPU: 0, epoch: 19, iters: 127424, time: 0.008) nll: 2462.128174 \n",
      "(GPU: 0, epoch: 19, iters: 128224, time: 0.008) nll: 3129.637451 \n",
      "(GPU: 0, epoch: 19, iters: 129024, time: 0.008) nll: 5132.676758 \n",
      "(GPU: 0, epoch: 19, iters: 129824, time: 0.008) nll: 4063.321533 \n",
      "(GPU: 0, epoch: 19, iters: 130624, time: 0.008) nll: 3137.875977 \n",
      "(GPU: 0, epoch: 19, iters: 131424, time: 0.008) nll: 3895.262695 \n",
      "(GPU: 0, epoch: 19, iters: 132224, time: 0.008) nll: 4508.408691 \n",
      "(GPU: 0, epoch: 19, iters: 133024, time: 0.008) nll: 3803.240234 \n",
      "(GPU: 0, epoch: 19, iters: 133824, time: 0.008) nll: 3337.706055 \n",
      "(GPU: 0, epoch: 19, iters: 134624, time: 0.008) nll: 2688.476074 \n",
      "(GPU: 0, epoch: 19, iters: 135424, time: 0.008) nll: 3282.613770 \n",
      "(GPU: 0, epoch: 19, iters: 136224, time: 0.008) nll: 3679.429199 \n",
      "(GPU: 0, epoch: 19, iters: 137024, time: 0.008) nll: 2728.708008 \n",
      "(GPU: 0, epoch: 19, iters: 137824, time: 0.007) nll: 3986.833740 \n",
      "(GPU: 0, epoch: 19, iters: 138624, time: 0.008) nll: 3286.939453 \n",
      "(GPU: 0, epoch: 19, iters: 139424, time: 0.008) nll: 3517.860840 \n",
      "(GPU: 0, epoch: 19, iters: 140224, time: 0.008) nll: 5211.769531 \n",
      "[*] End of epoch 19 / 25 \t Time Taken: 1233 sec \n",
      "/cluster/54/streakfull/ADL4CV/Project/src/plz-autosdf/logs/bert2vqsc-text2shape-seq-LR1e-4-new-bert-2-prev_z_shape_input-try-2\n",
      "[*] learning rate = 0.0000707\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2959/4397 [13:52<06:11,  3.87it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 20, iters: 32, time: 0.004) nll: 3917.958740 \n",
      "(GPU: 0, epoch: 20, iters: 32, time: 0.004) nll: 4237.412109 \n",
      "(GPU: 0, epoch: 20, iters: 320, time: 0.008) nll: 4502.354980 \n",
      "(GPU: 0, epoch: 20, iters: 1120, time: 0.008) nll: 4309.996094 \n",
      "(GPU: 0, epoch: 20, iters: 1920, time: 0.008) nll: 3809.354492 \n",
      "(GPU: 0, epoch: 20, iters: 2720, time: 0.007) nll: 3390.358154 \n",
      "(GPU: 0, epoch: 20, iters: 3520, time: 0.008) nll: 3928.582031 \n",
      "(GPU: 0, epoch: 20, iters: 4320, time: 0.008) nll: 3895.083008 \n",
      "(GPU: 0, epoch: 20, iters: 5120, time: 0.008) nll: 4367.831543 \n",
      "(GPU: 0, epoch: 20, iters: 5920, time: 0.008) nll: 3110.713623 \n",
      "saving the latest model (epoch 20, total_steps 2820000)\n",
      "(GPU: 0, epoch: 20, iters: 6720, time: 0.008) nll: 3674.098633 \n",
      "(GPU: 0, epoch: 20, iters: 7520, time: 0.008) nll: 3895.118408 \n",
      "(GPU: 0, epoch: 20, iters: 8320, time: 0.008) nll: 4043.895996 \n",
      "(GPU: 0, epoch: 20, iters: 9120, time: 0.007) nll: 3481.455322 \n",
      "(GPU: 0, epoch: 20, iters: 9920, time: 0.008) nll: 2333.475830 \n",
      "(GPU: 0, epoch: 20, iters: 10720, time: 0.008) nll: 3737.198242 \n",
      "(GPU: 0, epoch: 20, iters: 11520, time: 0.008) nll: 2849.337891 \n",
      "(GPU: 0, epoch: 20, iters: 12320, time: 0.008) nll: 4576.083008 \n",
      "(GPU: 0, epoch: 20, iters: 13120, time: 0.008) nll: 3721.381836 \n",
      "(GPU: 0, epoch: 20, iters: 13920, time: 0.008) nll: 4475.458496 \n",
      "(GPU: 0, epoch: 20, iters: 14720, time: 0.008) nll: 4912.080566 \n",
      "(GPU: 0, epoch: 20, iters: 15520, time: 0.007) nll: 2452.087402 \n",
      "(GPU: 0, epoch: 20, iters: 16320, time: 0.008) nll: 2846.440186 \n",
      "(GPU: 0, epoch: 20, iters: 17120, time: 0.008) nll: 3474.143066 \n",
      "(GPU: 0, epoch: 20, iters: 17920, time: 0.008) nll: 3865.706787 \n",
      "(GPU: 0, epoch: 20, iters: 18720, time: 0.008) nll: 4417.836914 \n",
      "(GPU: 0, epoch: 20, iters: 19520, time: 0.008) nll: 2886.469238 \n",
      "(GPU: 0, epoch: 20, iters: 20320, time: 0.008) nll: 3834.216309 \n",
      "(GPU: 0, epoch: 20, iters: 21120, time: 0.008) nll: 3977.373047 \n",
      "(GPU: 0, epoch: 20, iters: 21920, time: 0.008) nll: 4545.975098 \n",
      "(GPU: 0, epoch: 20, iters: 22720, time: 0.008) nll: 3775.039062 \n",
      "(GPU: 0, epoch: 20, iters: 23520, time: 0.008) nll: 5020.268066 \n",
      "(GPU: 0, epoch: 20, iters: 24320, time: 0.008) nll: 4051.467773 \n",
      "(GPU: 0, epoch: 20, iters: 25120, time: 0.008) nll: 3646.522461 \n",
      "(GPU: 0, epoch: 20, iters: 25920, time: 0.008) nll: 3362.815918 \n",
      "saving the latest model (epoch 20, total_steps 2840000)\n",
      "(GPU: 0, epoch: 20, iters: 26720, time: 0.008) nll: 3454.022217 \n",
      "(GPU: 0, epoch: 20, iters: 27520, time: 0.008) nll: 4079.281738 \n",
      "(GPU: 0, epoch: 20, iters: 28320, time: 0.008) nll: 3620.770020 \n",
      "(GPU: 0, epoch: 20, iters: 29120, time: 0.008) nll: 4727.312500 \n",
      "(GPU: 0, epoch: 20, iters: 29920, time: 0.008) nll: 4411.185547 \n",
      "(GPU: 0, epoch: 20, iters: 30720, time: 0.008) nll: 3356.298584 \n",
      "(GPU: 0, epoch: 20, iters: 31520, time: 0.008) nll: 5080.242676 \n",
      "(GPU: 0, epoch: 20, iters: 32320, time: 0.008) nll: 3474.937012 \n",
      "(GPU: 0, epoch: 20, iters: 33120, time: 0.008) nll: 3786.935303 \n",
      "(GPU: 0, epoch: 20, iters: 33920, time: 0.008) nll: 3109.337646 \n",
      "(GPU: 0, epoch: 20, iters: 34720, time: 0.008) nll: 3715.045166 \n",
      "(GPU: 0, epoch: 20, iters: 35520, time: 0.008) nll: 3981.599365 \n",
      "(GPU: 0, epoch: 20, iters: 36320, time: 0.008) nll: 3177.894531 \n",
      "(GPU: 0, epoch: 20, iters: 37120, time: 0.008) nll: 2968.908203 \n",
      "(GPU: 0, epoch: 20, iters: 37920, time: 0.008) nll: 3930.997559 \n",
      "(GPU: 0, epoch: 20, iters: 38720, time: 0.008) nll: 3444.678955 \n",
      "(GPU: 0, epoch: 20, iters: 39520, time: 0.008) nll: 3397.688477 \n",
      "(GPU: 0, epoch: 20, iters: 40320, time: 0.008) nll: 3640.889160 \n",
      "(GPU: 0, epoch: 20, iters: 41120, time: 0.008) nll: 4182.354004 \n",
      "(GPU: 0, epoch: 20, iters: 41920, time: 0.008) nll: 2923.789551 \n",
      "(GPU: 0, epoch: 20, iters: 42720, time: 0.008) nll: 3934.736328 \n",
      "(GPU: 0, epoch: 20, iters: 43520, time: 0.008) nll: 3234.635986 \n",
      "(GPU: 0, epoch: 20, iters: 44320, time: 0.008) nll: 4064.916992 \n",
      "(GPU: 0, epoch: 20, iters: 45120, time: 0.008) nll: 3876.739746 \n",
      "(GPU: 0, epoch: 20, iters: 45920, time: 0.008) nll: 3954.271729 \n",
      "saving the latest model (epoch 20, total_steps 2860000)\n",
      "(GPU: 0, epoch: 20, iters: 46720, time: 0.008) nll: 3181.220703 \n",
      "(GPU: 0, epoch: 20, iters: 47520, time: 0.008) nll: 3281.947510 \n",
      "(GPU: 0, epoch: 20, iters: 48320, time: 0.008) nll: 3371.339844 \n",
      "(GPU: 0, epoch: 20, iters: 49120, time: 0.008) nll: 3088.244873 \n",
      "(GPU: 0, epoch: 20, iters: 49920, time: 0.008) nll: 4088.272705 \n",
      "(GPU: 0, epoch: 20, iters: 50720, time: 0.008) nll: 4468.635742 \n",
      "(GPU: 0, epoch: 20, iters: 51520, time: 0.008) nll: 4151.019043 \n",
      "(GPU: 0, epoch: 20, iters: 52320, time: 0.008) nll: 3319.543457 \n",
      "(GPU: 0, epoch: 20, iters: 53120, time: 0.008) nll: 3684.955811 \n",
      "(GPU: 0, epoch: 20, iters: 53920, time: 0.008) nll: 5681.026367 \n",
      "(GPU: 0, epoch: 20, iters: 54720, time: 0.008) nll: 5035.365234 \n",
      "(GPU: 0, epoch: 20, iters: 55520, time: 0.008) nll: 3420.198730 \n",
      "(GPU: 0, epoch: 20, iters: 56320, time: 0.008) nll: 4088.666016 \n",
      "(GPU: 0, epoch: 20, iters: 57120, time: 0.008) nll: 4230.593750 \n",
      "(GPU: 0, epoch: 20, iters: 57920, time: 0.008) nll: 3284.405273 \n",
      "(GPU: 0, epoch: 20, iters: 58720, time: 0.008) nll: 3869.041016 \n",
      "(GPU: 0, epoch: 20, iters: 59520, time: 0.008) nll: 4700.958008 \n",
      "(GPU: 0, epoch: 20, iters: 60320, time: 0.008) nll: 5858.551270 \n",
      "(GPU: 0, epoch: 20, iters: 61120, time: 0.008) nll: 4378.764648 \n",
      "(GPU: 0, epoch: 20, iters: 61920, time: 0.008) nll: 5243.056641 \n",
      "(GPU: 0, epoch: 20, iters: 62720, time: 0.008) nll: 4431.954102 \n",
      "(GPU: 0, epoch: 20, iters: 63520, time: 0.008) nll: 3784.422363 \n",
      "(GPU: 0, epoch: 20, iters: 64320, time: 0.008) nll: 4290.713867 \n",
      "(GPU: 0, epoch: 20, iters: 65120, time: 0.008) nll: 4700.775391 \n",
      "(GPU: 0, epoch: 20, iters: 65920, time: 0.008) nll: 2930.748047 \n",
      "(GPU: 0, epoch: 20, iters: 65920, time: 0.013) nll: 2889.452148 \n",
      "(GPU: 0, epoch: 20, iters: 65920, time: 0.013) nll: 3820.489502 \n",
      "saving the latest model (epoch 20, total_steps 2880000)\n",
      "(GPU: 0, epoch: 20, iters: 66720, time: 0.007) nll: 4786.943359 \n",
      "(GPU: 0, epoch: 20, iters: 67520, time: 0.008) nll: 3378.459717 \n",
      "(GPU: 0, epoch: 20, iters: 68320, time: 0.008) nll: 3501.277344 \n",
      "(GPU: 0, epoch: 20, iters: 69120, time: 0.008) nll: 3087.854248 \n",
      "(GPU: 0, epoch: 20, iters: 69920, time: 0.008) nll: 4091.052979 \n",
      "(GPU: 0, epoch: 20, iters: 70720, time: 0.008) nll: 3158.251709 \n",
      "(GPU: 0, epoch: 20, iters: 71520, time: 0.008) nll: 3941.923340 \n",
      "(GPU: 0, epoch: 20, iters: 72320, time: 0.008) nll: 3321.274902 \n",
      "(GPU: 0, epoch: 20, iters: 73120, time: 0.008) nll: 4934.753418 \n",
      "(GPU: 0, epoch: 20, iters: 73920, time: 0.008) nll: 4332.948242 \n",
      "(GPU: 0, epoch: 20, iters: 74720, time: 0.008) nll: 3964.876465 \n",
      "(GPU: 0, epoch: 20, iters: 75520, time: 0.008) nll: 3863.632080 \n",
      "(GPU: 0, epoch: 20, iters: 76320, time: 0.008) nll: 2360.324219 \n",
      "(GPU: 0, epoch: 20, iters: 77120, time: 0.008) nll: 3388.006836 \n",
      "(GPU: 0, epoch: 20, iters: 77920, time: 0.008) nll: 3476.603027 \n",
      "(GPU: 0, epoch: 20, iters: 78720, time: 0.008) nll: 3454.132080 \n",
      "(GPU: 0, epoch: 20, iters: 79520, time: 0.008) nll: 3553.005615 \n",
      "(GPU: 0, epoch: 20, iters: 80320, time: 0.008) nll: 4779.044922 \n",
      "(GPU: 0, epoch: 20, iters: 81120, time: 0.008) nll: 3754.088867 \n",
      "(GPU: 0, epoch: 20, iters: 81920, time: 0.008) nll: 3496.994385 \n",
      "(GPU: 0, epoch: 20, iters: 82720, time: 0.008) nll: 2969.986084 \n",
      "(GPU: 0, epoch: 20, iters: 83520, time: 0.008) nll: 4129.911133 \n",
      "(GPU: 0, epoch: 20, iters: 84320, time: 0.008) nll: 3044.470459 \n",
      "(GPU: 0, epoch: 20, iters: 85120, time: 0.008) nll: 2776.653320 \n",
      "(GPU: 0, epoch: 20, iters: 85920, time: 0.008) nll: 4111.935059 \n",
      "saving the latest model (epoch 20, total_steps 2900000)\n",
      "(GPU: 0, epoch: 20, iters: 86720, time: 0.008) nll: 4439.251465 \n",
      "(GPU: 0, epoch: 20, iters: 87520, time: 0.008) nll: 2556.600586 \n",
      "(GPU: 0, epoch: 20, iters: 88320, time: 0.008) nll: 4007.398438 \n",
      "(GPU: 0, epoch: 20, iters: 89120, time: 0.008) nll: 3120.591553 \n",
      "(GPU: 0, epoch: 20, iters: 89920, time: 0.008) nll: 3477.902588 \n",
      "(GPU: 0, epoch: 20, iters: 90720, time: 0.008) nll: 3221.035645 \n",
      "(GPU: 0, epoch: 20, iters: 91520, time: 0.008) nll: 4937.827148 \n",
      "(GPU: 0, epoch: 20, iters: 92320, time: 0.008) nll: 4298.557617 \n",
      "(GPU: 0, epoch: 20, iters: 93120, time: 0.008) nll: 5275.461426 \n",
      "(GPU: 0, epoch: 20, iters: 93920, time: 0.008) nll: 3616.945801 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 4000/4397 [18:47<01:42,  3.87it/s]  "
     ]
    }
   ],
   "source": [
    "rc = subprocess.call(\"./launchers/train_rand_tf_snet_code.sh\", shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981f8cb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
