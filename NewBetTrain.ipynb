{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21f9292f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28ddad9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: ./launchers/train_new_bert.sh: Permission denied\n"
     ]
    }
   ],
   "source": [
    "rc = subprocess.call(\"./launchers/train_new_bert.sh\", shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f9b0a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autosdf.yaml\t\t      filelists\t\t      README.md\r\n",
      "Compare.ipynb\t\t      info-shapenet.json      results\r\n",
      "configs\t\t\t      launchers\t\t      shape_set_paths.json\r\n",
      "datasets\t\t      logs\t\t      Test-Reproduce.ipynb\r\n",
      "demo_data\t\t      logs2\t\t      test_samples_paper.txt\r\n",
      "demo-lang-conditional.ipynb   models\t\t      text2ShapePP.json\r\n",
      "demo_shape_comp.ipynb\t      New-Bert-Sandbox.ipynb  train.py\r\n",
      "demo_single_view_recon.ipynb  NewBetTrain.ipynb       utils\r\n",
      "extract_code.py\t\t      options\r\n",
      "file.json\t\t      preprocess\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03f6cb39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Options -------------\n",
      "alpha: 0.75\n",
      "batch_size: 32\n",
      "bert_cfg: configs/bert2vq_shapeglot.yaml\n",
      "cat: chair\n",
      "checkpoints_dir: ./checkpoints\n",
      "ckpt: None\n",
      "continue_train: False\n",
      "dataset_mode: text2shape-seq\n",
      "debug: 0\n",
      "device: cuda\n",
      "display_freq: 3000\n",
      "gpu_ids: [0]\n",
      "gpu_ids_str: 0\n",
      "input_nc: 3\n",
      "iou_thres: 0.0\n",
      "isTrain: True\n",
      "lambda_L1: 10.0\n",
      "logs_dir: ./logs\n",
      "lr: 0.0001\n",
      "lr_decay_iters: 50\n",
      "lr_policy: lambda\n",
      "max_dataset_size: 100000000000\n",
      "model: bert2vqsc_v4\n",
      "nThreads: 9\n",
      "n_less: 0\n",
      "name: bert2vqsc_v4-text2shape-seq-LR1e-4-network-bert-shapeset-shapeset-SmoothL1Loss\n",
      "ndf: 64\n",
      "nepochs: 20\n",
      "nepochs_decay: 5\n",
      "ngf: 64\n",
      "output_nc: 3\n",
      "pix3d_mode: noBG\n",
      "print_freq: 25\n",
      "profiler: 0\n",
      "ratio: 1.0\n",
      "resnet2vq_ckpt: None\n",
      "resnet_arch: resnet18\n",
      "resnet_cfg: configs/resnet2vq_pix3d.yaml\n",
      "resnet_ckpt: None\n",
      "resnet_dset: None\n",
      "resnet_model: None\n",
      "resnet_norm: gn\n",
      "save_epoch_freq: 3\n",
      "save_latest_freq: 5000\n",
      "seed: 111\n",
      "serial_batches: False\n",
      "snet_mode: noBG\n",
      "tf_cfg: configs/rand_tf_snet_code.yaml\n",
      "topk: 30\n",
      "trunc_thres: 0.2\n",
      "use_bin_sdf: 0\n",
      "use_marginal: 0\n",
      "vq_cat: chair\n",
      "vq_cfg: configs/pvqvae_snet.yaml\n",
      "vq_ckpt: ../raw_dataset/checkpoints/vqvae.pth\n",
      "vq_dset: snet\n",
      "vq_model: pvqvae\n",
      "vq_note: default\n",
      "which_epoch: latest\n",
      "-------------- End ----------------\n",
      "[*] Dataset has been created: Text2Shape\n",
      "[*] # training images = 140707\n",
      "[*] # testing images = 16000\n",
      "---------- Networks initialized -------------\n",
      "-----------------------------------------------\n",
      "[*] Model has been created: BERT2VQSC-Model\n",
      "[*] \"bert2vqsc_v4\" initialized.\n",
      "[*] create image directory:\n",
      "/cluster/54/streakfull/ADL4CV/Project/src/plz-autosdf/logs/bert2vqsc_v4-text2shape-seq-LR1e-4-network-bert-shapeset-shapeset-SmoothL1Loss/images...\n",
      "[*] saving model and dataset files: /cluster/54/streakfull/ADL4CV/Project/src/plz-autosdf/models/bert2vq_scmodel_v4.py, /cluster/54/streakfull/ADL4CV/Project/src/plz-autosdf/datasets/text2shape.py\n",
      "140707 Length train dataset\n",
      "16000 Length test dataset\n",
      "4397 Length train_dl\n",
      "500 Length test_dl\n",
      "[*] Start training. name: bert2vqsc_v4-text2shape-seq-LR1e-4-network-bert-shapeset-shapeset-SmoothL1Loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/4397 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 0, iters: 32, time: 0.030) nll: 240.664337 \n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 3124/4397 [14:34<05:34,  3.80it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 0, iters: 32, time: 0.030) nll: 240.354553 \n",
      "(GPU: 0, epoch: 0, iters: 800, time: 0.008) nll: 129.345184 \n",
      "(GPU: 0, epoch: 0, iters: 1600, time: 0.008) nll: 110.621376 \n",
      "(GPU: 0, epoch: 0, iters: 2400, time: 0.008) nll: 107.525406 \n",
      "(GPU: 0, epoch: 0, iters: 3200, time: 0.008) nll: 96.924385 \n",
      "(GPU: 0, epoch: 0, iters: 4000, time: 0.008) nll: 85.864388 \n",
      "(GPU: 0, epoch: 0, iters: 4800, time: 0.008) nll: 65.093399 \n",
      "(GPU: 0, epoch: 0, iters: 5600, time: 0.008) nll: 59.234463 \n",
      "(GPU: 0, epoch: 0, iters: 6400, time: 0.008) nll: 83.974869 \n",
      "(GPU: 0, epoch: 0, iters: 7200, time: 0.008) nll: 78.900955 \n",
      "(GPU: 0, epoch: 0, iters: 8000, time: 0.008) nll: 79.716690 \n",
      "(GPU: 0, epoch: 0, iters: 8800, time: 0.008) nll: 96.696930 \n",
      "(GPU: 0, epoch: 0, iters: 9600, time: 0.008) nll: 115.157394 \n",
      "(GPU: 0, epoch: 0, iters: 10400, time: 0.008) nll: 83.096100 \n",
      "(GPU: 0, epoch: 0, iters: 11200, time: 0.008) nll: 62.225426 \n",
      "(GPU: 0, epoch: 0, iters: 12000, time: 0.008) nll: 68.056015 \n",
      "(GPU: 0, epoch: 0, iters: 12800, time: 0.008) nll: 69.393646 \n",
      "(GPU: 0, epoch: 0, iters: 13600, time: 0.008) nll: 73.041061 \n",
      "(GPU: 0, epoch: 0, iters: 14400, time: 0.008) nll: 69.099762 \n",
      "(GPU: 0, epoch: 0, iters: 15200, time: 0.008) nll: 66.657791 \n",
      "(GPU: 0, epoch: 0, iters: 16000, time: 0.008) nll: 80.582642 \n",
      "(GPU: 0, epoch: 0, iters: 16800, time: 0.008) nll: 58.233032 \n",
      "(GPU: 0, epoch: 0, iters: 17600, time: 0.008) nll: 73.335190 \n",
      "(GPU: 0, epoch: 0, iters: 18400, time: 0.008) nll: 62.650444 \n",
      "(GPU: 0, epoch: 0, iters: 19200, time: 0.008) nll: 77.069618 \n",
      "(GPU: 0, epoch: 0, iters: 20000, time: 0.008) nll: 78.945007 \n",
      "saving the latest model (epoch 0, total_steps 20000)\n",
      "(GPU: 0, epoch: 0, iters: 20800, time: 0.008) nll: 77.323677 \n",
      "(GPU: 0, epoch: 0, iters: 21600, time: 0.008) nll: 74.883667 \n",
      "(GPU: 0, epoch: 0, iters: 22400, time: 0.008) nll: 60.009109 \n",
      "(GPU: 0, epoch: 0, iters: 23200, time: 0.008) nll: 70.161339 \n",
      "(GPU: 0, epoch: 0, iters: 24000, time: 0.008) nll: 65.328423 \n",
      "(GPU: 0, epoch: 0, iters: 24800, time: 0.008) nll: 73.653809 \n",
      "(GPU: 0, epoch: 0, iters: 25600, time: 0.008) nll: 72.213684 \n",
      "(GPU: 0, epoch: 0, iters: 26400, time: 0.008) nll: 61.948048 \n",
      "(GPU: 0, epoch: 0, iters: 27200, time: 0.008) nll: 72.617828 \n",
      "(GPU: 0, epoch: 0, iters: 28000, time: 0.008) nll: 57.473679 \n",
      "(GPU: 0, epoch: 0, iters: 28800, time: 0.008) nll: 71.929520 \n",
      "(GPU: 0, epoch: 0, iters: 29600, time: 0.007) nll: 74.774551 \n",
      "(GPU: 0, epoch: 0, iters: 30400, time: 0.008) nll: 84.095871 \n",
      "(GPU: 0, epoch: 0, iters: 31200, time: 0.008) nll: 68.767433 \n",
      "(GPU: 0, epoch: 0, iters: 32000, time: 0.008) nll: 69.426888 \n",
      "(GPU: 0, epoch: 0, iters: 32800, time: 0.008) nll: 61.778885 \n",
      "(GPU: 0, epoch: 0, iters: 33600, time: 0.008) nll: 56.316761 \n",
      "(GPU: 0, epoch: 0, iters: 34400, time: 0.008) nll: 76.178452 \n",
      "(GPU: 0, epoch: 0, iters: 35200, time: 0.008) nll: 57.331314 \n",
      "(GPU: 0, epoch: 0, iters: 36000, time: 0.008) nll: 87.246796 \n",
      "(GPU: 0, epoch: 0, iters: 36800, time: 0.008) nll: 73.071449 \n",
      "(GPU: 0, epoch: 0, iters: 37600, time: 0.008) nll: 69.556267 \n",
      "(GPU: 0, epoch: 0, iters: 38400, time: 0.008) nll: 85.353851 \n",
      "(GPU: 0, epoch: 0, iters: 39200, time: 0.007) nll: 102.142426 \n",
      "(GPU: 0, epoch: 0, iters: 40000, time: 0.008) nll: 81.279922 \n",
      "saving the latest model (epoch 0, total_steps 40000)\n",
      "(GPU: 0, epoch: 0, iters: 40800, time: 0.008) nll: 52.720688 \n",
      "(GPU: 0, epoch: 0, iters: 41600, time: 0.008) nll: 61.999573 \n",
      "(GPU: 0, epoch: 0, iters: 42400, time: 0.008) nll: 82.148102 \n",
      "(GPU: 0, epoch: 0, iters: 43200, time: 0.008) nll: 51.275173 \n",
      "(GPU: 0, epoch: 0, iters: 44000, time: 0.008) nll: 79.192711 \n",
      "(GPU: 0, epoch: 0, iters: 44800, time: 0.008) nll: 82.216484 \n",
      "(GPU: 0, epoch: 0, iters: 45600, time: 0.008) nll: 79.070396 \n",
      "(GPU: 0, epoch: 0, iters: 46400, time: 0.008) nll: 49.822636 \n",
      "(GPU: 0, epoch: 0, iters: 47200, time: 0.008) nll: 56.702976 \n",
      "(GPU: 0, epoch: 0, iters: 48000, time: 0.008) nll: 53.886944 \n",
      "(GPU: 0, epoch: 0, iters: 48800, time: 0.008) nll: 70.398323 \n",
      "(GPU: 0, epoch: 0, iters: 49600, time: 0.008) nll: 74.682625 \n",
      "(GPU: 0, epoch: 0, iters: 50400, time: 0.008) nll: 43.416096 \n",
      "(GPU: 0, epoch: 0, iters: 51200, time: 0.008) nll: 76.068100 \n",
      "(GPU: 0, epoch: 0, iters: 52000, time: 0.008) nll: 74.182686 \n",
      "(GPU: 0, epoch: 0, iters: 52800, time: 0.008) nll: 91.033897 \n",
      "(GPU: 0, epoch: 0, iters: 53600, time: 0.008) nll: 53.468891 \n",
      "(GPU: 0, epoch: 0, iters: 54400, time: 0.009) nll: 72.098312 \n",
      "(GPU: 0, epoch: 0, iters: 55200, time: 0.008) nll: 52.469650 \n",
      "(GPU: 0, epoch: 0, iters: 56000, time: 0.008) nll: 82.265182 \n",
      "(GPU: 0, epoch: 0, iters: 56800, time: 0.007) nll: 68.928993 \n",
      "(GPU: 0, epoch: 0, iters: 57600, time: 0.008) nll: 75.702469 \n",
      "(GPU: 0, epoch: 0, iters: 58400, time: 0.008) nll: 55.673550 \n",
      "(GPU: 0, epoch: 0, iters: 59200, time: 0.008) nll: 91.383118 \n",
      "(GPU: 0, epoch: 0, iters: 60000, time: 0.008) nll: 64.341011 \n",
      "saving the latest model (epoch 0, total_steps 60000)\n",
      "(GPU: 0, epoch: 0, iters: 60800, time: 0.008) nll: 69.399651 \n",
      "(GPU: 0, epoch: 0, iters: 61600, time: 0.008) nll: 69.961517 \n",
      "(GPU: 0, epoch: 0, iters: 62400, time: 0.008) nll: 70.966881 \n",
      "(GPU: 0, epoch: 0, iters: 63200, time: 0.008) nll: 114.820724 \n",
      "(GPU: 0, epoch: 0, iters: 64000, time: 0.008) nll: 85.922836 \n",
      "(GPU: 0, epoch: 0, iters: 64800, time: 0.008) nll: 60.957470 \n",
      "(GPU: 0, epoch: 0, iters: 65600, time: 0.008) nll: 71.463867 \n",
      "(GPU: 0, epoch: 0, iters: 66400, time: 0.008) nll: 70.730209 \n",
      "(GPU: 0, epoch: 0, iters: 67200, time: 0.008) nll: 57.767899 \n",
      "(GPU: 0, epoch: 0, iters: 68000, time: 0.008) nll: 79.811234 \n",
      "(GPU: 0, epoch: 0, iters: 68800, time: 0.008) nll: 63.331852 \n",
      "(GPU: 0, epoch: 0, iters: 69600, time: 0.008) nll: 70.528534 \n",
      "(GPU: 0, epoch: 0, iters: 70400, time: 0.008) nll: 55.110779 \n",
      "(GPU: 0, epoch: 0, iters: 71200, time: 0.008) nll: 73.546310 \n",
      "(GPU: 0, epoch: 0, iters: 72000, time: 0.008) nll: 53.104958 \n",
      "(GPU: 0, epoch: 0, iters: 72800, time: 0.008) nll: 53.765026 \n",
      "(GPU: 0, epoch: 0, iters: 73600, time: 0.008) nll: 63.090183 \n",
      "(GPU: 0, epoch: 0, iters: 74400, time: 0.008) nll: 93.349030 \n",
      "(GPU: 0, epoch: 0, iters: 75200, time: 0.008) nll: 65.084564 \n",
      "(GPU: 0, epoch: 0, iters: 76000, time: 0.008) nll: 60.687870 \n",
      "(GPU: 0, epoch: 0, iters: 76800, time: 0.008) nll: 64.122757 \n",
      "(GPU: 0, epoch: 0, iters: 77600, time: 0.008) nll: 74.363388 \n",
      "(GPU: 0, epoch: 0, iters: 78400, time: 0.008) nll: 76.251648 \n",
      "(GPU: 0, epoch: 0, iters: 79200, time: 0.008) nll: 78.140808 \n",
      "(GPU: 0, epoch: 0, iters: 80000, time: 0.008) nll: 56.862785 \n",
      "saving the latest model (epoch 0, total_steps 80000)\n",
      "(GPU: 0, epoch: 0, iters: 80800, time: 0.008) nll: 61.547180 \n",
      "(GPU: 0, epoch: 0, iters: 81600, time: 0.008) nll: 51.698357 \n",
      "(GPU: 0, epoch: 0, iters: 82400, time: 0.008) nll: 75.568130 \n",
      "(GPU: 0, epoch: 0, iters: 83200, time: 0.008) nll: 87.314041 \n",
      "(GPU: 0, epoch: 0, iters: 84000, time: 0.008) nll: 69.096191 \n",
      "(GPU: 0, epoch: 0, iters: 84800, time: 0.008) nll: 78.365738 \n",
      "(GPU: 0, epoch: 0, iters: 85600, time: 0.008) nll: 91.403458 \n",
      "(GPU: 0, epoch: 0, iters: 86400, time: 0.008) nll: 65.441528 \n",
      "(GPU: 0, epoch: 0, iters: 87200, time: 0.008) nll: 70.320007 \n",
      "(GPU: 0, epoch: 0, iters: 88000, time: 0.008) nll: 63.739235 \n",
      "(GPU: 0, epoch: 0, iters: 88800, time: 0.008) nll: 76.709900 \n",
      "(GPU: 0, epoch: 0, iters: 89600, time: 0.008) nll: 58.728638 \n",
      "(GPU: 0, epoch: 0, iters: 90400, time: 0.008) nll: 60.084553 \n",
      "(GPU: 0, epoch: 0, iters: 91200, time: 0.008) nll: 86.726562 \n",
      "(GPU: 0, epoch: 0, iters: 92000, time: 0.008) nll: 92.539772 \n",
      "(GPU: 0, epoch: 0, iters: 92800, time: 0.008) nll: 76.826172 \n",
      "(GPU: 0, epoch: 0, iters: 93600, time: 0.008) nll: 66.476616 \n",
      "(GPU: 0, epoch: 0, iters: 94400, time: 0.008) nll: 72.571060 \n",
      "(GPU: 0, epoch: 0, iters: 95200, time: 0.008) nll: 76.052155 \n",
      "(GPU: 0, epoch: 0, iters: 96000, time: 0.008) nll: 62.555164 \n",
      "(GPU: 0, epoch: 0, iters: 96000, time: 0.013) nll: 62.489784 \n",
      "(GPU: 0, epoch: 0, iters: 96000, time: 0.013) nll: 84.725800 \n",
      "(GPU: 0, epoch: 0, iters: 96800, time: 0.008) nll: 78.127266 \n",
      "(GPU: 0, epoch: 0, iters: 97600, time: 0.008) nll: 54.575211 \n",
      "(GPU: 0, epoch: 0, iters: 98400, time: 0.008) nll: 79.266769 \n",
      "(GPU: 0, epoch: 0, iters: 99200, time: 0.008) nll: 62.845413 \n",
      "(GPU: 0, epoch: 0, iters: 100000, time: 0.008) nll: 82.590408 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4397/4397 [20:49<00:00,  3.52it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving the latest model (epoch 0, total_steps 100000)\n",
      "(GPU: 0, epoch: 0, iters: 100800, time: 0.008) nll: 68.229073 \n",
      "(GPU: 0, epoch: 0, iters: 101600, time: 0.008) nll: 76.803085 \n",
      "(GPU: 0, epoch: 0, iters: 102400, time: 0.008) nll: 90.661888 \n",
      "(GPU: 0, epoch: 0, iters: 103200, time: 0.008) nll: 41.672966 \n",
      "(GPU: 0, epoch: 0, iters: 104000, time: 0.008) nll: 73.993866 \n",
      "(GPU: 0, epoch: 0, iters: 104800, time: 0.007) nll: 55.084366 \n",
      "(GPU: 0, epoch: 0, iters: 105600, time: 0.008) nll: 79.969292 \n",
      "(GPU: 0, epoch: 0, iters: 106400, time: 0.008) nll: 79.249359 \n",
      "(GPU: 0, epoch: 0, iters: 107200, time: 0.008) nll: 79.365814 \n",
      "(GPU: 0, epoch: 0, iters: 108000, time: 0.008) nll: 37.751160 \n",
      "(GPU: 0, epoch: 0, iters: 108800, time: 0.008) nll: 58.322235 \n",
      "(GPU: 0, epoch: 0, iters: 109600, time: 0.008) nll: 79.457977 \n",
      "(GPU: 0, epoch: 0, iters: 110400, time: 0.008) nll: 79.988373 \n",
      "(GPU: 0, epoch: 0, iters: 111200, time: 0.008) nll: 86.318527 \n",
      "(GPU: 0, epoch: 0, iters: 112000, time: 0.008) nll: 76.370110 \n",
      "(GPU: 0, epoch: 0, iters: 112800, time: 0.008) nll: 59.354431 \n",
      "(GPU: 0, epoch: 0, iters: 113600, time: 0.008) nll: 79.885742 \n",
      "(GPU: 0, epoch: 0, iters: 114400, time: 0.007) nll: 43.585579 \n",
      "(GPU: 0, epoch: 0, iters: 115200, time: 0.008) nll: 58.733986 \n",
      "(GPU: 0, epoch: 0, iters: 116000, time: 0.008) nll: 66.081734 \n",
      "(GPU: 0, epoch: 0, iters: 116800, time: 0.008) nll: 57.383057 \n",
      "(GPU: 0, epoch: 0, iters: 117600, time: 0.008) nll: 68.492737 \n",
      "(GPU: 0, epoch: 0, iters: 118400, time: 0.008) nll: 81.573860 \n",
      "(GPU: 0, epoch: 0, iters: 119200, time: 0.008) nll: 82.306206 \n",
      "(GPU: 0, epoch: 0, iters: 120000, time: 0.008) nll: 63.682564 \n",
      "saving the latest model (epoch 0, total_steps 120000)\n",
      "(GPU: 0, epoch: 0, iters: 120800, time: 0.008) nll: 73.909149 \n",
      "(GPU: 0, epoch: 0, iters: 121600, time: 0.008) nll: 73.044975 \n",
      "(GPU: 0, epoch: 0, iters: 122400, time: 0.008) nll: 78.722435 \n",
      "(GPU: 0, epoch: 0, iters: 123200, time: 0.008) nll: 77.990959 \n",
      "(GPU: 0, epoch: 0, iters: 124000, time: 0.008) nll: 74.310028 \n",
      "(GPU: 0, epoch: 0, iters: 124800, time: 0.008) nll: 65.073547 \n",
      "(GPU: 0, epoch: 0, iters: 125600, time: 0.008) nll: 71.290680 \n",
      "(GPU: 0, epoch: 0, iters: 126400, time: 0.008) nll: 73.784630 \n",
      "(GPU: 0, epoch: 0, iters: 127200, time: 0.008) nll: 55.103458 \n",
      "(GPU: 0, epoch: 0, iters: 128000, time: 0.008) nll: 82.031677 \n",
      "(GPU: 0, epoch: 0, iters: 128800, time: 0.008) nll: 67.443817 \n",
      "(GPU: 0, epoch: 0, iters: 129600, time: 0.008) nll: 69.189896 \n",
      "(GPU: 0, epoch: 0, iters: 130400, time: 0.008) nll: 83.325272 \n",
      "(GPU: 0, epoch: 0, iters: 131200, time: 0.008) nll: 70.181427 \n",
      "(GPU: 0, epoch: 0, iters: 132000, time: 0.008) nll: 68.516418 \n",
      "(GPU: 0, epoch: 0, iters: 132800, time: 0.008) nll: 62.878891 \n",
      "(GPU: 0, epoch: 0, iters: 133600, time: 0.008) nll: 70.548645 \n",
      "(GPU: 0, epoch: 0, iters: 134400, time: 0.008) nll: 45.961128 \n",
      "(GPU: 0, epoch: 0, iters: 135200, time: 0.008) nll: 74.145988 \n",
      "(GPU: 0, epoch: 0, iters: 136000, time: 0.008) nll: 79.475204 \n",
      "(GPU: 0, epoch: 0, iters: 136800, time: 0.008) nll: 68.759666 \n",
      "(GPU: 0, epoch: 0, iters: 137600, time: 0.008) nll: 75.432716 \n",
      "(GPU: 0, epoch: 0, iters: 138400, time: 0.008) nll: 65.946579 \n",
      "(GPU: 0, epoch: 0, iters: 139200, time: 0.008) nll: 49.928993 \n",
      "(GPU: 0, epoch: 0, iters: 140000, time: 0.008) nll: 60.217064 \n",
      "saving the latest model (epoch 0, total_steps 140000)\n",
      "saving the model at the end of epoch 0, iters 140704\n",
      "([test] GPU: 0, epoch: 0) \n",
      "OrderedDict()\n",
      "[*] End of epoch 0 / 25 \t Time Taken: 1278 sec \n",
      "/cluster/54/streakfull/ADL4CV/Project/src/plz-autosdf/logs/bert2vqsc_v4-text2shape-seq-LR1e-4-network-bert-shapeset-shapeset-SmoothL1Loss\n",
      "[*] learning rate = 0.0000100\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 3102/4397 [14:28<05:38,  3.83it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 1, iters: 32, time: 0.004) nll: 63.981133 \n",
      "(GPU: 0, epoch: 1, iters: 32, time: 0.004) nll: 70.251152 \n",
      "(GPU: 0, epoch: 1, iters: 96, time: 0.007) nll: 68.419777 \n",
      "(GPU: 0, epoch: 1, iters: 896, time: 0.008) nll: 62.335129 \n",
      "(GPU: 0, epoch: 1, iters: 1696, time: 0.008) nll: 66.615921 \n",
      "(GPU: 0, epoch: 1, iters: 2496, time: 0.008) nll: 63.899040 \n",
      "(GPU: 0, epoch: 1, iters: 3296, time: 0.008) nll: 73.860435 \n",
      "(GPU: 0, epoch: 1, iters: 4096, time: 0.008) nll: 51.304337 \n",
      "(GPU: 0, epoch: 1, iters: 4896, time: 0.008) nll: 60.655785 \n",
      "(GPU: 0, epoch: 1, iters: 5696, time: 0.008) nll: 89.711670 \n",
      "(GPU: 0, epoch: 1, iters: 6496, time: 0.008) nll: 81.207825 \n",
      "(GPU: 0, epoch: 1, iters: 7296, time: 0.008) nll: 67.529068 \n",
      "(GPU: 0, epoch: 1, iters: 8096, time: 0.008) nll: 88.866959 \n",
      "(GPU: 0, epoch: 1, iters: 8896, time: 0.008) nll: 45.485931 \n",
      "(GPU: 0, epoch: 1, iters: 9696, time: 0.008) nll: 80.504555 \n",
      "(GPU: 0, epoch: 1, iters: 10496, time: 0.008) nll: 52.929314 \n",
      "(GPU: 0, epoch: 1, iters: 11296, time: 0.008) nll: 82.368698 \n",
      "(GPU: 0, epoch: 1, iters: 12096, time: 0.008) nll: 68.797737 \n",
      "(GPU: 0, epoch: 1, iters: 12896, time: 0.008) nll: 62.408661 \n",
      "(GPU: 0, epoch: 1, iters: 13696, time: 0.008) nll: 54.780632 \n",
      "(GPU: 0, epoch: 1, iters: 14496, time: 0.008) nll: 69.501534 \n",
      "(GPU: 0, epoch: 1, iters: 15296, time: 0.008) nll: 72.015694 \n",
      "(GPU: 0, epoch: 1, iters: 16096, time: 0.008) nll: 67.644440 \n",
      "(GPU: 0, epoch: 1, iters: 16896, time: 0.008) nll: 79.301834 \n",
      "(GPU: 0, epoch: 1, iters: 17696, time: 0.008) nll: 64.567490 \n",
      "(GPU: 0, epoch: 1, iters: 18496, time: 0.008) nll: 50.653130 \n",
      "(GPU: 0, epoch: 1, iters: 19296, time: 0.008) nll: 82.287918 \n",
      "saving the latest model (epoch 1, total_steps 160000)\n",
      "(GPU: 0, epoch: 1, iters: 20096, time: 0.008) nll: 47.612274 \n",
      "(GPU: 0, epoch: 1, iters: 20896, time: 0.008) nll: 71.859421 \n",
      "(GPU: 0, epoch: 1, iters: 21696, time: 0.008) nll: 48.879864 \n",
      "(GPU: 0, epoch: 1, iters: 22496, time: 0.008) nll: 79.346832 \n",
      "(GPU: 0, epoch: 1, iters: 23296, time: 0.008) nll: 40.860825 \n",
      "(GPU: 0, epoch: 1, iters: 24096, time: 0.008) nll: 49.313812 \n",
      "(GPU: 0, epoch: 1, iters: 24896, time: 0.008) nll: 83.373184 \n",
      "(GPU: 0, epoch: 1, iters: 25696, time: 0.008) nll: 69.337250 \n",
      "(GPU: 0, epoch: 1, iters: 26496, time: 0.008) nll: 57.176582 \n",
      "(GPU: 0, epoch: 1, iters: 27296, time: 0.008) nll: 64.098862 \n",
      "(GPU: 0, epoch: 1, iters: 28096, time: 0.008) nll: 56.034340 \n",
      "(GPU: 0, epoch: 1, iters: 28896, time: 0.008) nll: 83.438599 \n",
      "(GPU: 0, epoch: 1, iters: 29696, time: 0.008) nll: 59.144554 \n",
      "(GPU: 0, epoch: 1, iters: 30496, time: 0.008) nll: 76.350266 \n",
      "(GPU: 0, epoch: 1, iters: 31296, time: 0.008) nll: 83.496414 \n",
      "(GPU: 0, epoch: 1, iters: 32096, time: 0.008) nll: 67.459480 \n",
      "(GPU: 0, epoch: 1, iters: 32896, time: 0.008) nll: 73.284760 \n",
      "(GPU: 0, epoch: 1, iters: 33696, time: 0.008) nll: 68.826584 \n",
      "(GPU: 0, epoch: 1, iters: 34496, time: 0.008) nll: 94.442947 \n",
      "(GPU: 0, epoch: 1, iters: 35296, time: 0.008) nll: 42.084206 \n",
      "(GPU: 0, epoch: 1, iters: 36096, time: 0.008) nll: 69.584152 \n",
      "(GPU: 0, epoch: 1, iters: 36896, time: 0.008) nll: 71.939301 \n",
      "(GPU: 0, epoch: 1, iters: 37696, time: 0.008) nll: 56.736607 \n",
      "(GPU: 0, epoch: 1, iters: 38496, time: 0.008) nll: 72.014679 \n",
      "(GPU: 0, epoch: 1, iters: 39296, time: 0.008) nll: 76.265312 \n",
      "saving the latest model (epoch 1, total_steps 180000)\n",
      "(GPU: 0, epoch: 1, iters: 40096, time: 0.008) nll: 65.907234 \n",
      "(GPU: 0, epoch: 1, iters: 40896, time: 0.008) nll: 79.044174 \n",
      "(GPU: 0, epoch: 1, iters: 41696, time: 0.008) nll: 62.562084 \n",
      "(GPU: 0, epoch: 1, iters: 42496, time: 0.008) nll: 83.529266 \n",
      "(GPU: 0, epoch: 1, iters: 43296, time: 0.008) nll: 65.473335 \n",
      "(GPU: 0, epoch: 1, iters: 44096, time: 0.008) nll: 39.984581 \n",
      "(GPU: 0, epoch: 1, iters: 44896, time: 0.008) nll: 54.019760 \n",
      "(GPU: 0, epoch: 1, iters: 45696, time: 0.008) nll: 73.671806 \n",
      "(GPU: 0, epoch: 1, iters: 46496, time: 0.008) nll: 69.647369 \n",
      "(GPU: 0, epoch: 1, iters: 47296, time: 0.008) nll: 77.595139 \n",
      "(GPU: 0, epoch: 1, iters: 48096, time: 0.008) nll: 66.359138 \n",
      "(GPU: 0, epoch: 1, iters: 48896, time: 0.008) nll: 54.492233 \n",
      "(GPU: 0, epoch: 1, iters: 49696, time: 0.008) nll: 71.815475 \n",
      "(GPU: 0, epoch: 1, iters: 50496, time: 0.008) nll: 70.681267 \n",
      "(GPU: 0, epoch: 1, iters: 51296, time: 0.008) nll: 41.884422 \n",
      "(GPU: 0, epoch: 1, iters: 51296, time: 0.013) nll: 41.347336 \n",
      "(GPU: 0, epoch: 1, iters: 51296, time: 0.013) nll: 55.318611 \n",
      "(GPU: 0, epoch: 1, iters: 52096, time: 0.008) nll: 76.301224 \n",
      "(GPU: 0, epoch: 1, iters: 52896, time: 0.008) nll: 74.390823 \n",
      "(GPU: 0, epoch: 1, iters: 53696, time: 0.008) nll: 80.690109 \n",
      "(GPU: 0, epoch: 1, iters: 54496, time: 0.008) nll: 92.870132 \n",
      "(GPU: 0, epoch: 1, iters: 55296, time: 0.008) nll: 72.586365 \n",
      "(GPU: 0, epoch: 1, iters: 56096, time: 0.008) nll: 80.835724 \n",
      "(GPU: 0, epoch: 1, iters: 56896, time: 0.008) nll: 74.208084 \n",
      "(GPU: 0, epoch: 1, iters: 57696, time: 0.008) nll: 58.118538 \n",
      "(GPU: 0, epoch: 1, iters: 58496, time: 0.008) nll: 62.654663 \n",
      "(GPU: 0, epoch: 1, iters: 59296, time: 0.008) nll: 60.707863 \n",
      "saving the latest model (epoch 1, total_steps 200000)\n",
      "(GPU: 0, epoch: 1, iters: 60096, time: 0.008) nll: 45.999443 \n",
      "(GPU: 0, epoch: 1, iters: 60896, time: 0.008) nll: 58.333321 \n",
      "(GPU: 0, epoch: 1, iters: 61696, time: 0.008) nll: 63.741741 \n",
      "(GPU: 0, epoch: 1, iters: 62496, time: 0.008) nll: 65.451309 \n",
      "(GPU: 0, epoch: 1, iters: 63296, time: 0.008) nll: 85.193756 \n",
      "(GPU: 0, epoch: 1, iters: 64096, time: 0.008) nll: 86.258644 \n",
      "(GPU: 0, epoch: 1, iters: 64896, time: 0.008) nll: 58.841541 \n",
      "(GPU: 0, epoch: 1, iters: 65696, time: 0.008) nll: 49.797127 \n",
      "(GPU: 0, epoch: 1, iters: 66496, time: 0.008) nll: 63.330009 \n",
      "(GPU: 0, epoch: 1, iters: 67296, time: 0.008) nll: 80.675232 \n",
      "(GPU: 0, epoch: 1, iters: 68096, time: 0.008) nll: 89.528908 \n",
      "(GPU: 0, epoch: 1, iters: 68896, time: 0.008) nll: 57.829231 \n",
      "(GPU: 0, epoch: 1, iters: 69696, time: 0.008) nll: 85.544006 \n",
      "(GPU: 0, epoch: 1, iters: 70496, time: 0.008) nll: 86.382881 \n",
      "(GPU: 0, epoch: 1, iters: 71296, time: 0.008) nll: 61.257359 \n",
      "(GPU: 0, epoch: 1, iters: 72096, time: 0.008) nll: 73.563553 \n",
      "(GPU: 0, epoch: 1, iters: 72896, time: 0.008) nll: 70.854385 \n",
      "(GPU: 0, epoch: 1, iters: 73696, time: 0.008) nll: 80.889465 \n",
      "(GPU: 0, epoch: 1, iters: 74496, time: 0.008) nll: 70.285103 \n",
      "(GPU: 0, epoch: 1, iters: 75296, time: 0.008) nll: 64.489944 \n",
      "(GPU: 0, epoch: 1, iters: 76096, time: 0.008) nll: 66.863190 \n",
      "(GPU: 0, epoch: 1, iters: 76896, time: 0.008) nll: 54.659306 \n",
      "(GPU: 0, epoch: 1, iters: 77696, time: 0.008) nll: 54.356911 \n",
      "(GPU: 0, epoch: 1, iters: 78496, time: 0.008) nll: 73.534058 \n",
      "(GPU: 0, epoch: 1, iters: 79296, time: 0.008) nll: 56.463825 \n",
      "saving the latest model (epoch 1, total_steps 220000)\n",
      "(GPU: 0, epoch: 1, iters: 80096, time: 0.008) nll: 64.575607 \n",
      "(GPU: 0, epoch: 1, iters: 80896, time: 0.008) nll: 61.981987 \n",
      "(GPU: 0, epoch: 1, iters: 81696, time: 0.008) nll: 72.935623 \n",
      "(GPU: 0, epoch: 1, iters: 82496, time: 0.008) nll: 87.004639 \n",
      "(GPU: 0, epoch: 1, iters: 83296, time: 0.008) nll: 67.429626 \n",
      "(GPU: 0, epoch: 1, iters: 84096, time: 0.008) nll: 64.766197 \n",
      "(GPU: 0, epoch: 1, iters: 84896, time: 0.008) nll: 38.493927 \n",
      "(GPU: 0, epoch: 1, iters: 85696, time: 0.008) nll: 90.139206 \n",
      "(GPU: 0, epoch: 1, iters: 86496, time: 0.008) nll: 86.030380 \n",
      "(GPU: 0, epoch: 1, iters: 87296, time: 0.008) nll: 50.295330 \n",
      "(GPU: 0, epoch: 1, iters: 88096, time: 0.008) nll: 74.864601 \n",
      "(GPU: 0, epoch: 1, iters: 88896, time: 0.008) nll: 68.688049 \n",
      "(GPU: 0, epoch: 1, iters: 89696, time: 0.007) nll: 68.718475 \n",
      "(GPU: 0, epoch: 1, iters: 90496, time: 0.008) nll: 54.984474 \n",
      "(GPU: 0, epoch: 1, iters: 91296, time: 0.008) nll: 69.794609 \n",
      "(GPU: 0, epoch: 1, iters: 92096, time: 0.008) nll: 60.753922 \n",
      "(GPU: 0, epoch: 1, iters: 92896, time: 0.008) nll: 63.082325 \n",
      "(GPU: 0, epoch: 1, iters: 93696, time: 0.008) nll: 56.329830 \n",
      "(GPU: 0, epoch: 1, iters: 94496, time: 0.008) nll: 75.103683 \n",
      "(GPU: 0, epoch: 1, iters: 95296, time: 0.008) nll: 66.347183 \n",
      "(GPU: 0, epoch: 1, iters: 96096, time: 0.008) nll: 70.892136 \n",
      "(GPU: 0, epoch: 1, iters: 96896, time: 0.008) nll: 69.310661 \n",
      "(GPU: 0, epoch: 1, iters: 97696, time: 0.008) nll: 64.110779 \n",
      "(GPU: 0, epoch: 1, iters: 98496, time: 0.008) nll: 74.217392 \n",
      "(GPU: 0, epoch: 1, iters: 99296, time: 0.008) nll: 60.291595 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4397/4397 [20:46<00:00,  3.53it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving the latest model (epoch 1, total_steps 240000)\n",
      "(GPU: 0, epoch: 1, iters: 100096, time: 0.008) nll: 48.882057 \n",
      "(GPU: 0, epoch: 1, iters: 100896, time: 0.008) nll: 62.260231 \n",
      "(GPU: 0, epoch: 1, iters: 101696, time: 0.008) nll: 77.994095 \n",
      "(GPU: 0, epoch: 1, iters: 102496, time: 0.008) nll: 32.336864 \n",
      "(GPU: 0, epoch: 1, iters: 103296, time: 0.008) nll: 82.427872 \n",
      "(GPU: 0, epoch: 1, iters: 104096, time: 0.008) nll: 56.720901 \n",
      "(GPU: 0, epoch: 1, iters: 104896, time: 0.008) nll: 58.594833 \n",
      "(GPU: 0, epoch: 1, iters: 105696, time: 0.008) nll: 65.430534 \n",
      "(GPU: 0, epoch: 1, iters: 106496, time: 0.008) nll: 63.279064 \n",
      "(GPU: 0, epoch: 1, iters: 107296, time: 0.008) nll: 66.804337 \n",
      "(GPU: 0, epoch: 1, iters: 108096, time: 0.008) nll: 87.372459 \n",
      "(GPU: 0, epoch: 1, iters: 108896, time: 0.008) nll: 82.436241 \n",
      "(GPU: 0, epoch: 1, iters: 109696, time: 0.008) nll: 87.793365 \n",
      "(GPU: 0, epoch: 1, iters: 110496, time: 0.008) nll: 89.045273 \n",
      "(GPU: 0, epoch: 1, iters: 111296, time: 0.008) nll: 63.429909 \n",
      "(GPU: 0, epoch: 1, iters: 112096, time: 0.008) nll: 73.954575 \n",
      "(GPU: 0, epoch: 1, iters: 112896, time: 0.008) nll: 46.016617 \n",
      "(GPU: 0, epoch: 1, iters: 113696, time: 0.008) nll: 49.656876 \n",
      "(GPU: 0, epoch: 1, iters: 114496, time: 0.008) nll: 80.143120 \n",
      "(GPU: 0, epoch: 1, iters: 115296, time: 0.008) nll: 68.925232 \n",
      "(GPU: 0, epoch: 1, iters: 116096, time: 0.008) nll: 64.902626 \n",
      "(GPU: 0, epoch: 1, iters: 116896, time: 0.008) nll: 64.855072 \n",
      "(GPU: 0, epoch: 1, iters: 117696, time: 0.008) nll: 88.050583 \n",
      "(GPU: 0, epoch: 1, iters: 118496, time: 0.008) nll: 60.407089 \n",
      "(GPU: 0, epoch: 1, iters: 119296, time: 0.008) nll: 92.881660 \n",
      "saving the latest model (epoch 1, total_steps 260000)\n",
      "(GPU: 0, epoch: 1, iters: 120096, time: 0.008) nll: 81.858833 \n",
      "(GPU: 0, epoch: 1, iters: 120896, time: 0.008) nll: 68.694260 \n",
      "(GPU: 0, epoch: 1, iters: 121696, time: 0.008) nll: 73.328255 \n",
      "(GPU: 0, epoch: 1, iters: 122496, time: 0.008) nll: 77.870102 \n",
      "(GPU: 0, epoch: 1, iters: 123296, time: 0.008) nll: 61.761482 \n",
      "(GPU: 0, epoch: 1, iters: 124096, time: 0.008) nll: 53.829918 \n",
      "(GPU: 0, epoch: 1, iters: 124896, time: 0.008) nll: 91.292076 \n",
      "(GPU: 0, epoch: 1, iters: 125696, time: 0.009) nll: 63.325356 \n",
      "(GPU: 0, epoch: 1, iters: 126496, time: 0.008) nll: 50.933338 \n",
      "(GPU: 0, epoch: 1, iters: 127296, time: 0.008) nll: 65.379425 \n",
      "(GPU: 0, epoch: 1, iters: 128096, time: 0.008) nll: 59.708385 \n",
      "(GPU: 0, epoch: 1, iters: 128896, time: 0.008) nll: 85.169556 \n",
      "(GPU: 0, epoch: 1, iters: 129696, time: 0.008) nll: 51.376053 \n",
      "(GPU: 0, epoch: 1, iters: 130496, time: 0.008) nll: 77.985199 \n",
      "(GPU: 0, epoch: 1, iters: 131296, time: 0.008) nll: 69.006783 \n",
      "(GPU: 0, epoch: 1, iters: 132096, time: 0.008) nll: 77.272079 \n",
      "(GPU: 0, epoch: 1, iters: 132896, time: 0.008) nll: 41.847683 \n",
      "(GPU: 0, epoch: 1, iters: 133696, time: 0.008) nll: 93.960342 \n",
      "(GPU: 0, epoch: 1, iters: 134496, time: 0.008) nll: 72.784019 \n",
      "(GPU: 0, epoch: 1, iters: 135296, time: 0.008) nll: 68.983650 \n",
      "(GPU: 0, epoch: 1, iters: 136096, time: 0.008) nll: 47.368771 \n",
      "(GPU: 0, epoch: 1, iters: 136896, time: 0.008) nll: 74.219727 \n",
      "(GPU: 0, epoch: 1, iters: 137696, time: 0.008) nll: 68.427780 \n",
      "(GPU: 0, epoch: 1, iters: 138496, time: 0.008) nll: 70.816063 \n",
      "(GPU: 0, epoch: 1, iters: 139296, time: 0.008) nll: 75.272522 \n",
      "saving the latest model (epoch 1, total_steps 280000)\n",
      "(GPU: 0, epoch: 1, iters: 140096, time: 0.008) nll: 61.345509 \n",
      "[*] End of epoch 1 / 25 \t Time Taken: 1247 sec \n",
      "/cluster/54/streakfull/ADL4CV/Project/src/plz-autosdf/logs/bert2vqsc_v4-text2shape-seq-LR1e-4-network-bert-shapeset-shapeset-SmoothL1Loss\n",
      "[*] learning rate = 0.0000200\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 3105/4397 [14:41<05:38,  3.82it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 2, iters: 32, time: 0.004) nll: 91.081055 \n",
      "(GPU: 0, epoch: 2, iters: 32, time: 0.004) nll: 47.651939 \n",
      "(GPU: 0, epoch: 2, iters: 192, time: 0.008) nll: 66.061737 \n",
      "(GPU: 0, epoch: 2, iters: 992, time: 0.007) nll: 74.643707 \n",
      "(GPU: 0, epoch: 2, iters: 1792, time: 0.008) nll: 55.211941 \n",
      "(GPU: 0, epoch: 2, iters: 2592, time: 0.008) nll: 64.628067 \n",
      "(GPU: 0, epoch: 2, iters: 3392, time: 0.008) nll: 50.663307 \n",
      "(GPU: 0, epoch: 2, iters: 4192, time: 0.008) nll: 56.986721 \n",
      "(GPU: 0, epoch: 2, iters: 4992, time: 0.008) nll: 56.125755 \n",
      "(GPU: 0, epoch: 2, iters: 5792, time: 0.008) nll: 52.209854 \n",
      "(GPU: 0, epoch: 2, iters: 6592, time: 0.008) nll: 92.744652 \n",
      "(GPU: 0, epoch: 2, iters: 6592, time: 0.013) nll: 91.571960 \n",
      "(GPU: 0, epoch: 2, iters: 6592, time: 0.013) nll: 52.296486 \n",
      "(GPU: 0, epoch: 2, iters: 7392, time: 0.008) nll: 60.979210 \n",
      "(GPU: 0, epoch: 2, iters: 8192, time: 0.008) nll: 81.071320 \n",
      "(GPU: 0, epoch: 2, iters: 8992, time: 0.008) nll: 99.249542 \n",
      "(GPU: 0, epoch: 2, iters: 9792, time: 0.008) nll: 61.984127 \n",
      "(GPU: 0, epoch: 2, iters: 10592, time: 0.008) nll: 84.972961 \n",
      "(GPU: 0, epoch: 2, iters: 11392, time: 0.008) nll: 54.607929 \n",
      "(GPU: 0, epoch: 2, iters: 12192, time: 0.008) nll: 65.693443 \n",
      "(GPU: 0, epoch: 2, iters: 12992, time: 0.008) nll: 75.825562 \n",
      "(GPU: 0, epoch: 2, iters: 13792, time: 0.008) nll: 96.224960 \n",
      "(GPU: 0, epoch: 2, iters: 14592, time: 0.008) nll: 72.537933 \n",
      "(GPU: 0, epoch: 2, iters: 15392, time: 0.008) nll: 92.003189 \n",
      "(GPU: 0, epoch: 2, iters: 16192, time: 0.008) nll: 75.730698 \n",
      "(GPU: 0, epoch: 2, iters: 16992, time: 0.008) nll: 103.937630 \n",
      "(GPU: 0, epoch: 2, iters: 17792, time: 0.008) nll: 70.648300 \n",
      "(GPU: 0, epoch: 2, iters: 18592, time: 0.008) nll: 77.113556 \n",
      "saving the latest model (epoch 2, total_steps 300000)\n",
      "(GPU: 0, epoch: 2, iters: 19392, time: 0.008) nll: 83.372894 \n",
      "(GPU: 0, epoch: 2, iters: 20192, time: 0.008) nll: 67.535881 \n",
      "(GPU: 0, epoch: 2, iters: 20992, time: 0.008) nll: 62.383385 \n",
      "(GPU: 0, epoch: 2, iters: 21792, time: 0.008) nll: 63.445206 \n",
      "(GPU: 0, epoch: 2, iters: 22592, time: 0.008) nll: 71.193420 \n",
      "(GPU: 0, epoch: 2, iters: 23392, time: 0.008) nll: 46.046139 \n",
      "(GPU: 0, epoch: 2, iters: 24192, time: 0.008) nll: 76.129608 \n",
      "(GPU: 0, epoch: 2, iters: 24992, time: 0.008) nll: 77.358734 \n",
      "(GPU: 0, epoch: 2, iters: 25792, time: 0.008) nll: 88.560364 \n",
      "(GPU: 0, epoch: 2, iters: 26592, time: 0.008) nll: 65.770676 \n",
      "(GPU: 0, epoch: 2, iters: 27392, time: 0.008) nll: 64.949387 \n",
      "(GPU: 0, epoch: 2, iters: 28192, time: 0.008) nll: 65.693253 \n",
      "(GPU: 0, epoch: 2, iters: 28992, time: 0.008) nll: 48.787987 \n",
      "(GPU: 0, epoch: 2, iters: 29792, time: 0.008) nll: 63.753036 \n",
      "(GPU: 0, epoch: 2, iters: 30592, time: 0.008) nll: 68.805740 \n",
      "(GPU: 0, epoch: 2, iters: 31392, time: 0.008) nll: 74.754303 \n",
      "(GPU: 0, epoch: 2, iters: 32192, time: 0.008) nll: 63.739220 \n",
      "(GPU: 0, epoch: 2, iters: 32992, time: 0.008) nll: 55.681160 \n",
      "(GPU: 0, epoch: 2, iters: 33792, time: 0.008) nll: 76.437088 \n",
      "(GPU: 0, epoch: 2, iters: 34592, time: 0.008) nll: 65.708702 \n",
      "(GPU: 0, epoch: 2, iters: 35392, time: 0.008) nll: 81.263992 \n",
      "(GPU: 0, epoch: 2, iters: 36192, time: 0.008) nll: 89.158020 \n",
      "(GPU: 0, epoch: 2, iters: 36992, time: 0.008) nll: 87.593185 \n",
      "(GPU: 0, epoch: 2, iters: 37792, time: 0.008) nll: 45.691467 \n",
      "(GPU: 0, epoch: 2, iters: 38592, time: 0.008) nll: 46.365547 \n",
      "saving the latest model (epoch 2, total_steps 320000)\n",
      "(GPU: 0, epoch: 2, iters: 39392, time: 0.008) nll: 89.766876 \n",
      "(GPU: 0, epoch: 2, iters: 40192, time: 0.008) nll: 42.387398 \n",
      "(GPU: 0, epoch: 2, iters: 40992, time: 0.008) nll: 65.124252 \n",
      "(GPU: 0, epoch: 2, iters: 41792, time: 0.008) nll: 102.974655 \n",
      "(GPU: 0, epoch: 2, iters: 42592, time: 0.008) nll: 94.593834 \n",
      "(GPU: 0, epoch: 2, iters: 43392, time: 0.008) nll: 81.223816 \n",
      "(GPU: 0, epoch: 2, iters: 44192, time: 0.008) nll: 87.452522 \n",
      "(GPU: 0, epoch: 2, iters: 44992, time: 0.008) nll: 79.737122 \n",
      "(GPU: 0, epoch: 2, iters: 45792, time: 0.008) nll: 59.649277 \n",
      "(GPU: 0, epoch: 2, iters: 46592, time: 0.008) nll: 57.822208 \n",
      "(GPU: 0, epoch: 2, iters: 47392, time: 0.008) nll: 64.746246 \n",
      "(GPU: 0, epoch: 2, iters: 48192, time: 0.008) nll: 61.421318 \n",
      "(GPU: 0, epoch: 2, iters: 48992, time: 0.008) nll: 66.472565 \n",
      "(GPU: 0, epoch: 2, iters: 49792, time: 0.008) nll: 63.773212 \n",
      "(GPU: 0, epoch: 2, iters: 50592, time: 0.008) nll: 76.605698 \n",
      "(GPU: 0, epoch: 2, iters: 51392, time: 0.008) nll: 68.701515 \n",
      "(GPU: 0, epoch: 2, iters: 52192, time: 0.007) nll: 69.449318 \n",
      "(GPU: 0, epoch: 2, iters: 52992, time: 0.008) nll: 86.774231 \n",
      "(GPU: 0, epoch: 2, iters: 53792, time: 0.008) nll: 64.730103 \n",
      "(GPU: 0, epoch: 2, iters: 54592, time: 0.008) nll: 58.729843 \n",
      "(GPU: 0, epoch: 2, iters: 55392, time: 0.008) nll: 91.770561 \n",
      "(GPU: 0, epoch: 2, iters: 56192, time: 0.008) nll: 77.592468 \n",
      "(GPU: 0, epoch: 2, iters: 56992, time: 0.008) nll: 89.575516 \n",
      "(GPU: 0, epoch: 2, iters: 57792, time: 0.008) nll: 62.300575 \n",
      "(GPU: 0, epoch: 2, iters: 58592, time: 0.008) nll: 52.172905 \n",
      "saving the latest model (epoch 2, total_steps 340000)\n",
      "(GPU: 0, epoch: 2, iters: 59392, time: 0.008) nll: 83.264732 \n",
      "(GPU: 0, epoch: 2, iters: 60192, time: 0.008) nll: 57.314278 \n",
      "(GPU: 0, epoch: 2, iters: 60992, time: 0.008) nll: 85.161316 \n",
      "(GPU: 0, epoch: 2, iters: 61792, time: 0.008) nll: 55.356300 \n",
      "(GPU: 0, epoch: 2, iters: 62592, time: 0.008) nll: 67.966919 \n",
      "(GPU: 0, epoch: 2, iters: 63392, time: 0.008) nll: 69.718239 \n",
      "(GPU: 0, epoch: 2, iters: 64192, time: 0.008) nll: 90.809265 \n",
      "(GPU: 0, epoch: 2, iters: 64992, time: 0.008) nll: 74.654236 \n",
      "(GPU: 0, epoch: 2, iters: 65792, time: 0.008) nll: 58.232677 \n",
      "(GPU: 0, epoch: 2, iters: 66592, time: 0.008) nll: 100.730606 \n",
      "(GPU: 0, epoch: 2, iters: 67392, time: 0.008) nll: 91.232635 \n",
      "(GPU: 0, epoch: 2, iters: 68192, time: 0.008) nll: 71.811188 \n",
      "(GPU: 0, epoch: 2, iters: 68992, time: 0.008) nll: 73.647278 \n",
      "(GPU: 0, epoch: 2, iters: 69792, time: 0.008) nll: 69.631958 \n",
      "(GPU: 0, epoch: 2, iters: 70592, time: 0.008) nll: 75.467545 \n",
      "(GPU: 0, epoch: 2, iters: 71392, time: 0.008) nll: 58.248528 \n",
      "(GPU: 0, epoch: 2, iters: 72192, time: 0.008) nll: 84.421844 \n",
      "(GPU: 0, epoch: 2, iters: 72992, time: 0.008) nll: 67.451927 \n",
      "(GPU: 0, epoch: 2, iters: 73792, time: 0.008) nll: 78.308487 \n",
      "(GPU: 0, epoch: 2, iters: 74592, time: 0.008) nll: 60.717155 \n",
      "(GPU: 0, epoch: 2, iters: 75392, time: 0.008) nll: 79.648331 \n",
      "(GPU: 0, epoch: 2, iters: 76192, time: 0.008) nll: 58.685051 \n",
      "(GPU: 0, epoch: 2, iters: 76992, time: 0.008) nll: 84.703087 \n",
      "(GPU: 0, epoch: 2, iters: 77792, time: 0.008) nll: 51.790779 \n",
      "(GPU: 0, epoch: 2, iters: 78592, time: 0.008) nll: 60.200142 \n",
      "saving the latest model (epoch 2, total_steps 360000)\n",
      "(GPU: 0, epoch: 2, iters: 79392, time: 0.008) nll: 69.312683 \n",
      "(GPU: 0, epoch: 2, iters: 80192, time: 0.008) nll: 63.305824 \n",
      "(GPU: 0, epoch: 2, iters: 80992, time: 0.008) nll: 80.195831 \n",
      "(GPU: 0, epoch: 2, iters: 81792, time: 0.008) nll: 67.002174 \n",
      "(GPU: 0, epoch: 2, iters: 82592, time: 0.008) nll: 53.044800 \n",
      "(GPU: 0, epoch: 2, iters: 83392, time: 0.008) nll: 88.995071 \n",
      "(GPU: 0, epoch: 2, iters: 84192, time: 0.008) nll: 61.027912 \n",
      "(GPU: 0, epoch: 2, iters: 84992, time: 0.008) nll: 79.688354 \n",
      "(GPU: 0, epoch: 2, iters: 85792, time: 0.008) nll: 52.969196 \n",
      "(GPU: 0, epoch: 2, iters: 86592, time: 0.008) nll: 80.324539 \n",
      "(GPU: 0, epoch: 2, iters: 87392, time: 0.007) nll: 58.295746 \n",
      "(GPU: 0, epoch: 2, iters: 88192, time: 0.008) nll: 68.747711 \n",
      "(GPU: 0, epoch: 2, iters: 88992, time: 0.008) nll: 66.682053 \n",
      "(GPU: 0, epoch: 2, iters: 89792, time: 0.008) nll: 64.386932 \n",
      "(GPU: 0, epoch: 2, iters: 90592, time: 0.008) nll: 103.491676 \n",
      "(GPU: 0, epoch: 2, iters: 91392, time: 0.008) nll: 58.720158 \n",
      "(GPU: 0, epoch: 2, iters: 92192, time: 0.008) nll: 82.609329 \n",
      "(GPU: 0, epoch: 2, iters: 92992, time: 0.008) nll: 64.125427 \n",
      "(GPU: 0, epoch: 2, iters: 93792, time: 0.008) nll: 81.426529 \n",
      "(GPU: 0, epoch: 2, iters: 94592, time: 0.008) nll: 75.634445 \n",
      "(GPU: 0, epoch: 2, iters: 95392, time: 0.008) nll: 57.304752 \n",
      "(GPU: 0, epoch: 2, iters: 96192, time: 0.008) nll: 56.764427 \n",
      "(GPU: 0, epoch: 2, iters: 96992, time: 0.008) nll: 67.603333 \n",
      "(GPU: 0, epoch: 2, iters: 97792, time: 0.008) nll: 60.654713 \n",
      "(GPU: 0, epoch: 2, iters: 98592, time: 0.008) nll: 56.041569 \n",
      "saving the latest model (epoch 2, total_steps 380000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4397/4397 [20:47<00:00,  3.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 2, iters: 99392, time: 0.008) nll: 64.449989 \n",
      "(GPU: 0, epoch: 2, iters: 100192, time: 0.008) nll: 71.561478 \n",
      "(GPU: 0, epoch: 2, iters: 100992, time: 0.008) nll: 66.568665 \n",
      "(GPU: 0, epoch: 2, iters: 101792, time: 0.008) nll: 86.675262 \n",
      "(GPU: 0, epoch: 2, iters: 102592, time: 0.008) nll: 74.198090 \n",
      "(GPU: 0, epoch: 2, iters: 102592, time: 0.013) nll: 73.668930 \n",
      "(GPU: 0, epoch: 2, iters: 102592, time: 0.013) nll: 75.448257 \n",
      "(GPU: 0, epoch: 2, iters: 103392, time: 0.008) nll: 46.704048 \n",
      "(GPU: 0, epoch: 2, iters: 104192, time: 0.008) nll: 51.112473 \n",
      "(GPU: 0, epoch: 2, iters: 104992, time: 0.008) nll: 72.658272 \n",
      "(GPU: 0, epoch: 2, iters: 105792, time: 0.008) nll: 62.617805 \n",
      "(GPU: 0, epoch: 2, iters: 106592, time: 0.008) nll: 74.267982 \n",
      "(GPU: 0, epoch: 2, iters: 107392, time: 0.008) nll: 88.232971 \n",
      "(GPU: 0, epoch: 2, iters: 108192, time: 0.008) nll: 80.012955 \n",
      "(GPU: 0, epoch: 2, iters: 108992, time: 0.008) nll: 58.090862 \n",
      "(GPU: 0, epoch: 2, iters: 109792, time: 0.008) nll: 80.282242 \n",
      "(GPU: 0, epoch: 2, iters: 110592, time: 0.008) nll: 68.994354 \n",
      "(GPU: 0, epoch: 2, iters: 111392, time: 0.008) nll: 79.509979 \n",
      "(GPU: 0, epoch: 2, iters: 112192, time: 0.008) nll: 82.374817 \n",
      "(GPU: 0, epoch: 2, iters: 112992, time: 0.008) nll: 74.418091 \n",
      "(GPU: 0, epoch: 2, iters: 113792, time: 0.008) nll: 46.251144 \n",
      "(GPU: 0, epoch: 2, iters: 114592, time: 0.008) nll: 90.448959 \n",
      "(GPU: 0, epoch: 2, iters: 115392, time: 0.008) nll: 78.056145 \n",
      "(GPU: 0, epoch: 2, iters: 116192, time: 0.008) nll: 81.472824 \n",
      "(GPU: 0, epoch: 2, iters: 116992, time: 0.008) nll: 72.893867 \n",
      "(GPU: 0, epoch: 2, iters: 117792, time: 0.008) nll: 79.233704 \n",
      "(GPU: 0, epoch: 2, iters: 118592, time: 0.008) nll: 57.742088 \n",
      "saving the latest model (epoch 2, total_steps 400000)\n",
      "(GPU: 0, epoch: 2, iters: 119392, time: 0.008) nll: 60.640060 \n",
      "(GPU: 0, epoch: 2, iters: 120192, time: 0.008) nll: 76.224571 \n",
      "(GPU: 0, epoch: 2, iters: 120992, time: 0.008) nll: 58.226799 \n",
      "(GPU: 0, epoch: 2, iters: 121792, time: 0.008) nll: 85.046104 \n",
      "(GPU: 0, epoch: 2, iters: 122592, time: 0.008) nll: 70.383934 \n",
      "(GPU: 0, epoch: 2, iters: 123392, time: 0.008) nll: 81.673401 \n",
      "(GPU: 0, epoch: 2, iters: 124192, time: 0.008) nll: 82.474899 \n",
      "(GPU: 0, epoch: 2, iters: 124992, time: 0.008) nll: 66.707870 \n",
      "(GPU: 0, epoch: 2, iters: 125792, time: 0.008) nll: 66.021042 \n",
      "(GPU: 0, epoch: 2, iters: 126592, time: 0.008) nll: 53.744461 \n",
      "(GPU: 0, epoch: 2, iters: 127392, time: 0.008) nll: 63.751003 \n",
      "(GPU: 0, epoch: 2, iters: 128192, time: 0.008) nll: 58.894257 \n",
      "(GPU: 0, epoch: 2, iters: 128992, time: 0.008) nll: 88.048355 \n",
      "(GPU: 0, epoch: 2, iters: 129792, time: 0.008) nll: 89.132652 \n",
      "(GPU: 0, epoch: 2, iters: 130592, time: 0.008) nll: 62.374168 \n",
      "(GPU: 0, epoch: 2, iters: 131392, time: 0.008) nll: 48.382713 \n",
      "(GPU: 0, epoch: 2, iters: 132192, time: 0.008) nll: 88.453346 \n",
      "(GPU: 0, epoch: 2, iters: 132992, time: 0.008) nll: 72.145119 \n",
      "(GPU: 0, epoch: 2, iters: 133792, time: 0.008) nll: 77.506172 \n",
      "(GPU: 0, epoch: 2, iters: 134592, time: 0.008) nll: 87.705330 \n",
      "(GPU: 0, epoch: 2, iters: 135392, time: 0.007) nll: 77.447952 \n",
      "(GPU: 0, epoch: 2, iters: 136192, time: 0.008) nll: 77.347366 \n",
      "(GPU: 0, epoch: 2, iters: 136992, time: 0.008) nll: 74.261322 \n",
      "(GPU: 0, epoch: 2, iters: 137792, time: 0.008) nll: 83.093765 \n",
      "(GPU: 0, epoch: 2, iters: 138592, time: 0.008) nll: 41.343597 \n",
      "saving the latest model (epoch 2, total_steps 420000)\n",
      "(GPU: 0, epoch: 2, iters: 139392, time: 0.008) nll: 48.573875 \n",
      "(GPU: 0, epoch: 2, iters: 140192, time: 0.008) nll: 54.525845 \n",
      "[*] End of epoch 2 / 25 \t Time Taken: 1248 sec \n",
      "/cluster/54/streakfull/ADL4CV/Project/src/plz-autosdf/logs/bert2vqsc_v4-text2shape-seq-LR1e-4-network-bert-shapeset-shapeset-SmoothL1Loss\n",
      "[*] learning rate = 0.0000300\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 3108/4397 [14:41<05:39,  3.80it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 3, iters: 32, time: 0.004) nll: 91.330818 \n",
      "(GPU: 0, epoch: 3, iters: 32, time: 0.004) nll: 76.922668 \n",
      "(GPU: 0, epoch: 3, iters: 288, time: 0.008) nll: 69.573532 \n",
      "(GPU: 0, epoch: 3, iters: 1088, time: 0.008) nll: 62.229069 \n",
      "(GPU: 0, epoch: 3, iters: 1888, time: 0.008) nll: 85.142311 \n",
      "(GPU: 0, epoch: 3, iters: 2688, time: 0.008) nll: 85.690887 \n",
      "(GPU: 0, epoch: 3, iters: 3488, time: 0.008) nll: 67.331604 \n",
      "(GPU: 0, epoch: 3, iters: 4288, time: 0.008) nll: 71.026566 \n",
      "(GPU: 0, epoch: 3, iters: 5088, time: 0.008) nll: 67.405945 \n",
      "(GPU: 0, epoch: 3, iters: 5888, time: 0.008) nll: 56.170303 \n",
      "(GPU: 0, epoch: 3, iters: 6688, time: 0.008) nll: 79.439636 \n",
      "(GPU: 0, epoch: 3, iters: 7488, time: 0.008) nll: 71.357346 \n",
      "(GPU: 0, epoch: 3, iters: 8288, time: 0.008) nll: 45.649319 \n",
      "(GPU: 0, epoch: 3, iters: 9088, time: 0.008) nll: 89.834686 \n",
      "(GPU: 0, epoch: 3, iters: 9888, time: 0.008) nll: 44.138084 \n",
      "(GPU: 0, epoch: 3, iters: 10688, time: 0.008) nll: 70.468658 \n",
      "(GPU: 0, epoch: 3, iters: 11488, time: 0.008) nll: 79.510117 \n",
      "(GPU: 0, epoch: 3, iters: 12288, time: 0.008) nll: 60.278793 \n",
      "(GPU: 0, epoch: 3, iters: 13088, time: 0.008) nll: 56.315960 \n",
      "(GPU: 0, epoch: 3, iters: 13888, time: 0.008) nll: 63.424164 \n",
      "(GPU: 0, epoch: 3, iters: 14688, time: 0.008) nll: 63.443401 \n",
      "(GPU: 0, epoch: 3, iters: 15488, time: 0.008) nll: 54.923412 \n",
      "(GPU: 0, epoch: 3, iters: 16288, time: 0.008) nll: 101.947891 \n",
      "(GPU: 0, epoch: 3, iters: 17088, time: 0.008) nll: 69.432442 \n",
      "(GPU: 0, epoch: 3, iters: 17888, time: 0.008) nll: 59.652161 \n",
      "saving the latest model (epoch 3, total_steps 440000)\n",
      "(GPU: 0, epoch: 3, iters: 18688, time: 0.008) nll: 79.928307 \n",
      "(GPU: 0, epoch: 3, iters: 19488, time: 0.008) nll: 92.614677 \n",
      "(GPU: 0, epoch: 3, iters: 20288, time: 0.008) nll: 72.840561 \n",
      "(GPU: 0, epoch: 3, iters: 21088, time: 0.008) nll: 49.862358 \n",
      "(GPU: 0, epoch: 3, iters: 21888, time: 0.008) nll: 80.301178 \n",
      "(GPU: 0, epoch: 3, iters: 22688, time: 0.008) nll: 73.374130 \n",
      "(GPU: 0, epoch: 3, iters: 23488, time: 0.008) nll: 60.386940 \n",
      "(GPU: 0, epoch: 3, iters: 24288, time: 0.008) nll: 66.098999 \n",
      "(GPU: 0, epoch: 3, iters: 25088, time: 0.008) nll: 67.531631 \n",
      "(GPU: 0, epoch: 3, iters: 25888, time: 0.008) nll: 101.596092 \n",
      "(GPU: 0, epoch: 3, iters: 26688, time: 0.008) nll: 75.173431 \n",
      "(GPU: 0, epoch: 3, iters: 27488, time: 0.008) nll: 65.658218 \n",
      "(GPU: 0, epoch: 3, iters: 28288, time: 0.008) nll: 64.528557 \n",
      "(GPU: 0, epoch: 3, iters: 29088, time: 0.008) nll: 74.589401 \n",
      "(GPU: 0, epoch: 3, iters: 29888, time: 0.008) nll: 61.536510 \n",
      "(GPU: 0, epoch: 3, iters: 30688, time: 0.008) nll: 62.254154 \n",
      "(GPU: 0, epoch: 3, iters: 31488, time: 0.008) nll: 52.953663 \n",
      "(GPU: 0, epoch: 3, iters: 32288, time: 0.008) nll: 56.330956 \n",
      "(GPU: 0, epoch: 3, iters: 33088, time: 0.008) nll: 82.307709 \n",
      "(GPU: 0, epoch: 3, iters: 33888, time: 0.008) nll: 65.640488 \n",
      "(GPU: 0, epoch: 3, iters: 34688, time: 0.008) nll: 55.431641 \n",
      "(GPU: 0, epoch: 3, iters: 35488, time: 0.008) nll: 81.643440 \n",
      "(GPU: 0, epoch: 3, iters: 36288, time: 0.008) nll: 70.958038 \n",
      "(GPU: 0, epoch: 3, iters: 37088, time: 0.008) nll: 45.518806 \n",
      "(GPU: 0, epoch: 3, iters: 37888, time: 0.008) nll: 87.766113 \n",
      "saving the latest model (epoch 3, total_steps 460000)\n",
      "(GPU: 0, epoch: 3, iters: 38688, time: 0.008) nll: 87.524757 \n",
      "(GPU: 0, epoch: 3, iters: 39488, time: 0.008) nll: 74.261986 \n",
      "(GPU: 0, epoch: 3, iters: 40288, time: 0.008) nll: 77.293427 \n",
      "(GPU: 0, epoch: 3, iters: 41088, time: 0.008) nll: 85.608551 \n",
      "(GPU: 0, epoch: 3, iters: 41888, time: 0.008) nll: 75.351303 \n",
      "(GPU: 0, epoch: 3, iters: 42688, time: 0.008) nll: 82.878723 \n",
      "(GPU: 0, epoch: 3, iters: 43488, time: 0.008) nll: 60.148491 \n",
      "(GPU: 0, epoch: 3, iters: 44288, time: 0.008) nll: 80.124664 \n",
      "(GPU: 0, epoch: 3, iters: 45088, time: 0.008) nll: 73.507019 \n",
      "(GPU: 0, epoch: 3, iters: 45888, time: 0.008) nll: 82.261154 \n",
      "(GPU: 0, epoch: 3, iters: 46688, time: 0.007) nll: 56.328598 \n",
      "(GPU: 0, epoch: 3, iters: 47488, time: 0.008) nll: 73.587708 \n",
      "(GPU: 0, epoch: 3, iters: 48288, time: 0.008) nll: 69.370232 \n",
      "(GPU: 0, epoch: 3, iters: 49088, time: 0.008) nll: 68.195282 \n",
      "(GPU: 0, epoch: 3, iters: 49888, time: 0.008) nll: 81.255959 \n",
      "(GPU: 0, epoch: 3, iters: 50688, time: 0.008) nll: 67.938354 \n",
      "(GPU: 0, epoch: 3, iters: 51488, time: 0.008) nll: 99.179535 \n",
      "(GPU: 0, epoch: 3, iters: 52288, time: 0.008) nll: 56.193596 \n",
      "(GPU: 0, epoch: 3, iters: 53088, time: 0.008) nll: 71.580559 \n",
      "(GPU: 0, epoch: 3, iters: 53888, time: 0.008) nll: 68.338654 \n",
      "(GPU: 0, epoch: 3, iters: 54688, time: 0.008) nll: 64.194748 \n",
      "(GPU: 0, epoch: 3, iters: 55488, time: 0.008) nll: 64.516045 \n",
      "(GPU: 0, epoch: 3, iters: 56288, time: 0.008) nll: 58.406693 \n",
      "(GPU: 0, epoch: 3, iters: 57088, time: 0.008) nll: 74.846352 \n",
      "(GPU: 0, epoch: 3, iters: 57888, time: 0.007) nll: 78.462997 \n",
      "(GPU: 0, epoch: 3, iters: 57888, time: 0.012) nll: 77.086678 \n",
      "(GPU: 0, epoch: 3, iters: 57888, time: 0.012) nll: 63.869904 \n",
      "saving the latest model (epoch 3, total_steps 480000)\n",
      "(GPU: 0, epoch: 3, iters: 58688, time: 0.008) nll: 76.808548 \n",
      "(GPU: 0, epoch: 3, iters: 59488, time: 0.008) nll: 59.706280 \n",
      "(GPU: 0, epoch: 3, iters: 60288, time: 0.008) nll: 67.192749 \n",
      "(GPU: 0, epoch: 3, iters: 61088, time: 0.008) nll: 77.714615 \n",
      "(GPU: 0, epoch: 3, iters: 61888, time: 0.008) nll: 64.841637 \n",
      "(GPU: 0, epoch: 3, iters: 62688, time: 0.008) nll: 71.868172 \n",
      "(GPU: 0, epoch: 3, iters: 63488, time: 0.008) nll: 64.923538 \n",
      "(GPU: 0, epoch: 3, iters: 64288, time: 0.008) nll: 70.277321 \n",
      "(GPU: 0, epoch: 3, iters: 65088, time: 0.008) nll: 83.652512 \n",
      "(GPU: 0, epoch: 3, iters: 65888, time: 0.008) nll: 74.932716 \n",
      "(GPU: 0, epoch: 3, iters: 66688, time: 0.008) nll: 55.596237 \n",
      "(GPU: 0, epoch: 3, iters: 67488, time: 0.008) nll: 63.886330 \n",
      "(GPU: 0, epoch: 3, iters: 68288, time: 0.008) nll: 56.917450 \n",
      "(GPU: 0, epoch: 3, iters: 69088, time: 0.008) nll: 55.216591 \n",
      "(GPU: 0, epoch: 3, iters: 69888, time: 0.008) nll: 87.671272 \n",
      "(GPU: 0, epoch: 3, iters: 70688, time: 0.008) nll: 51.127327 \n",
      "(GPU: 0, epoch: 3, iters: 71488, time: 0.008) nll: 65.329102 \n",
      "(GPU: 0, epoch: 3, iters: 72288, time: 0.008) nll: 59.854870 \n",
      "(GPU: 0, epoch: 3, iters: 73088, time: 0.008) nll: 90.252007 \n",
      "(GPU: 0, epoch: 3, iters: 73888, time: 0.008) nll: 54.983200 \n",
      "(GPU: 0, epoch: 3, iters: 74688, time: 0.008) nll: 62.486256 \n",
      "(GPU: 0, epoch: 3, iters: 75488, time: 0.008) nll: 71.613785 \n",
      "(GPU: 0, epoch: 3, iters: 76288, time: 0.008) nll: 76.524590 \n",
      "(GPU: 0, epoch: 3, iters: 77088, time: 0.008) nll: 56.099651 \n",
      "(GPU: 0, epoch: 3, iters: 77888, time: 0.008) nll: 75.316437 \n",
      "saving the latest model (epoch 3, total_steps 500000)\n",
      "(GPU: 0, epoch: 3, iters: 78688, time: 0.008) nll: 62.527374 \n",
      "(GPU: 0, epoch: 3, iters: 79488, time: 0.008) nll: 67.289330 \n",
      "(GPU: 0, epoch: 3, iters: 80288, time: 0.008) nll: 62.221397 \n",
      "(GPU: 0, epoch: 3, iters: 81088, time: 0.008) nll: 84.728661 \n",
      "(GPU: 0, epoch: 3, iters: 81888, time: 0.008) nll: 59.654785 \n",
      "(GPU: 0, epoch: 3, iters: 82688, time: 0.008) nll: 75.762756 \n",
      "(GPU: 0, epoch: 3, iters: 83488, time: 0.008) nll: 119.779991 \n",
      "(GPU: 0, epoch: 3, iters: 84288, time: 0.008) nll: 79.441093 \n",
      "(GPU: 0, epoch: 3, iters: 85088, time: 0.008) nll: 46.052273 \n",
      "(GPU: 0, epoch: 3, iters: 85888, time: 0.008) nll: 69.623627 \n",
      "(GPU: 0, epoch: 3, iters: 86688, time: 0.008) nll: 75.238266 \n",
      "(GPU: 0, epoch: 3, iters: 87488, time: 0.008) nll: 74.927071 \n",
      "(GPU: 0, epoch: 3, iters: 88288, time: 0.008) nll: 65.019249 \n",
      "(GPU: 0, epoch: 3, iters: 89088, time: 0.008) nll: 65.395874 \n",
      "(GPU: 0, epoch: 3, iters: 89888, time: 0.008) nll: 76.179092 \n",
      "(GPU: 0, epoch: 3, iters: 90688, time: 0.008) nll: 80.183105 \n",
      "(GPU: 0, epoch: 3, iters: 91488, time: 0.007) nll: 75.371841 \n",
      "(GPU: 0, epoch: 3, iters: 92288, time: 0.008) nll: 78.595695 \n",
      "(GPU: 0, epoch: 3, iters: 93088, time: 0.008) nll: 67.754845 \n",
      "(GPU: 0, epoch: 3, iters: 93888, time: 0.008) nll: 76.272560 \n",
      "(GPU: 0, epoch: 3, iters: 94688, time: 0.008) nll: 63.502098 \n",
      "(GPU: 0, epoch: 3, iters: 95488, time: 0.008) nll: 57.624981 \n",
      "(GPU: 0, epoch: 3, iters: 96288, time: 0.008) nll: 62.124397 \n",
      "(GPU: 0, epoch: 3, iters: 97088, time: 0.008) nll: 48.844826 \n",
      "(GPU: 0, epoch: 3, iters: 97888, time: 0.008) nll: 69.123474 \n",
      "saving the latest model (epoch 3, total_steps 520000)\n",
      "(GPU: 0, epoch: 3, iters: 98688, time: 0.008) nll: 45.657909 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4397/4397 [20:45<00:00,  3.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 3, iters: 99488, time: 0.008) nll: 57.338104 \n",
      "(GPU: 0, epoch: 3, iters: 100288, time: 0.008) nll: 63.470787 \n",
      "(GPU: 0, epoch: 3, iters: 101088, time: 0.008) nll: 69.133347 \n",
      "(GPU: 0, epoch: 3, iters: 101888, time: 0.008) nll: 65.672974 \n",
      "(GPU: 0, epoch: 3, iters: 102688, time: 0.008) nll: 55.248379 \n",
      "(GPU: 0, epoch: 3, iters: 103488, time: 0.008) nll: 60.978188 \n",
      "(GPU: 0, epoch: 3, iters: 104288, time: 0.008) nll: 70.788933 \n",
      "(GPU: 0, epoch: 3, iters: 105088, time: 0.008) nll: 63.305038 \n",
      "(GPU: 0, epoch: 3, iters: 105888, time: 0.008) nll: 79.751213 \n",
      "(GPU: 0, epoch: 3, iters: 106688, time: 0.008) nll: 54.847813 \n",
      "(GPU: 0, epoch: 3, iters: 107488, time: 0.008) nll: 61.610001 \n",
      "(GPU: 0, epoch: 3, iters: 108288, time: 0.008) nll: 49.969925 \n",
      "(GPU: 0, epoch: 3, iters: 109088, time: 0.008) nll: 59.405499 \n",
      "(GPU: 0, epoch: 3, iters: 109888, time: 0.008) nll: 68.224266 \n",
      "(GPU: 0, epoch: 3, iters: 110688, time: 0.008) nll: 63.189278 \n",
      "(GPU: 0, epoch: 3, iters: 111488, time: 0.008) nll: 43.349327 \n",
      "(GPU: 0, epoch: 3, iters: 112288, time: 0.008) nll: 72.901482 \n",
      "(GPU: 0, epoch: 3, iters: 113088, time: 0.008) nll: 61.619568 \n",
      "(GPU: 0, epoch: 3, iters: 113888, time: 0.008) nll: 70.282883 \n",
      "(GPU: 0, epoch: 3, iters: 114688, time: 0.008) nll: 74.277710 \n",
      "(GPU: 0, epoch: 3, iters: 115488, time: 0.008) nll: 55.053093 \n",
      "(GPU: 0, epoch: 3, iters: 116288, time: 0.008) nll: 59.808201 \n",
      "(GPU: 0, epoch: 3, iters: 117088, time: 0.008) nll: 71.611526 \n",
      "(GPU: 0, epoch: 3, iters: 117888, time: 0.008) nll: 57.201569 \n",
      "saving the latest model (epoch 3, total_steps 540000)\n",
      "(GPU: 0, epoch: 3, iters: 118688, time: 0.008) nll: 94.972633 \n",
      "(GPU: 0, epoch: 3, iters: 119488, time: 0.008) nll: 87.302582 \n",
      "(GPU: 0, epoch: 3, iters: 120288, time: 0.008) nll: 61.237564 \n",
      "(GPU: 0, epoch: 3, iters: 121088, time: 0.008) nll: 79.208153 \n",
      "(GPU: 0, epoch: 3, iters: 121888, time: 0.008) nll: 78.466393 \n",
      "(GPU: 0, epoch: 3, iters: 122688, time: 0.008) nll: 64.337212 \n",
      "(GPU: 0, epoch: 3, iters: 123488, time: 0.008) nll: 64.236702 \n",
      "(GPU: 0, epoch: 3, iters: 124288, time: 0.008) nll: 64.209457 \n",
      "(GPU: 0, epoch: 3, iters: 125088, time: 0.008) nll: 82.486382 \n",
      "(GPU: 0, epoch: 3, iters: 125888, time: 0.008) nll: 72.439590 \n",
      "(GPU: 0, epoch: 3, iters: 126688, time: 0.008) nll: 110.687485 \n",
      "(GPU: 0, epoch: 3, iters: 127488, time: 0.008) nll: 69.238998 \n",
      "(GPU: 0, epoch: 3, iters: 128288, time: 0.007) nll: 54.115932 \n",
      "(GPU: 0, epoch: 3, iters: 129088, time: 0.008) nll: 82.699066 \n",
      "(GPU: 0, epoch: 3, iters: 129888, time: 0.008) nll: 56.794022 \n",
      "(GPU: 0, epoch: 3, iters: 130688, time: 0.008) nll: 93.761292 \n",
      "(GPU: 0, epoch: 3, iters: 131488, time: 0.008) nll: 52.276741 \n",
      "(GPU: 0, epoch: 3, iters: 132288, time: 0.008) nll: 63.530396 \n",
      "(GPU: 0, epoch: 3, iters: 133088, time: 0.008) nll: 74.242844 \n",
      "(GPU: 0, epoch: 3, iters: 133888, time: 0.008) nll: 92.987053 \n",
      "(GPU: 0, epoch: 3, iters: 134688, time: 0.008) nll: 69.258423 \n",
      "(GPU: 0, epoch: 3, iters: 135488, time: 0.008) nll: 75.717636 \n",
      "(GPU: 0, epoch: 3, iters: 136288, time: 0.008) nll: 74.080017 \n",
      "(GPU: 0, epoch: 3, iters: 137088, time: 0.008) nll: 89.253403 \n",
      "(GPU: 0, epoch: 3, iters: 137888, time: 0.008) nll: 65.094330 \n",
      "saving the latest model (epoch 3, total_steps 560000)\n",
      "(GPU: 0, epoch: 3, iters: 138688, time: 0.008) nll: 41.962776 \n",
      "(GPU: 0, epoch: 3, iters: 139488, time: 0.008) nll: 44.020996 \n",
      "(GPU: 0, epoch: 3, iters: 140288, time: 0.008) nll: 54.133587 \n",
      "saving the model at the end of epoch 3, iters 562816\n",
      "([test] GPU: 0, epoch: 3) \n",
      "OrderedDict()\n",
      "[*] End of epoch 3 / 25 \t Time Taken: 1274 sec \n",
      "/cluster/54/streakfull/ADL4CV/Project/src/plz-autosdf/logs/bert2vqsc_v4-text2shape-seq-LR1e-4-network-bert-shapeset-shapeset-SmoothL1Loss\n",
      "[*] learning rate = 0.0000400\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 3111/4397 [14:42<05:33,  3.86it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 4, iters: 32, time: 0.004) nll: 66.588043 \n",
      "(GPU: 0, epoch: 4, iters: 32, time: 0.004) nll: 78.571617 \n",
      "(GPU: 0, epoch: 4, iters: 384, time: 0.008) nll: 84.052170 \n",
      "(GPU: 0, epoch: 4, iters: 1184, time: 0.008) nll: 82.006042 \n",
      "(GPU: 0, epoch: 4, iters: 1984, time: 0.008) nll: 88.882195 \n",
      "(GPU: 0, epoch: 4, iters: 2784, time: 0.008) nll: 58.046425 \n",
      "(GPU: 0, epoch: 4, iters: 3584, time: 0.008) nll: 75.162895 \n",
      "(GPU: 0, epoch: 4, iters: 4384, time: 0.008) nll: 51.246178 \n",
      "(GPU: 0, epoch: 4, iters: 5184, time: 0.008) nll: 61.832939 \n",
      "(GPU: 0, epoch: 4, iters: 5984, time: 0.008) nll: 70.048813 \n",
      "(GPU: 0, epoch: 4, iters: 6784, time: 0.008) nll: 67.166161 \n",
      "(GPU: 0, epoch: 4, iters: 7584, time: 0.008) nll: 61.090225 \n",
      "(GPU: 0, epoch: 4, iters: 8384, time: 0.008) nll: 74.766830 \n",
      "(GPU: 0, epoch: 4, iters: 9184, time: 0.008) nll: 63.081337 \n",
      "(GPU: 0, epoch: 4, iters: 9984, time: 0.008) nll: 53.254749 \n",
      "(GPU: 0, epoch: 4, iters: 10784, time: 0.007) nll: 75.543777 \n",
      "(GPU: 0, epoch: 4, iters: 11584, time: 0.008) nll: 57.935318 \n",
      "(GPU: 0, epoch: 4, iters: 12384, time: 0.008) nll: 83.130249 \n",
      "(GPU: 0, epoch: 4, iters: 13184, time: 0.008) nll: 54.422447 \n",
      "(GPU: 0, epoch: 4, iters: 13184, time: 0.013) nll: 54.128407 \n",
      "(GPU: 0, epoch: 4, iters: 13184, time: 0.013) nll: 63.445518 \n",
      "(GPU: 0, epoch: 4, iters: 13984, time: 0.007) nll: 69.993263 \n",
      "(GPU: 0, epoch: 4, iters: 14784, time: 0.008) nll: 76.661858 \n",
      "(GPU: 0, epoch: 4, iters: 15584, time: 0.008) nll: 67.088875 \n",
      "(GPU: 0, epoch: 4, iters: 16384, time: 0.008) nll: 77.631577 \n",
      "(GPU: 0, epoch: 4, iters: 17184, time: 0.008) nll: 85.006241 \n",
      "saving the latest model (epoch 4, total_steps 580000)\n",
      "(GPU: 0, epoch: 4, iters: 17984, time: 0.008) nll: 66.111893 \n",
      "(GPU: 0, epoch: 4, iters: 18784, time: 0.008) nll: 82.787537 \n",
      "(GPU: 0, epoch: 4, iters: 19584, time: 0.008) nll: 76.086807 \n",
      "(GPU: 0, epoch: 4, iters: 20384, time: 0.008) nll: 62.436432 \n",
      "(GPU: 0, epoch: 4, iters: 21184, time: 0.008) nll: 81.154541 \n",
      "(GPU: 0, epoch: 4, iters: 21984, time: 0.008) nll: 62.198532 \n",
      "(GPU: 0, epoch: 4, iters: 22784, time: 0.008) nll: 61.419769 \n",
      "(GPU: 0, epoch: 4, iters: 23584, time: 0.008) nll: 64.532898 \n",
      "(GPU: 0, epoch: 4, iters: 24384, time: 0.008) nll: 70.336502 \n",
      "(GPU: 0, epoch: 4, iters: 25184, time: 0.008) nll: 51.727871 \n",
      "(GPU: 0, epoch: 4, iters: 25984, time: 0.008) nll: 59.313148 \n",
      "(GPU: 0, epoch: 4, iters: 26784, time: 0.008) nll: 63.483486 \n",
      "(GPU: 0, epoch: 4, iters: 27584, time: 0.008) nll: 41.547981 \n",
      "(GPU: 0, epoch: 4, iters: 28384, time: 0.007) nll: 57.148731 \n",
      "(GPU: 0, epoch: 4, iters: 29184, time: 0.008) nll: 70.017296 \n",
      "(GPU: 0, epoch: 4, iters: 29984, time: 0.008) nll: 74.106964 \n",
      "(GPU: 0, epoch: 4, iters: 30784, time: 0.008) nll: 60.905209 \n",
      "(GPU: 0, epoch: 4, iters: 31584, time: 0.008) nll: 79.183472 \n",
      "(GPU: 0, epoch: 4, iters: 32384, time: 0.008) nll: 80.787704 \n",
      "(GPU: 0, epoch: 4, iters: 33184, time: 0.008) nll: 78.052765 \n",
      "(GPU: 0, epoch: 4, iters: 33984, time: 0.008) nll: 89.376572 \n",
      "(GPU: 0, epoch: 4, iters: 34784, time: 0.008) nll: 64.303665 \n",
      "(GPU: 0, epoch: 4, iters: 35584, time: 0.008) nll: 52.709579 \n",
      "(GPU: 0, epoch: 4, iters: 36384, time: 0.008) nll: 76.836853 \n",
      "(GPU: 0, epoch: 4, iters: 37184, time: 0.008) nll: 62.359550 \n",
      "saving the latest model (epoch 4, total_steps 600000)\n",
      "(GPU: 0, epoch: 4, iters: 37984, time: 0.008) nll: 76.038849 \n",
      "(GPU: 0, epoch: 4, iters: 38784, time: 0.008) nll: 80.198807 \n",
      "(GPU: 0, epoch: 4, iters: 39584, time: 0.008) nll: 91.299477 \n",
      "(GPU: 0, epoch: 4, iters: 40384, time: 0.008) nll: 60.587704 \n",
      "(GPU: 0, epoch: 4, iters: 41184, time: 0.008) nll: 53.020821 \n",
      "(GPU: 0, epoch: 4, iters: 41984, time: 0.008) nll: 79.047356 \n",
      "(GPU: 0, epoch: 4, iters: 42784, time: 0.008) nll: 64.398804 \n",
      "(GPU: 0, epoch: 4, iters: 43584, time: 0.008) nll: 50.288860 \n",
      "(GPU: 0, epoch: 4, iters: 44384, time: 0.008) nll: 62.107178 \n",
      "(GPU: 0, epoch: 4, iters: 45184, time: 0.008) nll: 90.370422 \n",
      "(GPU: 0, epoch: 4, iters: 45984, time: 0.008) nll: 83.893204 \n",
      "(GPU: 0, epoch: 4, iters: 46784, time: 0.008) nll: 80.722069 \n",
      "(GPU: 0, epoch: 4, iters: 47584, time: 0.008) nll: 65.641052 \n",
      "(GPU: 0, epoch: 4, iters: 48384, time: 0.008) nll: 60.995770 \n",
      "(GPU: 0, epoch: 4, iters: 49184, time: 0.008) nll: 61.537552 \n",
      "(GPU: 0, epoch: 4, iters: 49984, time: 0.008) nll: 83.328690 \n",
      "(GPU: 0, epoch: 4, iters: 50784, time: 0.008) nll: 64.686035 \n",
      "(GPU: 0, epoch: 4, iters: 51584, time: 0.008) nll: 71.972298 \n",
      "(GPU: 0, epoch: 4, iters: 52384, time: 0.008) nll: 89.870163 \n",
      "(GPU: 0, epoch: 4, iters: 53184, time: 0.008) nll: 67.630005 \n",
      "(GPU: 0, epoch: 4, iters: 53984, time: 0.008) nll: 83.532730 \n",
      "(GPU: 0, epoch: 4, iters: 54784, time: 0.008) nll: 55.660686 \n",
      "(GPU: 0, epoch: 4, iters: 55584, time: 0.008) nll: 65.600563 \n",
      "(GPU: 0, epoch: 4, iters: 56384, time: 0.008) nll: 58.633064 \n",
      "(GPU: 0, epoch: 4, iters: 57184, time: 0.008) nll: 61.771694 \n",
      "saving the latest model (epoch 4, total_steps 620000)\n",
      "(GPU: 0, epoch: 4, iters: 57984, time: 0.008) nll: 79.430801 \n",
      "(GPU: 0, epoch: 4, iters: 58784, time: 0.008) nll: 89.161400 \n",
      "(GPU: 0, epoch: 4, iters: 59584, time: 0.008) nll: 68.599922 \n",
      "(GPU: 0, epoch: 4, iters: 60384, time: 0.008) nll: 63.987957 \n",
      "(GPU: 0, epoch: 4, iters: 61184, time: 0.008) nll: 54.283546 \n",
      "(GPU: 0, epoch: 4, iters: 61984, time: 0.008) nll: 63.709328 \n",
      "(GPU: 0, epoch: 4, iters: 62784, time: 0.008) nll: 66.163429 \n",
      "(GPU: 0, epoch: 4, iters: 63584, time: 0.008) nll: 58.817955 \n",
      "(GPU: 0, epoch: 4, iters: 64384, time: 0.008) nll: 71.948776 \n",
      "(GPU: 0, epoch: 4, iters: 65184, time: 0.007) nll: 87.334244 \n",
      "(GPU: 0, epoch: 4, iters: 65984, time: 0.008) nll: 82.823013 \n",
      "(GPU: 0, epoch: 4, iters: 66784, time: 0.008) nll: 55.514759 \n",
      "(GPU: 0, epoch: 4, iters: 67584, time: 0.008) nll: 60.108673 \n",
      "(GPU: 0, epoch: 4, iters: 68384, time: 0.008) nll: 65.176529 \n",
      "(GPU: 0, epoch: 4, iters: 69184, time: 0.008) nll: 53.204582 \n",
      "(GPU: 0, epoch: 4, iters: 69984, time: 0.008) nll: 64.142113 \n",
      "(GPU: 0, epoch: 4, iters: 70784, time: 0.008) nll: 89.653191 \n",
      "(GPU: 0, epoch: 4, iters: 71584, time: 0.007) nll: 62.975609 \n",
      "(GPU: 0, epoch: 4, iters: 72384, time: 0.008) nll: 74.055328 \n",
      "(GPU: 0, epoch: 4, iters: 73184, time: 0.008) nll: 81.438507 \n",
      "(GPU: 0, epoch: 4, iters: 73984, time: 0.008) nll: 46.641914 \n",
      "(GPU: 0, epoch: 4, iters: 74784, time: 0.008) nll: 63.624969 \n",
      "(GPU: 0, epoch: 4, iters: 75584, time: 0.008) nll: 80.481972 \n",
      "(GPU: 0, epoch: 4, iters: 76384, time: 0.008) nll: 59.542866 \n",
      "(GPU: 0, epoch: 4, iters: 77184, time: 0.008) nll: 56.306343 \n",
      "saving the latest model (epoch 4, total_steps 640000)\n",
      "(GPU: 0, epoch: 4, iters: 77984, time: 0.008) nll: 78.372772 \n",
      "(GPU: 0, epoch: 4, iters: 78784, time: 0.008) nll: 69.185333 \n",
      "(GPU: 0, epoch: 4, iters: 79584, time: 0.007) nll: 60.443108 \n",
      "(GPU: 0, epoch: 4, iters: 80384, time: 0.008) nll: 75.344177 \n",
      "(GPU: 0, epoch: 4, iters: 81184, time: 0.008) nll: 86.602669 \n",
      "(GPU: 0, epoch: 4, iters: 81984, time: 0.008) nll: 93.189453 \n",
      "(GPU: 0, epoch: 4, iters: 82784, time: 0.008) nll: 56.553318 \n",
      "(GPU: 0, epoch: 4, iters: 83584, time: 0.008) nll: 90.487213 \n",
      "(GPU: 0, epoch: 4, iters: 84384, time: 0.008) nll: 51.635128 \n",
      "(GPU: 0, epoch: 4, iters: 85184, time: 0.008) nll: 62.306778 \n",
      "(GPU: 0, epoch: 4, iters: 85984, time: 0.008) nll: 69.594315 \n",
      "(GPU: 0, epoch: 4, iters: 86784, time: 0.008) nll: 71.503220 \n",
      "(GPU: 0, epoch: 4, iters: 87584, time: 0.008) nll: 61.651142 \n",
      "(GPU: 0, epoch: 4, iters: 88384, time: 0.008) nll: 76.369194 \n",
      "(GPU: 0, epoch: 4, iters: 89184, time: 0.008) nll: 75.821976 \n",
      "(GPU: 0, epoch: 4, iters: 89984, time: 0.008) nll: 58.654598 \n",
      "(GPU: 0, epoch: 4, iters: 90784, time: 0.008) nll: 57.474949 \n",
      "(GPU: 0, epoch: 4, iters: 91584, time: 0.008) nll: 97.815361 \n",
      "(GPU: 0, epoch: 4, iters: 92384, time: 0.008) nll: 78.875320 \n",
      "(GPU: 0, epoch: 4, iters: 93184, time: 0.008) nll: 62.307938 \n",
      "(GPU: 0, epoch: 4, iters: 93984, time: 0.008) nll: 74.328163 \n",
      "(GPU: 0, epoch: 4, iters: 94784, time: 0.008) nll: 69.209564 \n",
      "(GPU: 0, epoch: 4, iters: 95584, time: 0.008) nll: 84.742317 \n",
      "(GPU: 0, epoch: 4, iters: 96384, time: 0.008) nll: 53.909294 \n",
      "(GPU: 0, epoch: 4, iters: 97184, time: 0.007) nll: 61.400208 \n",
      "saving the latest model (epoch 4, total_steps 660000)\n",
      "(GPU: 0, epoch: 4, iters: 97984, time: 0.008) nll: 67.607422 \n",
      "(GPU: 0, epoch: 4, iters: 98784, time: 0.007) nll: 60.002007 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4397/4397 [20:46<00:00,  3.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 4, iters: 99584, time: 0.008) nll: 62.043659 \n",
      "(GPU: 0, epoch: 4, iters: 100384, time: 0.008) nll: 77.429581 \n",
      "(GPU: 0, epoch: 4, iters: 101184, time: 0.008) nll: 82.992516 \n",
      "(GPU: 0, epoch: 4, iters: 101984, time: 0.007) nll: 86.393997 \n",
      "(GPU: 0, epoch: 4, iters: 102784, time: 0.008) nll: 58.990730 \n",
      "(GPU: 0, epoch: 4, iters: 103584, time: 0.008) nll: 59.581833 \n",
      "(GPU: 0, epoch: 4, iters: 104384, time: 0.008) nll: 82.409767 \n",
      "(GPU: 0, epoch: 4, iters: 105184, time: 0.007) nll: 79.787888 \n",
      "(GPU: 0, epoch: 4, iters: 105984, time: 0.008) nll: 44.449295 \n",
      "(GPU: 0, epoch: 4, iters: 106784, time: 0.007) nll: 56.623299 \n",
      "(GPU: 0, epoch: 4, iters: 107584, time: 0.008) nll: 89.933456 \n",
      "(GPU: 0, epoch: 4, iters: 108384, time: 0.008) nll: 64.844208 \n",
      "(GPU: 0, epoch: 4, iters: 109184, time: 0.008) nll: 62.081238 \n",
      "(GPU: 0, epoch: 4, iters: 109184, time: 0.013) nll: 62.165920 \n",
      "(GPU: 0, epoch: 4, iters: 109184, time: 0.013) nll: 75.063766 \n",
      "(GPU: 0, epoch: 4, iters: 109984, time: 0.008) nll: 73.273590 \n",
      "(GPU: 0, epoch: 4, iters: 110784, time: 0.008) nll: 48.864979 \n",
      "(GPU: 0, epoch: 4, iters: 111584, time: 0.008) nll: 61.847176 \n",
      "(GPU: 0, epoch: 4, iters: 112384, time: 0.008) nll: 67.878029 \n",
      "(GPU: 0, epoch: 4, iters: 113184, time: 0.008) nll: 79.562744 \n",
      "(GPU: 0, epoch: 4, iters: 113984, time: 0.008) nll: 54.900467 \n",
      "(GPU: 0, epoch: 4, iters: 114784, time: 0.008) nll: 66.321793 \n",
      "(GPU: 0, epoch: 4, iters: 115584, time: 0.008) nll: 43.895729 \n",
      "(GPU: 0, epoch: 4, iters: 116384, time: 0.008) nll: 80.917908 \n",
      "(GPU: 0, epoch: 4, iters: 117184, time: 0.008) nll: 70.143204 \n",
      "saving the latest model (epoch 4, total_steps 680000)\n",
      "(GPU: 0, epoch: 4, iters: 117984, time: 0.007) nll: 70.402626 \n",
      "(GPU: 0, epoch: 4, iters: 118784, time: 0.008) nll: 83.884521 \n",
      "(GPU: 0, epoch: 4, iters: 119584, time: 0.008) nll: 59.262234 \n",
      "(GPU: 0, epoch: 4, iters: 120384, time: 0.008) nll: 76.638016 \n",
      "(GPU: 0, epoch: 4, iters: 121184, time: 0.008) nll: 100.155472 \n",
      "(GPU: 0, epoch: 4, iters: 121984, time: 0.008) nll: 63.952591 \n",
      "(GPU: 0, epoch: 4, iters: 122784, time: 0.008) nll: 81.581573 \n",
      "(GPU: 0, epoch: 4, iters: 123584, time: 0.008) nll: 38.689430 \n",
      "(GPU: 0, epoch: 4, iters: 124384, time: 0.008) nll: 65.228546 \n",
      "(GPU: 0, epoch: 4, iters: 125184, time: 0.008) nll: 56.587357 \n",
      "(GPU: 0, epoch: 4, iters: 125984, time: 0.008) nll: 55.613415 \n",
      "(GPU: 0, epoch: 4, iters: 126784, time: 0.008) nll: 45.137207 \n",
      "(GPU: 0, epoch: 4, iters: 127584, time: 0.008) nll: 69.876930 \n",
      "(GPU: 0, epoch: 4, iters: 128384, time: 0.008) nll: 80.707123 \n",
      "(GPU: 0, epoch: 4, iters: 129184, time: 0.008) nll: 69.643097 \n",
      "(GPU: 0, epoch: 4, iters: 129984, time: 0.008) nll: 65.302071 \n",
      "(GPU: 0, epoch: 4, iters: 130784, time: 0.008) nll: 44.454254 \n",
      "(GPU: 0, epoch: 4, iters: 131584, time: 0.008) nll: 62.741081 \n",
      "(GPU: 0, epoch: 4, iters: 132384, time: 0.007) nll: 82.970299 \n",
      "(GPU: 0, epoch: 4, iters: 133184, time: 0.008) nll: 87.314743 \n",
      "(GPU: 0, epoch: 4, iters: 133984, time: 0.008) nll: 68.365807 \n",
      "(GPU: 0, epoch: 4, iters: 134784, time: 0.008) nll: 54.097267 \n",
      "(GPU: 0, epoch: 4, iters: 135584, time: 0.008) nll: 56.017006 \n",
      "(GPU: 0, epoch: 4, iters: 136384, time: 0.008) nll: 81.797043 \n",
      "(GPU: 0, epoch: 4, iters: 137184, time: 0.008) nll: 84.753021 \n",
      "saving the latest model (epoch 4, total_steps 700000)\n",
      "(GPU: 0, epoch: 4, iters: 137984, time: 0.008) nll: 74.274513 \n",
      "(GPU: 0, epoch: 4, iters: 138784, time: 0.008) nll: 90.404678 \n",
      "(GPU: 0, epoch: 4, iters: 139584, time: 0.008) nll: 65.488037 \n",
      "(GPU: 0, epoch: 4, iters: 140384, time: 0.008) nll: 74.852783 \n",
      "[*] End of epoch 4 / 25 \t Time Taken: 1247 sec \n",
      "/cluster/54/streakfull/ADL4CV/Project/src/plz-autosdf/logs/bert2vqsc_v4-text2shape-seq-LR1e-4-network-bert-shapeset-shapeset-SmoothL1Loss\n",
      "[*] learning rate = 0.0000500\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 3114/4397 [14:43<05:32,  3.85it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 5, iters: 32, time: 0.004) nll: 42.076237 \n",
      "(GPU: 0, epoch: 5, iters: 32, time: 0.004) nll: 55.038147 \n",
      "(GPU: 0, epoch: 5, iters: 480, time: 0.008) nll: 32.289490 \n",
      "(GPU: 0, epoch: 5, iters: 1280, time: 0.008) nll: 68.145515 \n",
      "(GPU: 0, epoch: 5, iters: 2080, time: 0.008) nll: 61.070801 \n",
      "(GPU: 0, epoch: 5, iters: 2880, time: 0.008) nll: 69.058922 \n",
      "(GPU: 0, epoch: 5, iters: 3680, time: 0.008) nll: 51.873634 \n",
      "(GPU: 0, epoch: 5, iters: 4480, time: 0.008) nll: 68.512993 \n",
      "(GPU: 0, epoch: 5, iters: 5280, time: 0.008) nll: 83.475929 \n",
      "(GPU: 0, epoch: 5, iters: 6080, time: 0.008) nll: 59.054195 \n",
      "(GPU: 0, epoch: 5, iters: 6880, time: 0.008) nll: 77.115555 \n",
      "(GPU: 0, epoch: 5, iters: 7680, time: 0.008) nll: 46.715435 \n",
      "(GPU: 0, epoch: 5, iters: 8480, time: 0.008) nll: 53.111870 \n",
      "(GPU: 0, epoch: 5, iters: 9280, time: 0.008) nll: 71.906189 \n",
      "(GPU: 0, epoch: 5, iters: 10080, time: 0.008) nll: 73.952492 \n",
      "(GPU: 0, epoch: 5, iters: 10880, time: 0.008) nll: 32.718491 \n",
      "(GPU: 0, epoch: 5, iters: 11680, time: 0.008) nll: 71.347809 \n",
      "(GPU: 0, epoch: 5, iters: 12480, time: 0.008) nll: 58.970200 \n",
      "(GPU: 0, epoch: 5, iters: 13280, time: 0.008) nll: 73.208824 \n",
      "(GPU: 0, epoch: 5, iters: 14080, time: 0.008) nll: 61.323044 \n",
      "(GPU: 0, epoch: 5, iters: 14880, time: 0.008) nll: 69.644707 \n",
      "(GPU: 0, epoch: 5, iters: 15680, time: 0.009) nll: 57.885765 \n",
      "(GPU: 0, epoch: 5, iters: 16480, time: 0.008) nll: 55.752373 \n",
      "saving the latest model (epoch 5, total_steps 720000)\n",
      "(GPU: 0, epoch: 5, iters: 17280, time: 0.008) nll: 75.662331 \n",
      "(GPU: 0, epoch: 5, iters: 18080, time: 0.008) nll: 49.106297 \n",
      "(GPU: 0, epoch: 5, iters: 18880, time: 0.008) nll: 74.927963 \n",
      "(GPU: 0, epoch: 5, iters: 19680, time: 0.008) nll: 85.270294 \n",
      "(GPU: 0, epoch: 5, iters: 20480, time: 0.008) nll: 81.997147 \n",
      "(GPU: 0, epoch: 5, iters: 21280, time: 0.008) nll: 74.316788 \n",
      "(GPU: 0, epoch: 5, iters: 22080, time: 0.008) nll: 54.511280 \n",
      "(GPU: 0, epoch: 5, iters: 22880, time: 0.008) nll: 69.366135 \n",
      "(GPU: 0, epoch: 5, iters: 23680, time: 0.008) nll: 66.520645 \n",
      "(GPU: 0, epoch: 5, iters: 24480, time: 0.008) nll: 56.106903 \n",
      "(GPU: 0, epoch: 5, iters: 25280, time: 0.008) nll: 57.890160 \n",
      "(GPU: 0, epoch: 5, iters: 26080, time: 0.008) nll: 62.369537 \n",
      "(GPU: 0, epoch: 5, iters: 26880, time: 0.008) nll: 70.931137 \n",
      "(GPU: 0, epoch: 5, iters: 27680, time: 0.008) nll: 61.895058 \n",
      "(GPU: 0, epoch: 5, iters: 28480, time: 0.008) nll: 65.652588 \n",
      "(GPU: 0, epoch: 5, iters: 29280, time: 0.008) nll: 89.978668 \n",
      "(GPU: 0, epoch: 5, iters: 30080, time: 0.008) nll: 61.916435 \n",
      "(GPU: 0, epoch: 5, iters: 30880, time: 0.008) nll: 66.845993 \n",
      "(GPU: 0, epoch: 5, iters: 31680, time: 0.008) nll: 83.105530 \n",
      "(GPU: 0, epoch: 5, iters: 32480, time: 0.008) nll: 68.422966 \n",
      "(GPU: 0, epoch: 5, iters: 33280, time: 0.008) nll: 56.061981 \n",
      "(GPU: 0, epoch: 5, iters: 34080, time: 0.008) nll: 99.370323 \n",
      "(GPU: 0, epoch: 5, iters: 34880, time: 0.008) nll: 73.186737 \n",
      "(GPU: 0, epoch: 5, iters: 35680, time: 0.008) nll: 81.819008 \n",
      "(GPU: 0, epoch: 5, iters: 36480, time: 0.008) nll: 80.470505 \n",
      "saving the latest model (epoch 5, total_steps 740000)\n",
      "(GPU: 0, epoch: 5, iters: 37280, time: 0.008) nll: 58.852798 \n",
      "(GPU: 0, epoch: 5, iters: 38080, time: 0.008) nll: 76.065353 \n",
      "(GPU: 0, epoch: 5, iters: 38880, time: 0.008) nll: 71.186432 \n",
      "(GPU: 0, epoch: 5, iters: 39680, time: 0.008) nll: 68.521271 \n",
      "(GPU: 0, epoch: 5, iters: 40480, time: 0.008) nll: 63.412136 \n",
      "(GPU: 0, epoch: 5, iters: 41280, time: 0.008) nll: 62.170792 \n",
      "(GPU: 0, epoch: 5, iters: 42080, time: 0.008) nll: 79.675751 \n",
      "(GPU: 0, epoch: 5, iters: 42880, time: 0.008) nll: 89.549957 \n",
      "(GPU: 0, epoch: 5, iters: 43680, time: 0.008) nll: 73.776367 \n",
      "(GPU: 0, epoch: 5, iters: 44480, time: 0.008) nll: 81.813667 \n",
      "(GPU: 0, epoch: 5, iters: 45280, time: 0.008) nll: 53.690765 \n",
      "(GPU: 0, epoch: 5, iters: 46080, time: 0.008) nll: 68.086121 \n",
      "(GPU: 0, epoch: 5, iters: 46880, time: 0.008) nll: 54.884762 \n",
      "(GPU: 0, epoch: 5, iters: 47680, time: 0.008) nll: 60.723591 \n",
      "(GPU: 0, epoch: 5, iters: 48480, time: 0.008) nll: 63.123901 \n",
      "(GPU: 0, epoch: 5, iters: 49280, time: 0.008) nll: 77.475800 \n",
      "(GPU: 0, epoch: 5, iters: 50080, time: 0.008) nll: 98.925903 \n",
      "(GPU: 0, epoch: 5, iters: 50880, time: 0.008) nll: 65.697189 \n",
      "(GPU: 0, epoch: 5, iters: 51680, time: 0.008) nll: 55.821037 \n",
      "(GPU: 0, epoch: 5, iters: 52480, time: 0.008) nll: 70.632919 \n",
      "(GPU: 0, epoch: 5, iters: 53280, time: 0.008) nll: 48.507797 \n",
      "(GPU: 0, epoch: 5, iters: 54080, time: 0.008) nll: 80.166550 \n",
      "(GPU: 0, epoch: 5, iters: 54880, time: 0.008) nll: 80.036911 \n",
      "(GPU: 0, epoch: 5, iters: 55680, time: 0.008) nll: 104.416382 \n",
      "(GPU: 0, epoch: 5, iters: 56480, time: 0.008) nll: 74.876221 \n",
      "saving the latest model (epoch 5, total_steps 760000)\n",
      "(GPU: 0, epoch: 5, iters: 57280, time: 0.008) nll: 65.303497 \n",
      "(GPU: 0, epoch: 5, iters: 58080, time: 0.008) nll: 70.605560 \n",
      "(GPU: 0, epoch: 5, iters: 58880, time: 0.008) nll: 83.103180 \n",
      "(GPU: 0, epoch: 5, iters: 59680, time: 0.008) nll: 68.110825 \n",
      "(GPU: 0, epoch: 5, iters: 60480, time: 0.008) nll: 53.424816 \n",
      "(GPU: 0, epoch: 5, iters: 61280, time: 0.008) nll: 49.163929 \n",
      "(GPU: 0, epoch: 5, iters: 62080, time: 0.008) nll: 63.556610 \n",
      "(GPU: 0, epoch: 5, iters: 62880, time: 0.008) nll: 74.455048 \n",
      "(GPU: 0, epoch: 5, iters: 63680, time: 0.008) nll: 56.366402 \n",
      "(GPU: 0, epoch: 5, iters: 64480, time: 0.008) nll: 57.134335 \n",
      "(GPU: 0, epoch: 5, iters: 64480, time: 0.013) nll: 56.642941 \n",
      "(GPU: 0, epoch: 5, iters: 64480, time: 0.013) nll: 59.151001 \n",
      "(GPU: 0, epoch: 5, iters: 65280, time: 0.008) nll: 84.965546 \n",
      "(GPU: 0, epoch: 5, iters: 66080, time: 0.007) nll: 57.108437 \n",
      "(GPU: 0, epoch: 5, iters: 66880, time: 0.008) nll: 78.558563 \n",
      "(GPU: 0, epoch: 5, iters: 67680, time: 0.008) nll: 93.466721 \n",
      "(GPU: 0, epoch: 5, iters: 68480, time: 0.008) nll: 64.486336 \n",
      "(GPU: 0, epoch: 5, iters: 69280, time: 0.008) nll: 90.618195 \n",
      "(GPU: 0, epoch: 5, iters: 70080, time: 0.008) nll: 48.755245 \n",
      "(GPU: 0, epoch: 5, iters: 70880, time: 0.008) nll: 73.404167 \n",
      "(GPU: 0, epoch: 5, iters: 71680, time: 0.008) nll: 71.064941 \n",
      "(GPU: 0, epoch: 5, iters: 72480, time: 0.008) nll: 70.911850 \n",
      "(GPU: 0, epoch: 5, iters: 73280, time: 0.008) nll: 67.923843 \n",
      "(GPU: 0, epoch: 5, iters: 74080, time: 0.008) nll: 61.868568 \n",
      "(GPU: 0, epoch: 5, iters: 74880, time: 0.008) nll: 65.797455 \n",
      "(GPU: 0, epoch: 5, iters: 75680, time: 0.008) nll: 72.385078 \n",
      "(GPU: 0, epoch: 5, iters: 76480, time: 0.008) nll: 66.947838 \n",
      "saving the latest model (epoch 5, total_steps 780000)\n",
      "(GPU: 0, epoch: 5, iters: 77280, time: 0.008) nll: 64.717468 \n",
      "(GPU: 0, epoch: 5, iters: 78080, time: 0.008) nll: 58.925621 \n",
      "(GPU: 0, epoch: 5, iters: 78880, time: 0.008) nll: 58.696850 \n",
      "(GPU: 0, epoch: 5, iters: 79680, time: 0.008) nll: 60.957401 \n",
      "(GPU: 0, epoch: 5, iters: 80480, time: 0.008) nll: 68.242752 \n",
      "(GPU: 0, epoch: 5, iters: 81280, time: 0.008) nll: 64.223328 \n",
      "(GPU: 0, epoch: 5, iters: 82080, time: 0.008) nll: 44.082718 \n",
      "(GPU: 0, epoch: 5, iters: 82880, time: 0.008) nll: 69.136322 \n",
      "(GPU: 0, epoch: 5, iters: 83680, time: 0.008) nll: 66.140327 \n",
      "(GPU: 0, epoch: 5, iters: 84480, time: 0.008) nll: 59.532623 \n",
      "(GPU: 0, epoch: 5, iters: 85280, time: 0.008) nll: 79.786179 \n",
      "(GPU: 0, epoch: 5, iters: 86080, time: 0.008) nll: 63.840805 \n",
      "(GPU: 0, epoch: 5, iters: 86880, time: 0.008) nll: 43.909798 \n",
      "(GPU: 0, epoch: 5, iters: 87680, time: 0.008) nll: 42.038498 \n",
      "(GPU: 0, epoch: 5, iters: 88480, time: 0.008) nll: 82.113419 \n",
      "(GPU: 0, epoch: 5, iters: 89280, time: 0.008) nll: 62.557693 \n",
      "(GPU: 0, epoch: 5, iters: 90080, time: 0.008) nll: 59.118034 \n",
      "(GPU: 0, epoch: 5, iters: 90880, time: 0.008) nll: 57.739346 \n",
      "(GPU: 0, epoch: 5, iters: 91680, time: 0.008) nll: 52.361935 \n",
      "(GPU: 0, epoch: 5, iters: 92480, time: 0.008) nll: 65.976395 \n",
      "(GPU: 0, epoch: 5, iters: 93280, time: 0.008) nll: 87.539894 \n",
      "(GPU: 0, epoch: 5, iters: 94080, time: 0.008) nll: 65.051720 \n",
      "(GPU: 0, epoch: 5, iters: 94880, time: 0.008) nll: 88.047394 \n",
      "(GPU: 0, epoch: 5, iters: 95680, time: 0.008) nll: 56.426781 \n",
      "(GPU: 0, epoch: 5, iters: 96480, time: 0.008) nll: 58.949463 \n",
      "saving the latest model (epoch 5, total_steps 800000)\n",
      "(GPU: 0, epoch: 5, iters: 97280, time: 0.008) nll: 77.372772 \n",
      "(GPU: 0, epoch: 5, iters: 98080, time: 0.008) nll: 75.214302 \n",
      "(GPU: 0, epoch: 5, iters: 98880, time: 0.008) nll: 73.875931 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4397/4397 [20:47<00:00,  3.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 5, iters: 99680, time: 0.008) nll: 61.068672 \n",
      "(GPU: 0, epoch: 5, iters: 100480, time: 0.008) nll: 83.396790 \n",
      "(GPU: 0, epoch: 5, iters: 101280, time: 0.008) nll: 59.296303 \n",
      "(GPU: 0, epoch: 5, iters: 102080, time: 0.008) nll: 81.305763 \n",
      "(GPU: 0, epoch: 5, iters: 102880, time: 0.008) nll: 87.000923 \n",
      "(GPU: 0, epoch: 5, iters: 103680, time: 0.008) nll: 37.998337 \n",
      "(GPU: 0, epoch: 5, iters: 104480, time: 0.007) nll: 70.917542 \n",
      "(GPU: 0, epoch: 5, iters: 105280, time: 0.008) nll: 75.783844 \n",
      "(GPU: 0, epoch: 5, iters: 106080, time: 0.008) nll: 49.225876 \n",
      "(GPU: 0, epoch: 5, iters: 106880, time: 0.008) nll: 83.639427 \n",
      "(GPU: 0, epoch: 5, iters: 107680, time: 0.008) nll: 58.676811 \n",
      "(GPU: 0, epoch: 5, iters: 108480, time: 0.008) nll: 88.681480 \n",
      "(GPU: 0, epoch: 5, iters: 109280, time: 0.008) nll: 70.796890 \n",
      "(GPU: 0, epoch: 5, iters: 110080, time: 0.008) nll: 79.625870 \n",
      "(GPU: 0, epoch: 5, iters: 110880, time: 0.008) nll: 73.896027 \n",
      "(GPU: 0, epoch: 5, iters: 111680, time: 0.008) nll: 82.616150 \n",
      "(GPU: 0, epoch: 5, iters: 112480, time: 0.008) nll: 54.388351 \n",
      "(GPU: 0, epoch: 5, iters: 113280, time: 0.008) nll: 61.672112 \n",
      "(GPU: 0, epoch: 5, iters: 114080, time: 0.008) nll: 93.066696 \n",
      "(GPU: 0, epoch: 5, iters: 114880, time: 0.008) nll: 77.687401 \n",
      "(GPU: 0, epoch: 5, iters: 115680, time: 0.008) nll: 55.373169 \n",
      "(GPU: 0, epoch: 5, iters: 116480, time: 0.008) nll: 83.755211 \n",
      "saving the latest model (epoch 5, total_steps 820000)\n",
      "(GPU: 0, epoch: 5, iters: 117280, time: 0.008) nll: 64.197624 \n",
      "(GPU: 0, epoch: 5, iters: 118080, time: 0.008) nll: 79.817505 \n",
      "(GPU: 0, epoch: 5, iters: 118880, time: 0.008) nll: 62.775646 \n",
      "(GPU: 0, epoch: 5, iters: 119680, time: 0.008) nll: 77.014397 \n",
      "(GPU: 0, epoch: 5, iters: 120480, time: 0.008) nll: 70.001617 \n",
      "(GPU: 0, epoch: 5, iters: 121280, time: 0.008) nll: 53.931644 \n",
      "(GPU: 0, epoch: 5, iters: 122080, time: 0.007) nll: 46.071930 \n",
      "(GPU: 0, epoch: 5, iters: 122880, time: 0.008) nll: 73.719315 \n",
      "(GPU: 0, epoch: 5, iters: 123680, time: 0.008) nll: 78.436363 \n",
      "(GPU: 0, epoch: 5, iters: 124480, time: 0.008) nll: 89.807373 \n",
      "(GPU: 0, epoch: 5, iters: 125280, time: 0.008) nll: 64.017479 \n",
      "(GPU: 0, epoch: 5, iters: 126080, time: 0.008) nll: 58.843979 \n",
      "(GPU: 0, epoch: 5, iters: 126880, time: 0.008) nll: 63.446419 \n",
      "(GPU: 0, epoch: 5, iters: 127680, time: 0.008) nll: 64.271149 \n",
      "(GPU: 0, epoch: 5, iters: 128480, time: 0.008) nll: 69.098892 \n",
      "(GPU: 0, epoch: 5, iters: 129280, time: 0.008) nll: 75.675995 \n",
      "(GPU: 0, epoch: 5, iters: 130080, time: 0.008) nll: 74.594482 \n",
      "(GPU: 0, epoch: 5, iters: 130880, time: 0.008) nll: 61.796745 \n",
      "(GPU: 0, epoch: 5, iters: 131680, time: 0.008) nll: 78.808640 \n",
      "(GPU: 0, epoch: 5, iters: 132480, time: 0.008) nll: 79.620712 \n",
      "(GPU: 0, epoch: 5, iters: 133280, time: 0.008) nll: 68.996346 \n",
      "(GPU: 0, epoch: 5, iters: 134080, time: 0.008) nll: 73.955437 \n",
      "(GPU: 0, epoch: 5, iters: 134880, time: 0.008) nll: 52.490524 \n",
      "(GPU: 0, epoch: 5, iters: 135680, time: 0.008) nll: 88.494247 \n",
      "(GPU: 0, epoch: 5, iters: 136480, time: 0.008) nll: 65.277779 \n",
      "saving the latest model (epoch 5, total_steps 840000)\n",
      "(GPU: 0, epoch: 5, iters: 137280, time: 0.008) nll: 50.984001 \n",
      "(GPU: 0, epoch: 5, iters: 138080, time: 0.008) nll: 67.316422 \n",
      "(GPU: 0, epoch: 5, iters: 138880, time: 0.008) nll: 63.201324 \n",
      "(GPU: 0, epoch: 5, iters: 139680, time: 0.008) nll: 38.711842 \n",
      "(GPU: 0, epoch: 5, iters: 140480, time: 0.008) nll: 77.153397 \n",
      "[*] End of epoch 5 / 25 \t Time Taken: 1247 sec \n",
      "/cluster/54/streakfull/ADL4CV/Project/src/plz-autosdf/logs/bert2vqsc_v4-text2shape-seq-LR1e-4-network-bert-shapeset-shapeset-SmoothL1Loss\n",
      "[*] learning rate = 0.0000600\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 3117/4397 [14:45<05:33,  3.84it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 6, iters: 32, time: 0.004) nll: 70.431015 \n",
      "(GPU: 0, epoch: 6, iters: 32, time: 0.004) nll: 70.453606 \n",
      "(GPU: 0, epoch: 6, iters: 576, time: 0.008) nll: 61.688580 \n",
      "(GPU: 0, epoch: 6, iters: 1376, time: 0.008) nll: 64.022743 \n",
      "(GPU: 0, epoch: 6, iters: 2176, time: 0.008) nll: 50.905266 \n",
      "(GPU: 0, epoch: 6, iters: 2976, time: 0.008) nll: 64.260193 \n",
      "(GPU: 0, epoch: 6, iters: 3776, time: 0.008) nll: 57.777245 \n",
      "(GPU: 0, epoch: 6, iters: 4576, time: 0.008) nll: 69.485077 \n",
      "(GPU: 0, epoch: 6, iters: 5376, time: 0.008) nll: 68.811111 \n",
      "(GPU: 0, epoch: 6, iters: 6176, time: 0.008) nll: 74.412613 \n",
      "(GPU: 0, epoch: 6, iters: 6976, time: 0.008) nll: 86.495506 \n",
      "(GPU: 0, epoch: 6, iters: 7776, time: 0.008) nll: 77.711754 \n",
      "(GPU: 0, epoch: 6, iters: 8576, time: 0.008) nll: 80.995865 \n",
      "(GPU: 0, epoch: 6, iters: 9376, time: 0.008) nll: 63.954071 \n",
      "(GPU: 0, epoch: 6, iters: 10176, time: 0.008) nll: 71.452591 \n",
      "(GPU: 0, epoch: 6, iters: 10976, time: 0.008) nll: 66.935776 \n",
      "(GPU: 0, epoch: 6, iters: 11776, time: 0.008) nll: 65.406525 \n",
      "(GPU: 0, epoch: 6, iters: 12576, time: 0.008) nll: 54.335358 \n",
      "(GPU: 0, epoch: 6, iters: 13376, time: 0.008) nll: 65.803459 \n",
      "(GPU: 0, epoch: 6, iters: 14176, time: 0.008) nll: 67.611847 \n",
      "(GPU: 0, epoch: 6, iters: 14976, time: 0.008) nll: 81.497742 \n",
      "(GPU: 0, epoch: 6, iters: 15776, time: 0.008) nll: 73.714417 \n",
      "saving the latest model (epoch 6, total_steps 860000)\n",
      "(GPU: 0, epoch: 6, iters: 16576, time: 0.008) nll: 83.210587 \n",
      "(GPU: 0, epoch: 6, iters: 17376, time: 0.008) nll: 75.109062 \n",
      "(GPU: 0, epoch: 6, iters: 18176, time: 0.008) nll: 86.292061 \n",
      "(GPU: 0, epoch: 6, iters: 18976, time: 0.008) nll: 52.726246 \n",
      "(GPU: 0, epoch: 6, iters: 19776, time: 0.008) nll: 69.208572 \n",
      "(GPU: 0, epoch: 6, iters: 19776, time: 0.013) nll: 67.627609 \n",
      "(GPU: 0, epoch: 6, iters: 19776, time: 0.013) nll: 74.790329 \n",
      "(GPU: 0, epoch: 6, iters: 20576, time: 0.008) nll: 67.155823 \n",
      "(GPU: 0, epoch: 6, iters: 21376, time: 0.008) nll: 85.059143 \n",
      "(GPU: 0, epoch: 6, iters: 22176, time: 0.008) nll: 84.576996 \n",
      "(GPU: 0, epoch: 6, iters: 22976, time: 0.008) nll: 55.806622 \n",
      "(GPU: 0, epoch: 6, iters: 23776, time: 0.008) nll: 50.000797 \n",
      "(GPU: 0, epoch: 6, iters: 24576, time: 0.008) nll: 76.643158 \n",
      "(GPU: 0, epoch: 6, iters: 25376, time: 0.007) nll: 84.760071 \n",
      "(GPU: 0, epoch: 6, iters: 26176, time: 0.008) nll: 63.019051 \n",
      "(GPU: 0, epoch: 6, iters: 26976, time: 0.008) nll: 79.036415 \n",
      "(GPU: 0, epoch: 6, iters: 27776, time: 0.008) nll: 47.408218 \n",
      "(GPU: 0, epoch: 6, iters: 28576, time: 0.008) nll: 65.994301 \n",
      "(GPU: 0, epoch: 6, iters: 29376, time: 0.008) nll: 67.466370 \n",
      "(GPU: 0, epoch: 6, iters: 30176, time: 0.008) nll: 69.918976 \n",
      "(GPU: 0, epoch: 6, iters: 30976, time: 0.008) nll: 57.487823 \n",
      "(GPU: 0, epoch: 6, iters: 31776, time: 0.008) nll: 49.805000 \n",
      "(GPU: 0, epoch: 6, iters: 32576, time: 0.008) nll: 58.577148 \n",
      "(GPU: 0, epoch: 6, iters: 33376, time: 0.008) nll: 86.445610 \n",
      "(GPU: 0, epoch: 6, iters: 34176, time: 0.008) nll: 66.463760 \n",
      "(GPU: 0, epoch: 6, iters: 34976, time: 0.008) nll: 88.395187 \n",
      "(GPU: 0, epoch: 6, iters: 35776, time: 0.008) nll: 76.321671 \n",
      "saving the latest model (epoch 6, total_steps 880000)\n",
      "(GPU: 0, epoch: 6, iters: 36576, time: 0.008) nll: 73.126053 \n",
      "(GPU: 0, epoch: 6, iters: 37376, time: 0.008) nll: 66.236732 \n",
      "(GPU: 0, epoch: 6, iters: 38176, time: 0.007) nll: 66.759911 \n",
      "(GPU: 0, epoch: 6, iters: 38976, time: 0.008) nll: 62.111748 \n",
      "(GPU: 0, epoch: 6, iters: 39776, time: 0.007) nll: 74.029739 \n",
      "(GPU: 0, epoch: 6, iters: 40576, time: 0.008) nll: 69.659515 \n",
      "(GPU: 0, epoch: 6, iters: 41376, time: 0.008) nll: 60.332581 \n",
      "(GPU: 0, epoch: 6, iters: 42176, time: 0.008) nll: 74.002228 \n",
      "(GPU: 0, epoch: 6, iters: 42976, time: 0.008) nll: 66.846535 \n",
      "(GPU: 0, epoch: 6, iters: 43776, time: 0.008) nll: 64.380829 \n",
      "(GPU: 0, epoch: 6, iters: 44576, time: 0.008) nll: 71.239929 \n",
      "(GPU: 0, epoch: 6, iters: 45376, time: 0.008) nll: 69.946205 \n",
      "(GPU: 0, epoch: 6, iters: 46176, time: 0.008) nll: 87.629532 \n",
      "(GPU: 0, epoch: 6, iters: 46976, time: 0.008) nll: 73.734474 \n",
      "(GPU: 0, epoch: 6, iters: 47776, time: 0.007) nll: 68.034714 \n",
      "(GPU: 0, epoch: 6, iters: 48576, time: 0.008) nll: 83.549500 \n",
      "(GPU: 0, epoch: 6, iters: 49376, time: 0.007) nll: 51.475830 \n",
      "(GPU: 0, epoch: 6, iters: 50176, time: 0.008) nll: 69.769867 \n",
      "(GPU: 0, epoch: 6, iters: 50976, time: 0.008) nll: 78.204849 \n",
      "(GPU: 0, epoch: 6, iters: 51776, time: 0.008) nll: 55.772812 \n",
      "(GPU: 0, epoch: 6, iters: 52576, time: 0.008) nll: 72.882233 \n",
      "(GPU: 0, epoch: 6, iters: 53376, time: 0.008) nll: 70.275711 \n",
      "(GPU: 0, epoch: 6, iters: 54176, time: 0.008) nll: 77.929092 \n",
      "(GPU: 0, epoch: 6, iters: 54976, time: 0.008) nll: 71.826889 \n",
      "(GPU: 0, epoch: 6, iters: 55776, time: 0.008) nll: 50.279438 \n",
      "saving the latest model (epoch 6, total_steps 900000)\n",
      "(GPU: 0, epoch: 6, iters: 56576, time: 0.008) nll: 58.285667 \n",
      "(GPU: 0, epoch: 6, iters: 57376, time: 0.008) nll: 72.290207 \n",
      "(GPU: 0, epoch: 6, iters: 58176, time: 0.008) nll: 61.396435 \n",
      "(GPU: 0, epoch: 6, iters: 58976, time: 0.008) nll: 63.763206 \n",
      "(GPU: 0, epoch: 6, iters: 59776, time: 0.008) nll: 61.549778 \n",
      "(GPU: 0, epoch: 6, iters: 60576, time: 0.008) nll: 72.971069 \n",
      "(GPU: 0, epoch: 6, iters: 61376, time: 0.008) nll: 73.634331 \n",
      "(GPU: 0, epoch: 6, iters: 62176, time: 0.008) nll: 87.192596 \n",
      "(GPU: 0, epoch: 6, iters: 62976, time: 0.008) nll: 71.304314 \n",
      "(GPU: 0, epoch: 6, iters: 63776, time: 0.008) nll: 58.139267 \n",
      "(GPU: 0, epoch: 6, iters: 64576, time: 0.008) nll: 70.719055 \n",
      "(GPU: 0, epoch: 6, iters: 65376, time: 0.008) nll: 63.288971 \n",
      "(GPU: 0, epoch: 6, iters: 66176, time: 0.008) nll: 67.449844 \n",
      "(GPU: 0, epoch: 6, iters: 66976, time: 0.008) nll: 81.381691 \n",
      "(GPU: 0, epoch: 6, iters: 67776, time: 0.008) nll: 76.816246 \n",
      "(GPU: 0, epoch: 6, iters: 68576, time: 0.007) nll: 79.514023 \n",
      "(GPU: 0, epoch: 6, iters: 69376, time: 0.008) nll: 85.283783 \n",
      "(GPU: 0, epoch: 6, iters: 70176, time: 0.008) nll: 67.176003 \n",
      "(GPU: 0, epoch: 6, iters: 70976, time: 0.008) nll: 82.162781 \n",
      "(GPU: 0, epoch: 6, iters: 71776, time: 0.008) nll: 71.045258 \n",
      "(GPU: 0, epoch: 6, iters: 72576, time: 0.008) nll: 76.425255 \n",
      "(GPU: 0, epoch: 6, iters: 73376, time: 0.008) nll: 53.198959 \n",
      "(GPU: 0, epoch: 6, iters: 74176, time: 0.008) nll: 62.179947 \n",
      "(GPU: 0, epoch: 6, iters: 74976, time: 0.008) nll: 65.306992 \n",
      "(GPU: 0, epoch: 6, iters: 75776, time: 0.008) nll: 65.222404 \n",
      "saving the latest model (epoch 6, total_steps 920000)\n",
      "(GPU: 0, epoch: 6, iters: 76576, time: 0.008) nll: 56.958565 \n",
      "(GPU: 0, epoch: 6, iters: 77376, time: 0.008) nll: 90.145386 \n",
      "(GPU: 0, epoch: 6, iters: 78176, time: 0.008) nll: 54.583199 \n",
      "(GPU: 0, epoch: 6, iters: 78976, time: 0.008) nll: 68.003296 \n",
      "(GPU: 0, epoch: 6, iters: 79776, time: 0.008) nll: 73.252319 \n",
      "(GPU: 0, epoch: 6, iters: 80576, time: 0.008) nll: 70.123924 \n",
      "(GPU: 0, epoch: 6, iters: 81376, time: 0.008) nll: 72.905983 \n",
      "(GPU: 0, epoch: 6, iters: 82176, time: 0.008) nll: 63.141499 \n",
      "(GPU: 0, epoch: 6, iters: 82976, time: 0.007) nll: 53.040016 \n",
      "(GPU: 0, epoch: 6, iters: 83776, time: 0.008) nll: 67.897110 \n",
      "(GPU: 0, epoch: 6, iters: 84576, time: 0.008) nll: 46.851711 \n",
      "(GPU: 0, epoch: 6, iters: 85376, time: 0.008) nll: 63.439068 \n",
      "(GPU: 0, epoch: 6, iters: 86176, time: 0.008) nll: 53.902512 \n",
      "(GPU: 0, epoch: 6, iters: 86976, time: 0.008) nll: 77.191719 \n",
      "(GPU: 0, epoch: 6, iters: 87776, time: 0.008) nll: 98.884140 \n",
      "(GPU: 0, epoch: 6, iters: 88576, time: 0.008) nll: 55.597794 \n",
      "(GPU: 0, epoch: 6, iters: 89376, time: 0.008) nll: 62.525494 \n",
      "(GPU: 0, epoch: 6, iters: 90176, time: 0.008) nll: 63.058044 \n",
      "(GPU: 0, epoch: 6, iters: 90976, time: 0.008) nll: 78.070786 \n",
      "(GPU: 0, epoch: 6, iters: 91776, time: 0.008) nll: 59.785618 \n",
      "(GPU: 0, epoch: 6, iters: 92576, time: 0.008) nll: 61.412479 \n",
      "(GPU: 0, epoch: 6, iters: 93376, time: 0.008) nll: 80.242607 \n",
      "(GPU: 0, epoch: 6, iters: 94176, time: 0.008) nll: 71.395874 \n",
      "(GPU: 0, epoch: 6, iters: 94976, time: 0.008) nll: 44.816986 \n",
      "(GPU: 0, epoch: 6, iters: 95776, time: 0.008) nll: 80.560959 \n",
      "saving the latest model (epoch 6, total_steps 940000)\n",
      "(GPU: 0, epoch: 6, iters: 96576, time: 0.008) nll: 67.646759 \n",
      "(GPU: 0, epoch: 6, iters: 97376, time: 0.008) nll: 90.168732 \n",
      "(GPU: 0, epoch: 6, iters: 98176, time: 0.008) nll: 80.024017 \n",
      "(GPU: 0, epoch: 6, iters: 98976, time: 0.008) nll: 77.062057 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4397/4397 [20:49<00:00,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 6, iters: 99776, time: 0.008) nll: 55.739273 \n",
      "(GPU: 0, epoch: 6, iters: 100576, time: 0.008) nll: 91.060135 \n",
      "(GPU: 0, epoch: 6, iters: 101376, time: 0.008) nll: 53.266136 \n",
      "(GPU: 0, epoch: 6, iters: 102176, time: 0.008) nll: 54.056747 \n",
      "(GPU: 0, epoch: 6, iters: 102976, time: 0.008) nll: 89.260056 \n",
      "(GPU: 0, epoch: 6, iters: 103776, time: 0.008) nll: 50.688087 \n",
      "(GPU: 0, epoch: 6, iters: 104576, time: 0.008) nll: 84.128838 \n",
      "(GPU: 0, epoch: 6, iters: 105376, time: 0.008) nll: 50.332710 \n",
      "(GPU: 0, epoch: 6, iters: 106176, time: 0.008) nll: 74.683327 \n",
      "(GPU: 0, epoch: 6, iters: 106976, time: 0.008) nll: 60.457985 \n",
      "(GPU: 0, epoch: 6, iters: 107776, time: 0.008) nll: 60.349457 \n",
      "(GPU: 0, epoch: 6, iters: 108576, time: 0.008) nll: 68.765732 \n",
      "(GPU: 0, epoch: 6, iters: 109376, time: 0.008) nll: 78.573624 \n",
      "(GPU: 0, epoch: 6, iters: 110176, time: 0.008) nll: 72.928970 \n",
      "(GPU: 0, epoch: 6, iters: 110976, time: 0.008) nll: 66.749207 \n",
      "(GPU: 0, epoch: 6, iters: 111776, time: 0.008) nll: 54.184769 \n",
      "(GPU: 0, epoch: 6, iters: 112576, time: 0.008) nll: 75.354416 \n",
      "(GPU: 0, epoch: 6, iters: 113376, time: 0.008) nll: 36.375538 \n",
      "(GPU: 0, epoch: 6, iters: 114176, time: 0.008) nll: 69.826378 \n",
      "(GPU: 0, epoch: 6, iters: 114976, time: 0.008) nll: 88.317505 \n",
      "(GPU: 0, epoch: 6, iters: 115776, time: 0.008) nll: 77.549065 \n",
      "(GPU: 0, epoch: 6, iters: 115776, time: 0.013) nll: 77.066147 \n",
      "(GPU: 0, epoch: 6, iters: 115776, time: 0.013) nll: 61.606964 \n",
      "saving the latest model (epoch 6, total_steps 960000)\n",
      "(GPU: 0, epoch: 6, iters: 116576, time: 0.008) nll: 57.351830 \n",
      "(GPU: 0, epoch: 6, iters: 117376, time: 0.008) nll: 80.601845 \n",
      "(GPU: 0, epoch: 6, iters: 118176, time: 0.008) nll: 68.296043 \n",
      "(GPU: 0, epoch: 6, iters: 118976, time: 0.008) nll: 79.224518 \n",
      "(GPU: 0, epoch: 6, iters: 119776, time: 0.008) nll: 75.318726 \n",
      "(GPU: 0, epoch: 6, iters: 120576, time: 0.008) nll: 75.854736 \n",
      "(GPU: 0, epoch: 6, iters: 121376, time: 0.008) nll: 66.936111 \n",
      "(GPU: 0, epoch: 6, iters: 122176, time: 0.008) nll: 80.535065 \n",
      "(GPU: 0, epoch: 6, iters: 122976, time: 0.008) nll: 85.836220 \n",
      "(GPU: 0, epoch: 6, iters: 123776, time: 0.008) nll: 70.952469 \n",
      "(GPU: 0, epoch: 6, iters: 124576, time: 0.008) nll: 53.435303 \n",
      "(GPU: 0, epoch: 6, iters: 125376, time: 0.008) nll: 68.326530 \n",
      "(GPU: 0, epoch: 6, iters: 126176, time: 0.008) nll: 63.366077 \n",
      "(GPU: 0, epoch: 6, iters: 126976, time: 0.008) nll: 66.691475 \n",
      "(GPU: 0, epoch: 6, iters: 127776, time: 0.008) nll: 77.306183 \n",
      "(GPU: 0, epoch: 6, iters: 128576, time: 0.008) nll: 61.345585 \n",
      "(GPU: 0, epoch: 6, iters: 129376, time: 0.007) nll: 60.729012 \n",
      "(GPU: 0, epoch: 6, iters: 130176, time: 0.008) nll: 49.293274 \n",
      "(GPU: 0, epoch: 6, iters: 130976, time: 0.008) nll: 62.789574 \n",
      "(GPU: 0, epoch: 6, iters: 131776, time: 0.008) nll: 57.590546 \n",
      "(GPU: 0, epoch: 6, iters: 132576, time: 0.008) nll: 54.388649 \n",
      "(GPU: 0, epoch: 6, iters: 133376, time: 0.008) nll: 77.203506 \n",
      "(GPU: 0, epoch: 6, iters: 134176, time: 0.008) nll: 71.248840 \n",
      "(GPU: 0, epoch: 6, iters: 134976, time: 0.008) nll: 60.934559 \n",
      "(GPU: 0, epoch: 6, iters: 135776, time: 0.008) nll: 76.797989 \n",
      "saving the latest model (epoch 6, total_steps 980000)\n",
      "(GPU: 0, epoch: 6, iters: 136576, time: 0.008) nll: 77.343750 \n",
      "(GPU: 0, epoch: 6, iters: 137376, time: 0.008) nll: 73.477982 \n",
      "(GPU: 0, epoch: 6, iters: 138176, time: 0.008) nll: 49.986008 \n",
      "(GPU: 0, epoch: 6, iters: 138976, time: 0.008) nll: 57.425247 \n",
      "(GPU: 0, epoch: 6, iters: 139776, time: 0.008) nll: 72.308014 \n",
      "(GPU: 0, epoch: 6, iters: 140576, time: 0.008) nll: 63.304394 \n",
      "saving the model at the end of epoch 6, iters 984928\n",
      "([test] GPU: 0, epoch: 6) \n",
      "OrderedDict()\n",
      "[*] End of epoch 6 / 25 \t Time Taken: 1276 sec \n",
      "/cluster/54/streakfull/ADL4CV/Project/src/plz-autosdf/logs/bert2vqsc_v4-text2shape-seq-LR1e-4-network-bert-shapeset-shapeset-SmoothL1Loss\n",
      "[*] learning rate = 0.0000700\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 3095/4397 [14:40<05:38,  3.84it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 7, iters: 32, time: 0.004) nll: 76.437866 \n",
      "(GPU: 0, epoch: 7, iters: 32, time: 0.004) nll: 77.762939 \n",
      "(GPU: 0, epoch: 7, iters: 672, time: 0.008) nll: 75.044052 \n",
      "(GPU: 0, epoch: 7, iters: 1472, time: 0.008) nll: 74.091599 \n",
      "(GPU: 0, epoch: 7, iters: 2272, time: 0.008) nll: 64.328117 \n",
      "(GPU: 0, epoch: 7, iters: 3072, time: 0.008) nll: 63.916046 \n",
      "(GPU: 0, epoch: 7, iters: 3872, time: 0.008) nll: 79.596619 \n",
      "(GPU: 0, epoch: 7, iters: 4672, time: 0.008) nll: 45.445450 \n",
      "(GPU: 0, epoch: 7, iters: 5472, time: 0.008) nll: 68.402618 \n",
      "(GPU: 0, epoch: 7, iters: 6272, time: 0.008) nll: 62.026676 \n",
      "(GPU: 0, epoch: 7, iters: 7072, time: 0.008) nll: 57.554043 \n",
      "(GPU: 0, epoch: 7, iters: 7872, time: 0.008) nll: 72.186844 \n",
      "(GPU: 0, epoch: 7, iters: 8672, time: 0.008) nll: 68.268600 \n",
      "(GPU: 0, epoch: 7, iters: 9472, time: 0.008) nll: 64.541206 \n",
      "(GPU: 0, epoch: 7, iters: 10272, time: 0.008) nll: 49.732605 \n",
      "(GPU: 0, epoch: 7, iters: 11072, time: 0.008) nll: 53.951511 \n",
      "(GPU: 0, epoch: 7, iters: 11872, time: 0.008) nll: 42.425735 \n",
      "(GPU: 0, epoch: 7, iters: 12672, time: 0.008) nll: 76.604256 \n",
      "(GPU: 0, epoch: 7, iters: 13472, time: 0.008) nll: 63.294456 \n",
      "(GPU: 0, epoch: 7, iters: 14272, time: 0.008) nll: 73.835709 \n",
      "(GPU: 0, epoch: 7, iters: 15072, time: 0.008) nll: 81.684799 \n",
      "saving the latest model (epoch 7, total_steps 1000000)\n",
      "(GPU: 0, epoch: 7, iters: 15872, time: 0.008) nll: 68.663254 \n",
      "(GPU: 0, epoch: 7, iters: 16672, time: 0.008) nll: 54.774464 \n",
      "(GPU: 0, epoch: 7, iters: 17472, time: 0.008) nll: 73.317337 \n",
      "(GPU: 0, epoch: 7, iters: 18272, time: 0.008) nll: 75.416656 \n",
      "(GPU: 0, epoch: 7, iters: 19072, time: 0.008) nll: 61.789028 \n",
      "(GPU: 0, epoch: 7, iters: 19872, time: 0.008) nll: 61.693375 \n",
      "(GPU: 0, epoch: 7, iters: 20672, time: 0.008) nll: 58.546547 \n",
      "(GPU: 0, epoch: 7, iters: 21472, time: 0.008) nll: 75.593857 \n",
      "(GPU: 0, epoch: 7, iters: 22272, time: 0.008) nll: 80.076904 \n",
      "(GPU: 0, epoch: 7, iters: 23072, time: 0.008) nll: 51.259884 \n",
      "(GPU: 0, epoch: 7, iters: 23872, time: 0.008) nll: 76.650436 \n",
      "(GPU: 0, epoch: 7, iters: 24672, time: 0.008) nll: 94.246552 \n",
      "(GPU: 0, epoch: 7, iters: 25472, time: 0.008) nll: 65.247772 \n",
      "(GPU: 0, epoch: 7, iters: 26272, time: 0.008) nll: 62.465412 \n",
      "(GPU: 0, epoch: 7, iters: 27072, time: 0.008) nll: 56.024323 \n",
      "(GPU: 0, epoch: 7, iters: 27872, time: 0.008) nll: 64.409012 \n",
      "(GPU: 0, epoch: 7, iters: 28672, time: 0.008) nll: 64.895546 \n",
      "(GPU: 0, epoch: 7, iters: 29472, time: 0.008) nll: 79.948975 \n",
      "(GPU: 0, epoch: 7, iters: 30272, time: 0.008) nll: 45.286072 \n",
      "(GPU: 0, epoch: 7, iters: 31072, time: 0.008) nll: 54.425896 \n",
      "(GPU: 0, epoch: 7, iters: 31872, time: 0.008) nll: 54.078041 \n",
      "(GPU: 0, epoch: 7, iters: 32672, time: 0.008) nll: 63.916904 \n",
      "(GPU: 0, epoch: 7, iters: 33472, time: 0.008) nll: 66.091202 \n",
      "(GPU: 0, epoch: 7, iters: 34272, time: 0.008) nll: 74.685730 \n",
      "(GPU: 0, epoch: 7, iters: 35072, time: 0.008) nll: 78.533569 \n",
      "saving the latest model (epoch 7, total_steps 1020000)\n",
      "(GPU: 0, epoch: 7, iters: 35872, time: 0.008) nll: 83.752800 \n",
      "(GPU: 0, epoch: 7, iters: 36672, time: 0.008) nll: 69.539032 \n",
      "(GPU: 0, epoch: 7, iters: 37472, time: 0.008) nll: 54.434502 \n",
      "(GPU: 0, epoch: 7, iters: 38272, time: 0.008) nll: 55.353409 \n",
      "(GPU: 0, epoch: 7, iters: 39072, time: 0.008) nll: 53.473324 \n",
      "(GPU: 0, epoch: 7, iters: 39872, time: 0.008) nll: 63.906815 \n",
      "(GPU: 0, epoch: 7, iters: 40672, time: 0.008) nll: 74.428291 \n",
      "(GPU: 0, epoch: 7, iters: 41472, time: 0.008) nll: 63.004753 \n",
      "(GPU: 0, epoch: 7, iters: 42272, time: 0.008) nll: 67.166931 \n",
      "(GPU: 0, epoch: 7, iters: 43072, time: 0.008) nll: 68.808800 \n",
      "(GPU: 0, epoch: 7, iters: 43872, time: 0.008) nll: 61.075634 \n",
      "(GPU: 0, epoch: 7, iters: 44672, time: 0.008) nll: 80.183220 \n",
      "(GPU: 0, epoch: 7, iters: 45472, time: 0.008) nll: 77.409706 \n",
      "(GPU: 0, epoch: 7, iters: 46272, time: 0.008) nll: 60.353584 \n",
      "(GPU: 0, epoch: 7, iters: 47072, time: 0.008) nll: 54.818104 \n",
      "(GPU: 0, epoch: 7, iters: 47872, time: 0.008) nll: 80.256866 \n",
      "(GPU: 0, epoch: 7, iters: 48672, time: 0.008) nll: 83.742966 \n",
      "(GPU: 0, epoch: 7, iters: 49472, time: 0.008) nll: 56.996834 \n",
      "(GPU: 0, epoch: 7, iters: 50272, time: 0.008) nll: 59.693527 \n",
      "(GPU: 0, epoch: 7, iters: 51072, time: 0.008) nll: 69.845337 \n",
      "(GPU: 0, epoch: 7, iters: 51872, time: 0.008) nll: 72.022812 \n",
      "(GPU: 0, epoch: 7, iters: 52672, time: 0.008) nll: 91.954666 \n",
      "(GPU: 0, epoch: 7, iters: 53472, time: 0.008) nll: 67.987625 \n",
      "(GPU: 0, epoch: 7, iters: 54272, time: 0.008) nll: 80.647507 \n",
      "(GPU: 0, epoch: 7, iters: 55072, time: 0.008) nll: 68.491226 \n",
      "saving the latest model (epoch 7, total_steps 1040000)\n",
      "(GPU: 0, epoch: 7, iters: 55872, time: 0.008) nll: 90.459106 \n",
      "(GPU: 0, epoch: 7, iters: 56672, time: 0.008) nll: 66.983833 \n",
      "(GPU: 0, epoch: 7, iters: 57472, time: 0.008) nll: 85.431122 \n",
      "(GPU: 0, epoch: 7, iters: 58272, time: 0.008) nll: 65.544540 \n",
      "(GPU: 0, epoch: 7, iters: 59072, time: 0.008) nll: 59.069515 \n",
      "(GPU: 0, epoch: 7, iters: 59872, time: 0.007) nll: 51.288010 \n",
      "(GPU: 0, epoch: 7, iters: 60672, time: 0.008) nll: 81.825401 \n",
      "(GPU: 0, epoch: 7, iters: 61472, time: 0.008) nll: 65.199959 \n",
      "(GPU: 0, epoch: 7, iters: 62272, time: 0.008) nll: 79.926826 \n",
      "(GPU: 0, epoch: 7, iters: 63072, time: 0.008) nll: 74.231705 \n",
      "(GPU: 0, epoch: 7, iters: 63872, time: 0.008) nll: 73.219864 \n",
      "(GPU: 0, epoch: 7, iters: 64672, time: 0.008) nll: 62.218185 \n",
      "(GPU: 0, epoch: 7, iters: 65472, time: 0.008) nll: 55.261795 \n",
      "(GPU: 0, epoch: 7, iters: 66272, time: 0.008) nll: 59.021980 \n",
      "(GPU: 0, epoch: 7, iters: 67072, time: 0.008) nll: 74.698616 \n",
      "(GPU: 0, epoch: 7, iters: 67872, time: 0.008) nll: 71.705444 \n",
      "(GPU: 0, epoch: 7, iters: 68672, time: 0.008) nll: 51.664856 \n",
      "(GPU: 0, epoch: 7, iters: 69472, time: 0.007) nll: 68.668121 \n",
      "(GPU: 0, epoch: 7, iters: 70272, time: 0.008) nll: 58.587540 \n",
      "(GPU: 0, epoch: 7, iters: 71072, time: 0.008) nll: 81.108612 \n",
      "(GPU: 0, epoch: 7, iters: 71072, time: 0.013) nll: 80.796471 \n",
      "(GPU: 0, epoch: 7, iters: 71072, time: 0.013) nll: 89.625366 \n",
      "(GPU: 0, epoch: 7, iters: 71872, time: 0.008) nll: 73.229904 \n",
      "(GPU: 0, epoch: 7, iters: 72672, time: 0.008) nll: 88.376175 \n",
      "(GPU: 0, epoch: 7, iters: 73472, time: 0.008) nll: 61.785301 \n",
      "(GPU: 0, epoch: 7, iters: 74272, time: 0.008) nll: 81.090797 \n",
      "(GPU: 0, epoch: 7, iters: 75072, time: 0.008) nll: 77.302124 \n",
      "saving the latest model (epoch 7, total_steps 1060000)\n",
      "(GPU: 0, epoch: 7, iters: 75872, time: 0.008) nll: 80.323410 \n",
      "(GPU: 0, epoch: 7, iters: 76672, time: 0.008) nll: 72.646095 \n",
      "(GPU: 0, epoch: 7, iters: 77472, time: 0.008) nll: 77.162689 \n",
      "(GPU: 0, epoch: 7, iters: 78272, time: 0.008) nll: 84.209076 \n",
      "(GPU: 0, epoch: 7, iters: 79072, time: 0.008) nll: 74.357864 \n",
      "(GPU: 0, epoch: 7, iters: 79872, time: 0.008) nll: 97.869102 \n",
      "(GPU: 0, epoch: 7, iters: 80672, time: 0.008) nll: 93.449905 \n",
      "(GPU: 0, epoch: 7, iters: 81472, time: 0.008) nll: 86.880302 \n",
      "(GPU: 0, epoch: 7, iters: 82272, time: 0.008) nll: 78.871338 \n",
      "(GPU: 0, epoch: 7, iters: 83072, time: 0.008) nll: 48.431042 \n",
      "(GPU: 0, epoch: 7, iters: 83872, time: 0.007) nll: 92.214531 \n",
      "(GPU: 0, epoch: 7, iters: 84672, time: 0.008) nll: 55.850498 \n",
      "(GPU: 0, epoch: 7, iters: 85472, time: 0.008) nll: 94.728905 \n",
      "(GPU: 0, epoch: 7, iters: 86272, time: 0.008) nll: 94.363281 \n",
      "(GPU: 0, epoch: 7, iters: 87072, time: 0.008) nll: 58.596085 \n",
      "(GPU: 0, epoch: 7, iters: 87872, time: 0.008) nll: 89.381874 \n",
      "(GPU: 0, epoch: 7, iters: 88672, time: 0.008) nll: 82.585274 \n",
      "(GPU: 0, epoch: 7, iters: 89472, time: 0.008) nll: 73.402138 \n",
      "(GPU: 0, epoch: 7, iters: 90272, time: 0.008) nll: 49.617310 \n",
      "(GPU: 0, epoch: 7, iters: 91072, time: 0.008) nll: 103.853287 \n",
      "(GPU: 0, epoch: 7, iters: 91872, time: 0.008) nll: 111.239861 \n",
      "(GPU: 0, epoch: 7, iters: 92672, time: 0.009) nll: 76.219681 \n",
      "(GPU: 0, epoch: 7, iters: 93472, time: 0.008) nll: 75.876450 \n",
      "(GPU: 0, epoch: 7, iters: 94272, time: 0.008) nll: 90.631134 \n",
      "(GPU: 0, epoch: 7, iters: 95072, time: 0.008) nll: 59.055908 \n",
      "saving the latest model (epoch 7, total_steps 1080000)\n",
      "(GPU: 0, epoch: 7, iters: 95872, time: 0.008) nll: 46.684937 \n",
      "(GPU: 0, epoch: 7, iters: 96672, time: 0.008) nll: 63.557224 \n",
      "(GPU: 0, epoch: 7, iters: 97472, time: 0.008) nll: 49.545258 \n",
      "(GPU: 0, epoch: 7, iters: 98272, time: 0.008) nll: 66.603546 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4397/4397 [20:50<00:00,  3.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 7, iters: 99072, time: 0.008) nll: 68.121628 \n",
      "(GPU: 0, epoch: 7, iters: 99872, time: 0.008) nll: 75.362846 \n",
      "(GPU: 0, epoch: 7, iters: 100672, time: 0.008) nll: 61.332840 \n",
      "(GPU: 0, epoch: 7, iters: 101472, time: 0.008) nll: 60.897652 \n",
      "(GPU: 0, epoch: 7, iters: 102272, time: 0.008) nll: 74.973358 \n",
      "(GPU: 0, epoch: 7, iters: 103072, time: 0.008) nll: 64.265709 \n",
      "(GPU: 0, epoch: 7, iters: 103872, time: 0.008) nll: 77.730827 \n",
      "(GPU: 0, epoch: 7, iters: 104672, time: 0.008) nll: 83.424088 \n",
      "(GPU: 0, epoch: 7, iters: 105472, time: 0.008) nll: 88.767426 \n",
      "(GPU: 0, epoch: 7, iters: 106272, time: 0.008) nll: 62.532669 \n",
      "(GPU: 0, epoch: 7, iters: 107072, time: 0.008) nll: 62.427883 \n",
      "(GPU: 0, epoch: 7, iters: 107872, time: 0.008) nll: 66.486145 \n",
      "(GPU: 0, epoch: 7, iters: 108672, time: 0.008) nll: 78.920837 \n",
      "(GPU: 0, epoch: 7, iters: 109472, time: 0.008) nll: 67.541687 \n",
      "(GPU: 0, epoch: 7, iters: 110272, time: 0.008) nll: 69.972031 \n",
      "(GPU: 0, epoch: 7, iters: 111072, time: 0.008) nll: 61.518486 \n",
      "(GPU: 0, epoch: 7, iters: 111872, time: 0.008) nll: 56.728909 \n",
      "(GPU: 0, epoch: 7, iters: 112672, time: 0.008) nll: 58.228611 \n",
      "(GPU: 0, epoch: 7, iters: 113472, time: 0.008) nll: 93.148376 \n",
      "(GPU: 0, epoch: 7, iters: 114272, time: 0.008) nll: 80.770325 \n",
      "(GPU: 0, epoch: 7, iters: 115072, time: 0.008) nll: 87.917603 \n",
      "saving the latest model (epoch 7, total_steps 1100000)\n",
      "(GPU: 0, epoch: 7, iters: 115872, time: 0.008) nll: 49.223274 \n",
      "(GPU: 0, epoch: 7, iters: 116672, time: 0.008) nll: 64.360603 \n",
      "(GPU: 0, epoch: 7, iters: 117472, time: 0.008) nll: 87.545319 \n",
      "(GPU: 0, epoch: 7, iters: 118272, time: 0.008) nll: 62.758057 \n",
      "(GPU: 0, epoch: 7, iters: 119072, time: 0.008) nll: 64.751465 \n",
      "(GPU: 0, epoch: 7, iters: 119872, time: 0.008) nll: 70.725601 \n",
      "(GPU: 0, epoch: 7, iters: 120672, time: 0.008) nll: 89.999260 \n",
      "(GPU: 0, epoch: 7, iters: 121472, time: 0.008) nll: 60.726925 \n",
      "(GPU: 0, epoch: 7, iters: 122272, time: 0.008) nll: 47.458862 \n",
      "(GPU: 0, epoch: 7, iters: 123072, time: 0.008) nll: 48.086876 \n",
      "(GPU: 0, epoch: 7, iters: 123872, time: 0.008) nll: 63.998955 \n",
      "(GPU: 0, epoch: 7, iters: 124672, time: 0.008) nll: 63.113789 \n",
      "(GPU: 0, epoch: 7, iters: 125472, time: 0.008) nll: 99.598289 \n",
      "(GPU: 0, epoch: 7, iters: 126272, time: 0.008) nll: 68.234421 \n",
      "(GPU: 0, epoch: 7, iters: 127072, time: 0.008) nll: 62.829075 \n",
      "(GPU: 0, epoch: 7, iters: 127872, time: 0.008) nll: 66.116699 \n",
      "(GPU: 0, epoch: 7, iters: 128672, time: 0.007) nll: 55.261925 \n",
      "(GPU: 0, epoch: 7, iters: 129472, time: 0.008) nll: 91.796234 \n",
      "(GPU: 0, epoch: 7, iters: 130272, time: 0.008) nll: 85.983002 \n",
      "(GPU: 0, epoch: 7, iters: 131072, time: 0.008) nll: 72.736839 \n",
      "(GPU: 0, epoch: 7, iters: 131872, time: 0.008) nll: 80.237572 \n",
      "(GPU: 0, epoch: 7, iters: 132672, time: 0.008) nll: 78.564552 \n",
      "(GPU: 0, epoch: 7, iters: 133472, time: 0.008) nll: 108.253517 \n",
      "(GPU: 0, epoch: 7, iters: 134272, time: 0.008) nll: 70.728500 \n",
      "(GPU: 0, epoch: 7, iters: 135072, time: 0.008) nll: 58.784706 \n",
      "saving the latest model (epoch 7, total_steps 1120000)\n",
      "(GPU: 0, epoch: 7, iters: 135872, time: 0.009) nll: 80.283127 \n",
      "(GPU: 0, epoch: 7, iters: 136672, time: 0.008) nll: 56.967255 \n",
      "(GPU: 0, epoch: 7, iters: 137472, time: 0.008) nll: 76.532211 \n",
      "(GPU: 0, epoch: 7, iters: 138272, time: 0.008) nll: 67.748451 \n",
      "(GPU: 0, epoch: 7, iters: 139072, time: 0.008) nll: 63.353855 \n",
      "(GPU: 0, epoch: 7, iters: 139872, time: 0.008) nll: 72.592171 \n",
      "(GPU: 0, epoch: 7, iters: 140672, time: 0.008) nll: 59.769424 \n",
      "[*] End of epoch 7 / 25 \t Time Taken: 1250 sec \n",
      "/cluster/54/streakfull/ADL4CV/Project/src/plz-autosdf/logs/bert2vqsc_v4-text2shape-seq-LR1e-4-network-bert-shapeset-shapeset-SmoothL1Loss\n",
      "[*] learning rate = 0.0000800\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 3098/4397 [14:42<05:38,  3.84it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 8, iters: 32, time: 0.004) nll: 54.027847 \n",
      "(GPU: 0, epoch: 8, iters: 32, time: 0.004) nll: 46.965443 \n",
      "(GPU: 0, epoch: 8, iters: 768, time: 0.008) nll: 85.081757 \n",
      "(GPU: 0, epoch: 8, iters: 1568, time: 0.008) nll: 98.663353 \n",
      "(GPU: 0, epoch: 8, iters: 2368, time: 0.008) nll: 84.164383 \n",
      "(GPU: 0, epoch: 8, iters: 3168, time: 0.008) nll: 72.009857 \n",
      "(GPU: 0, epoch: 8, iters: 3968, time: 0.008) nll: 72.004593 \n",
      "(GPU: 0, epoch: 8, iters: 4768, time: 0.008) nll: 58.856178 \n",
      "(GPU: 0, epoch: 8, iters: 5568, time: 0.008) nll: 88.266449 \n",
      "(GPU: 0, epoch: 8, iters: 6368, time: 0.007) nll: 79.378128 \n",
      "(GPU: 0, epoch: 8, iters: 7168, time: 0.008) nll: 76.845566 \n",
      "(GPU: 0, epoch: 8, iters: 7968, time: 0.008) nll: 88.157127 \n",
      "(GPU: 0, epoch: 8, iters: 8768, time: 0.008) nll: 80.007294 \n",
      "(GPU: 0, epoch: 8, iters: 9568, time: 0.008) nll: 78.260147 \n",
      "(GPU: 0, epoch: 8, iters: 10368, time: 0.008) nll: 70.709274 \n",
      "(GPU: 0, epoch: 8, iters: 11168, time: 0.008) nll: 61.141441 \n",
      "(GPU: 0, epoch: 8, iters: 11968, time: 0.008) nll: 63.240509 \n",
      "(GPU: 0, epoch: 8, iters: 12768, time: 0.008) nll: 63.648365 \n",
      "(GPU: 0, epoch: 8, iters: 13568, time: 0.008) nll: 88.925018 \n",
      "(GPU: 0, epoch: 8, iters: 14368, time: 0.008) nll: 62.138329 \n",
      "saving the latest model (epoch 8, total_steps 1140000)\n",
      "(GPU: 0, epoch: 8, iters: 15168, time: 0.008) nll: 65.101654 \n",
      "(GPU: 0, epoch: 8, iters: 15968, time: 0.008) nll: 80.155960 \n",
      "(GPU: 0, epoch: 8, iters: 16768, time: 0.008) nll: 74.162537 \n",
      "(GPU: 0, epoch: 8, iters: 17568, time: 0.008) nll: 72.168015 \n",
      "(GPU: 0, epoch: 8, iters: 18368, time: 0.008) nll: 69.704308 \n",
      "(GPU: 0, epoch: 8, iters: 19168, time: 0.008) nll: 73.979004 \n",
      "(GPU: 0, epoch: 8, iters: 19968, time: 0.008) nll: 40.339302 \n",
      "(GPU: 0, epoch: 8, iters: 20768, time: 0.008) nll: 69.973793 \n",
      "(GPU: 0, epoch: 8, iters: 21568, time: 0.008) nll: 43.423645 \n",
      "(GPU: 0, epoch: 8, iters: 22368, time: 0.008) nll: 63.359543 \n",
      "(GPU: 0, epoch: 8, iters: 23168, time: 0.008) nll: 71.344788 \n",
      "(GPU: 0, epoch: 8, iters: 23968, time: 0.008) nll: 50.309082 \n",
      "(GPU: 0, epoch: 8, iters: 24768, time: 0.008) nll: 66.618767 \n",
      "(GPU: 0, epoch: 8, iters: 25568, time: 0.008) nll: 63.391888 \n",
      "(GPU: 0, epoch: 8, iters: 26368, time: 0.008) nll: 58.283150 \n",
      "(GPU: 0, epoch: 8, iters: 26368, time: 0.013) nll: 57.244637 \n",
      "(GPU: 0, epoch: 8, iters: 26368, time: 0.013) nll: 70.995720 \n",
      "(GPU: 0, epoch: 8, iters: 27168, time: 0.008) nll: 76.533913 \n",
      "(GPU: 0, epoch: 8, iters: 27968, time: 0.008) nll: 81.139259 \n",
      "(GPU: 0, epoch: 8, iters: 28768, time: 0.008) nll: 74.541283 \n",
      "(GPU: 0, epoch: 8, iters: 29568, time: 0.008) nll: 54.526718 \n",
      "(GPU: 0, epoch: 8, iters: 30368, time: 0.007) nll: 68.893059 \n",
      "(GPU: 0, epoch: 8, iters: 31168, time: 0.008) nll: 62.423157 \n",
      "(GPU: 0, epoch: 8, iters: 31968, time: 0.008) nll: 58.704407 \n",
      "(GPU: 0, epoch: 8, iters: 32768, time: 0.008) nll: 79.422867 \n",
      "(GPU: 0, epoch: 8, iters: 33568, time: 0.008) nll: 58.044113 \n",
      "(GPU: 0, epoch: 8, iters: 34368, time: 0.008) nll: 67.162590 \n",
      "saving the latest model (epoch 8, total_steps 1160000)\n",
      "(GPU: 0, epoch: 8, iters: 35168, time: 0.008) nll: 47.973736 \n",
      "(GPU: 0, epoch: 8, iters: 35968, time: 0.008) nll: 67.506889 \n",
      "(GPU: 0, epoch: 8, iters: 36768, time: 0.008) nll: 72.085236 \n",
      "(GPU: 0, epoch: 8, iters: 37568, time: 0.008) nll: 53.805313 \n",
      "(GPU: 0, epoch: 8, iters: 38368, time: 0.008) nll: 75.322510 \n",
      "(GPU: 0, epoch: 8, iters: 39168, time: 0.008) nll: 79.646729 \n",
      "(GPU: 0, epoch: 8, iters: 39968, time: 0.008) nll: 82.014297 \n",
      "(GPU: 0, epoch: 8, iters: 40768, time: 0.008) nll: 46.204048 \n",
      "(GPU: 0, epoch: 8, iters: 41568, time: 0.008) nll: 65.675278 \n",
      "(GPU: 0, epoch: 8, iters: 42368, time: 0.008) nll: 64.177513 \n",
      "(GPU: 0, epoch: 8, iters: 43168, time: 0.008) nll: 57.845779 \n",
      "(GPU: 0, epoch: 8, iters: 43968, time: 0.008) nll: 51.889702 \n",
      "(GPU: 0, epoch: 8, iters: 44768, time: 0.008) nll: 59.897842 \n",
      "(GPU: 0, epoch: 8, iters: 45568, time: 0.008) nll: 97.581505 \n",
      "(GPU: 0, epoch: 8, iters: 46368, time: 0.008) nll: 90.372749 \n",
      "(GPU: 0, epoch: 8, iters: 47168, time: 0.008) nll: 70.548279 \n",
      "(GPU: 0, epoch: 8, iters: 47968, time: 0.007) nll: 80.812782 \n",
      "(GPU: 0, epoch: 8, iters: 48768, time: 0.008) nll: 94.161942 \n",
      "(GPU: 0, epoch: 8, iters: 49568, time: 0.008) nll: 87.173035 \n",
      "(GPU: 0, epoch: 8, iters: 50368, time: 0.008) nll: 51.812187 \n",
      "(GPU: 0, epoch: 8, iters: 51168, time: 0.008) nll: 76.283005 \n",
      "(GPU: 0, epoch: 8, iters: 51968, time: 0.008) nll: 67.919678 \n",
      "(GPU: 0, epoch: 8, iters: 52768, time: 0.008) nll: 71.739990 \n",
      "(GPU: 0, epoch: 8, iters: 53568, time: 0.008) nll: 50.066422 \n",
      "(GPU: 0, epoch: 8, iters: 54368, time: 0.008) nll: 64.541939 \n",
      "saving the latest model (epoch 8, total_steps 1180000)\n",
      "(GPU: 0, epoch: 8, iters: 55168, time: 0.008) nll: 80.857315 \n",
      "(GPU: 0, epoch: 8, iters: 55968, time: 0.008) nll: 86.831833 \n",
      "(GPU: 0, epoch: 8, iters: 56768, time: 0.008) nll: 72.674789 \n",
      "(GPU: 0, epoch: 8, iters: 57568, time: 0.008) nll: 61.783813 \n",
      "(GPU: 0, epoch: 8, iters: 58368, time: 0.008) nll: 65.070129 \n",
      "(GPU: 0, epoch: 8, iters: 59168, time: 0.008) nll: 74.113068 \n",
      "(GPU: 0, epoch: 8, iters: 59968, time: 0.008) nll: 65.949997 \n",
      "(GPU: 0, epoch: 8, iters: 60768, time: 0.008) nll: 59.904552 \n",
      "(GPU: 0, epoch: 8, iters: 61568, time: 0.008) nll: 102.019745 \n",
      "(GPU: 0, epoch: 8, iters: 62368, time: 0.008) nll: 75.346191 \n",
      "(GPU: 0, epoch: 8, iters: 63168, time: 0.008) nll: 80.330902 \n",
      "(GPU: 0, epoch: 8, iters: 63968, time: 0.008) nll: 58.136265 \n",
      "(GPU: 0, epoch: 8, iters: 64768, time: 0.008) nll: 49.042122 \n",
      "(GPU: 0, epoch: 8, iters: 65568, time: 0.008) nll: 81.929916 \n",
      "(GPU: 0, epoch: 8, iters: 66368, time: 0.008) nll: 71.724533 \n",
      "(GPU: 0, epoch: 8, iters: 67168, time: 0.008) nll: 64.847778 \n",
      "(GPU: 0, epoch: 8, iters: 67968, time: 0.008) nll: 81.892502 \n",
      "(GPU: 0, epoch: 8, iters: 68768, time: 0.008) nll: 69.682755 \n",
      "(GPU: 0, epoch: 8, iters: 69568, time: 0.008) nll: 47.271561 \n",
      "(GPU: 0, epoch: 8, iters: 70368, time: 0.008) nll: 66.815201 \n",
      "(GPU: 0, epoch: 8, iters: 71168, time: 0.008) nll: 74.269821 \n",
      "(GPU: 0, epoch: 8, iters: 71968, time: 0.008) nll: 59.529030 \n",
      "(GPU: 0, epoch: 8, iters: 72768, time: 0.008) nll: 41.689510 \n",
      "(GPU: 0, epoch: 8, iters: 73568, time: 0.008) nll: 76.520432 \n",
      "(GPU: 0, epoch: 8, iters: 74368, time: 0.008) nll: 48.430508 \n",
      "saving the latest model (epoch 8, total_steps 1200000)\n",
      "(GPU: 0, epoch: 8, iters: 75168, time: 0.007) nll: 79.280174 \n",
      "(GPU: 0, epoch: 8, iters: 75968, time: 0.008) nll: 82.385445 \n",
      "(GPU: 0, epoch: 8, iters: 76768, time: 0.008) nll: 67.324646 \n",
      "(GPU: 0, epoch: 8, iters: 77568, time: 0.008) nll: 63.936718 \n",
      "(GPU: 0, epoch: 8, iters: 78368, time: 0.008) nll: 70.763412 \n",
      "(GPU: 0, epoch: 8, iters: 79168, time: 0.008) nll: 62.830238 \n",
      "(GPU: 0, epoch: 8, iters: 79968, time: 0.008) nll: 96.536209 \n",
      "(GPU: 0, epoch: 8, iters: 80768, time: 0.008) nll: 92.776962 \n",
      "(GPU: 0, epoch: 8, iters: 81568, time: 0.007) nll: 68.733047 \n",
      "(GPU: 0, epoch: 8, iters: 82368, time: 0.008) nll: 62.279129 \n",
      "(GPU: 0, epoch: 8, iters: 83168, time: 0.008) nll: 74.092613 \n",
      "(GPU: 0, epoch: 8, iters: 83968, time: 0.008) nll: 61.688911 \n",
      "(GPU: 0, epoch: 8, iters: 84768, time: 0.008) nll: 81.060181 \n",
      "(GPU: 0, epoch: 8, iters: 85568, time: 0.008) nll: 57.851051 \n",
      "(GPU: 0, epoch: 8, iters: 86368, time: 0.008) nll: 68.329041 \n",
      "(GPU: 0, epoch: 8, iters: 87168, time: 0.008) nll: 75.802521 \n",
      "(GPU: 0, epoch: 8, iters: 87968, time: 0.008) nll: 59.294563 \n",
      "(GPU: 0, epoch: 8, iters: 88768, time: 0.008) nll: 73.177864 \n",
      "(GPU: 0, epoch: 8, iters: 89568, time: 0.008) nll: 72.173782 \n",
      "(GPU: 0, epoch: 8, iters: 90368, time: 0.008) nll: 47.405846 \n",
      "(GPU: 0, epoch: 8, iters: 91168, time: 0.008) nll: 91.845436 \n",
      "(GPU: 0, epoch: 8, iters: 91968, time: 0.008) nll: 79.891464 \n",
      "(GPU: 0, epoch: 8, iters: 92768, time: 0.008) nll: 75.748672 \n",
      "(GPU: 0, epoch: 8, iters: 93568, time: 0.008) nll: 66.375793 \n",
      "(GPU: 0, epoch: 8, iters: 94368, time: 0.008) nll: 70.136269 \n",
      "saving the latest model (epoch 8, total_steps 1220000)\n",
      "(GPU: 0, epoch: 8, iters: 95168, time: 0.008) nll: 43.838329 \n",
      "(GPU: 0, epoch: 8, iters: 95968, time: 0.008) nll: 49.511715 \n",
      "(GPU: 0, epoch: 8, iters: 96768, time: 0.008) nll: 58.743790 \n",
      "(GPU: 0, epoch: 8, iters: 97568, time: 0.008) nll: 62.441185 \n",
      "(GPU: 0, epoch: 8, iters: 98368, time: 0.008) nll: 76.539085 \n",
      "(GPU: 0, epoch: 8, iters: 99168, time: 0.008) nll: 47.937325 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4397/4397 [20:51<00:00,  3.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(GPU: 0, epoch: 8, iters: 99968, time: 0.008) nll: 82.518997 \n",
      "(GPU: 0, epoch: 8, iters: 100768, time: 0.008) nll: 67.105179 \n",
      "(GPU: 0, epoch: 8, iters: 101568, time: 0.008) nll: 79.727867 \n",
      "(GPU: 0, epoch: 8, iters: 102368, time: 0.008) nll: 81.553040 \n",
      "(GPU: 0, epoch: 8, iters: 103168, time: 0.008) nll: 48.122810 \n",
      "(GPU: 0, epoch: 8, iters: 103968, time: 0.008) nll: 62.268078 \n",
      "(GPU: 0, epoch: 8, iters: 104768, time: 0.008) nll: 91.509094 \n",
      "(GPU: 0, epoch: 8, iters: 105568, time: 0.008) nll: 47.222328 \n",
      "(GPU: 0, epoch: 8, iters: 106368, time: 0.008) nll: 71.534561 \n",
      "(GPU: 0, epoch: 8, iters: 107168, time: 0.008) nll: 56.166737 \n",
      "(GPU: 0, epoch: 8, iters: 107968, time: 0.008) nll: 80.006500 \n",
      "(GPU: 0, epoch: 8, iters: 108768, time: 0.008) nll: 63.806313 \n",
      "(GPU: 0, epoch: 8, iters: 109568, time: 0.008) nll: 59.149818 \n",
      "(GPU: 0, epoch: 8, iters: 110368, time: 0.008) nll: 58.891529 \n",
      "(GPU: 0, epoch: 8, iters: 111168, time: 0.008) nll: 73.880936 \n",
      "(GPU: 0, epoch: 8, iters: 111968, time: 0.008) nll: 45.209053 \n",
      "(GPU: 0, epoch: 8, iters: 112768, time: 0.008) nll: 80.797493 \n",
      "(GPU: 0, epoch: 8, iters: 113568, time: 0.008) nll: 54.841484 \n",
      "(GPU: 0, epoch: 8, iters: 114368, time: 0.008) nll: 71.158707 \n",
      "saving the latest model (epoch 8, total_steps 1240000)\n",
      "(GPU: 0, epoch: 8, iters: 115168, time: 0.008) nll: 55.763847 \n",
      "(GPU: 0, epoch: 8, iters: 115968, time: 0.008) nll: 89.084221 \n",
      "(GPU: 0, epoch: 8, iters: 116768, time: 0.008) nll: 64.972885 \n",
      "(GPU: 0, epoch: 8, iters: 117568, time: 0.008) nll: 66.911476 \n",
      "(GPU: 0, epoch: 8, iters: 118368, time: 0.008) nll: 57.793514 \n",
      "(GPU: 0, epoch: 8, iters: 119168, time: 0.008) nll: 69.467384 \n",
      "(GPU: 0, epoch: 8, iters: 119968, time: 0.008) nll: 29.103874 \n",
      "(GPU: 0, epoch: 8, iters: 120768, time: 0.008) nll: 54.136814 \n",
      "(GPU: 0, epoch: 8, iters: 121568, time: 0.008) nll: 56.171356 \n",
      "(GPU: 0, epoch: 8, iters: 122368, time: 0.008) nll: 50.191231 \n",
      "(GPU: 0, epoch: 8, iters: 122368, time: 0.013) nll: 49.426773 \n",
      "(GPU: 0, epoch: 8, iters: 122368, time: 0.013) nll: 80.515991 \n",
      "(GPU: 0, epoch: 8, iters: 123168, time: 0.008) nll: 75.988976 \n",
      "(GPU: 0, epoch: 8, iters: 123968, time: 0.008) nll: 63.764763 \n",
      "(GPU: 0, epoch: 8, iters: 124768, time: 0.008) nll: 83.772812 \n",
      "(GPU: 0, epoch: 8, iters: 125568, time: 0.008) nll: 85.240517 \n",
      "(GPU: 0, epoch: 8, iters: 126368, time: 0.008) nll: 53.190086 \n",
      "(GPU: 0, epoch: 8, iters: 127168, time: 0.008) nll: 69.224136 \n",
      "(GPU: 0, epoch: 8, iters: 127968, time: 0.008) nll: 68.849937 \n",
      "(GPU: 0, epoch: 8, iters: 128768, time: 0.008) nll: 70.045799 \n",
      "(GPU: 0, epoch: 8, iters: 129568, time: 0.008) nll: 70.568085 \n",
      "(GPU: 0, epoch: 8, iters: 130368, time: 0.008) nll: 71.090378 \n",
      "(GPU: 0, epoch: 8, iters: 131168, time: 0.008) nll: 74.551285 \n",
      "(GPU: 0, epoch: 8, iters: 131968, time: 0.008) nll: 108.827858 \n",
      "(GPU: 0, epoch: 8, iters: 132768, time: 0.008) nll: 53.522629 \n",
      "(GPU: 0, epoch: 8, iters: 133568, time: 0.008) nll: 82.704651 \n",
      "(GPU: 0, epoch: 8, iters: 134368, time: 0.008) nll: 82.542450 \n",
      "saving the latest model (epoch 8, total_steps 1260000)\n",
      "(GPU: 0, epoch: 8, iters: 135168, time: 0.008) nll: 88.539520 \n",
      "(GPU: 0, epoch: 8, iters: 135968, time: 0.008) nll: 84.673157 \n",
      "(GPU: 0, epoch: 8, iters: 136768, time: 0.008) nll: 47.511993 \n",
      "(GPU: 0, epoch: 8, iters: 137568, time: 0.008) nll: 57.911926 \n",
      "(GPU: 0, epoch: 8, iters: 138368, time: 0.008) nll: 78.091980 \n",
      "(GPU: 0, epoch: 8, iters: 139168, time: 0.008) nll: 74.215424 \n",
      "(GPU: 0, epoch: 8, iters: 139968, time: 0.008) nll: 84.980049 \n",
      "[*] End of epoch 8 / 25 \t Time Taken: 1251 sec \n",
      "/cluster/54/streakfull/ADL4CV/Project/src/plz-autosdf/logs/bert2vqsc_v4-text2shape-seq-LR1e-4-network-bert-shapeset-shapeset-SmoothL1Loss\n",
      "[*] learning rate = 0.0000900\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 3101/4397 [14:46<05:39,  3.82it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 9, iters: 32, time: 0.004) nll: 55.664581 \n",
      "(GPU: 0, epoch: 9, iters: 32, time: 0.004) nll: 75.908218 \n",
      "(GPU: 0, epoch: 9, iters: 64, time: 0.003) nll: 62.924843 \n",
      "(GPU: 0, epoch: 9, iters: 864, time: 0.008) nll: 41.603165 \n",
      "(GPU: 0, epoch: 9, iters: 1664, time: 0.008) nll: 73.139084 \n",
      "(GPU: 0, epoch: 9, iters: 2464, time: 0.008) nll: 79.716362 \n",
      "(GPU: 0, epoch: 9, iters: 3264, time: 0.008) nll: 62.056767 \n",
      "(GPU: 0, epoch: 9, iters: 4064, time: 0.008) nll: 68.980377 \n",
      "(GPU: 0, epoch: 9, iters: 4864, time: 0.008) nll: 83.717300 \n",
      "(GPU: 0, epoch: 9, iters: 5664, time: 0.008) nll: 80.790421 \n",
      "(GPU: 0, epoch: 9, iters: 6464, time: 0.008) nll: 63.151855 \n",
      "(GPU: 0, epoch: 9, iters: 7264, time: 0.008) nll: 44.845169 \n",
      "(GPU: 0, epoch: 9, iters: 8064, time: 0.008) nll: 50.713516 \n",
      "(GPU: 0, epoch: 9, iters: 8864, time: 0.008) nll: 77.646805 \n",
      "(GPU: 0, epoch: 9, iters: 9664, time: 0.008) nll: 67.651184 \n",
      "(GPU: 0, epoch: 9, iters: 10464, time: 0.008) nll: 60.123688 \n",
      "(GPU: 0, epoch: 9, iters: 11264, time: 0.008) nll: 73.474251 \n",
      "(GPU: 0, epoch: 9, iters: 12064, time: 0.008) nll: 51.586678 \n",
      "(GPU: 0, epoch: 9, iters: 12864, time: 0.008) nll: 73.992233 \n",
      "(GPU: 0, epoch: 9, iters: 13664, time: 0.007) nll: 77.131058 \n",
      "saving the latest model (epoch 9, total_steps 1280000)\n",
      "(GPU: 0, epoch: 9, iters: 14464, time: 0.008) nll: 62.473087 \n",
      "(GPU: 0, epoch: 9, iters: 15264, time: 0.008) nll: 70.149033 \n",
      "(GPU: 0, epoch: 9, iters: 16064, time: 0.008) nll: 40.684692 \n",
      "(GPU: 0, epoch: 9, iters: 16864, time: 0.008) nll: 74.358673 \n",
      "(GPU: 0, epoch: 9, iters: 17664, time: 0.008) nll: 91.007080 \n",
      "(GPU: 0, epoch: 9, iters: 18464, time: 0.007) nll: 85.915169 \n",
      "(GPU: 0, epoch: 9, iters: 19264, time: 0.008) nll: 77.793335 \n",
      "(GPU: 0, epoch: 9, iters: 20064, time: 0.008) nll: 89.941925 \n",
      "(GPU: 0, epoch: 9, iters: 20864, time: 0.008) nll: 49.386337 \n",
      "(GPU: 0, epoch: 9, iters: 21664, time: 0.008) nll: 82.361008 \n",
      "(GPU: 0, epoch: 9, iters: 22464, time: 0.008) nll: 77.122803 \n",
      "(GPU: 0, epoch: 9, iters: 23264, time: 0.008) nll: 86.502434 \n",
      "(GPU: 0, epoch: 9, iters: 24064, time: 0.008) nll: 62.988434 \n",
      "(GPU: 0, epoch: 9, iters: 24864, time: 0.008) nll: 70.199745 \n",
      "(GPU: 0, epoch: 9, iters: 25664, time: 0.008) nll: 77.392349 \n",
      "(GPU: 0, epoch: 9, iters: 26464, time: 0.008) nll: 81.100983 \n",
      "(GPU: 0, epoch: 9, iters: 27264, time: 0.008) nll: 66.961594 \n",
      "(GPU: 0, epoch: 9, iters: 28064, time: 0.008) nll: 64.194443 \n",
      "(GPU: 0, epoch: 9, iters: 28864, time: 0.008) nll: 39.659492 \n",
      "(GPU: 0, epoch: 9, iters: 29664, time: 0.008) nll: 63.442696 \n",
      "(GPU: 0, epoch: 9, iters: 30464, time: 0.008) nll: 72.712212 \n",
      "(GPU: 0, epoch: 9, iters: 31264, time: 0.008) nll: 69.501801 \n",
      "(GPU: 0, epoch: 9, iters: 32064, time: 0.008) nll: 63.107105 \n",
      "(GPU: 0, epoch: 9, iters: 32864, time: 0.008) nll: 62.560013 \n",
      "(GPU: 0, epoch: 9, iters: 33664, time: 0.008) nll: 68.514061 \n",
      "saving the latest model (epoch 9, total_steps 1300000)\n",
      "(GPU: 0, epoch: 9, iters: 34464, time: 0.008) nll: 54.344414 \n",
      "(GPU: 0, epoch: 9, iters: 35264, time: 0.008) nll: 79.357727 \n",
      "(GPU: 0, epoch: 9, iters: 36064, time: 0.008) nll: 66.595764 \n",
      "(GPU: 0, epoch: 9, iters: 36864, time: 0.008) nll: 48.808300 \n",
      "(GPU: 0, epoch: 9, iters: 37664, time: 0.008) nll: 87.354988 \n",
      "(GPU: 0, epoch: 9, iters: 38464, time: 0.008) nll: 79.189056 \n",
      "(GPU: 0, epoch: 9, iters: 39264, time: 0.008) nll: 62.191990 \n",
      "(GPU: 0, epoch: 9, iters: 40064, time: 0.008) nll: 55.443810 \n",
      "(GPU: 0, epoch: 9, iters: 40864, time: 0.008) nll: 61.784092 \n",
      "(GPU: 0, epoch: 9, iters: 41664, time: 0.008) nll: 62.747070 \n",
      "(GPU: 0, epoch: 9, iters: 42464, time: 0.008) nll: 58.391235 \n",
      "(GPU: 0, epoch: 9, iters: 43264, time: 0.008) nll: 91.695503 \n",
      "(GPU: 0, epoch: 9, iters: 44064, time: 0.008) nll: 83.013672 \n",
      "(GPU: 0, epoch: 9, iters: 44864, time: 0.008) nll: 75.674202 \n",
      "(GPU: 0, epoch: 9, iters: 45664, time: 0.008) nll: 60.528843 \n",
      "(GPU: 0, epoch: 9, iters: 46464, time: 0.008) nll: 64.844406 \n",
      "(GPU: 0, epoch: 9, iters: 47264, time: 0.007) nll: 73.046585 \n",
      "(GPU: 0, epoch: 9, iters: 48064, time: 0.008) nll: 42.297230 \n",
      "(GPU: 0, epoch: 9, iters: 48864, time: 0.008) nll: 66.795120 \n",
      "(GPU: 0, epoch: 9, iters: 49664, time: 0.008) nll: 74.313927 \n",
      "(GPU: 0, epoch: 9, iters: 50464, time: 0.008) nll: 69.692169 \n",
      "(GPU: 0, epoch: 9, iters: 51264, time: 0.008) nll: 50.320961 \n",
      "(GPU: 0, epoch: 9, iters: 52064, time: 0.008) nll: 60.638527 \n",
      "(GPU: 0, epoch: 9, iters: 52864, time: 0.008) nll: 71.823746 \n",
      "(GPU: 0, epoch: 9, iters: 53664, time: 0.008) nll: 44.344376 \n",
      "saving the latest model (epoch 9, total_steps 1320000)\n",
      "(GPU: 0, epoch: 9, iters: 54464, time: 0.008) nll: 66.345154 \n",
      "(GPU: 0, epoch: 9, iters: 55264, time: 0.008) nll: 77.358910 \n",
      "(GPU: 0, epoch: 9, iters: 56064, time: 0.008) nll: 69.362373 \n",
      "(GPU: 0, epoch: 9, iters: 56864, time: 0.008) nll: 86.420151 \n",
      "(GPU: 0, epoch: 9, iters: 57664, time: 0.008) nll: 72.814896 \n",
      "(GPU: 0, epoch: 9, iters: 58464, time: 0.008) nll: 67.498100 \n",
      "(GPU: 0, epoch: 9, iters: 59264, time: 0.008) nll: 57.283661 \n",
      "(GPU: 0, epoch: 9, iters: 60064, time: 0.008) nll: 56.935326 \n",
      "(GPU: 0, epoch: 9, iters: 60864, time: 0.008) nll: 80.154808 \n",
      "(GPU: 0, epoch: 9, iters: 61664, time: 0.008) nll: 85.445847 \n",
      "(GPU: 0, epoch: 9, iters: 62464, time: 0.008) nll: 87.323929 \n",
      "(GPU: 0, epoch: 9, iters: 63264, time: 0.008) nll: 65.556732 \n",
      "(GPU: 0, epoch: 9, iters: 64064, time: 0.008) nll: 75.726654 \n",
      "(GPU: 0, epoch: 9, iters: 64864, time: 0.008) nll: 58.640526 \n",
      "(GPU: 0, epoch: 9, iters: 65664, time: 0.008) nll: 70.041847 \n",
      "(GPU: 0, epoch: 9, iters: 66464, time: 0.008) nll: 58.578186 \n",
      "(GPU: 0, epoch: 9, iters: 67264, time: 0.008) nll: 69.338516 \n",
      "(GPU: 0, epoch: 9, iters: 68064, time: 0.008) nll: 87.350845 \n",
      "(GPU: 0, epoch: 9, iters: 68864, time: 0.008) nll: 62.063484 \n",
      "(GPU: 0, epoch: 9, iters: 69664, time: 0.008) nll: 83.189896 \n",
      "(GPU: 0, epoch: 9, iters: 70464, time: 0.008) nll: 56.096489 \n",
      "(GPU: 0, epoch: 9, iters: 71264, time: 0.008) nll: 57.615692 \n",
      "(GPU: 0, epoch: 9, iters: 72064, time: 0.008) nll: 73.258347 \n",
      "(GPU: 0, epoch: 9, iters: 72864, time: 0.008) nll: 85.960197 \n",
      "(GPU: 0, epoch: 9, iters: 73664, time: 0.008) nll: 79.683731 \n",
      "saving the latest model (epoch 9, total_steps 1340000)\n",
      "(GPU: 0, epoch: 9, iters: 74464, time: 0.008) nll: 65.644676 \n",
      "(GPU: 0, epoch: 9, iters: 75264, time: 0.008) nll: 93.440964 \n",
      "(GPU: 0, epoch: 9, iters: 76064, time: 0.008) nll: 82.335236 \n",
      "(GPU: 0, epoch: 9, iters: 76864, time: 0.008) nll: 64.313400 \n",
      "(GPU: 0, epoch: 9, iters: 77664, time: 0.008) nll: 59.940769 \n",
      "(GPU: 0, epoch: 9, iters: 77664, time: 0.013) nll: 59.095802 \n",
      "(GPU: 0, epoch: 9, iters: 77664, time: 0.013) nll: 53.671738 \n",
      "(GPU: 0, epoch: 9, iters: 78464, time: 0.008) nll: 67.342667 \n",
      "(GPU: 0, epoch: 9, iters: 79264, time: 0.008) nll: 68.708450 \n",
      "(GPU: 0, epoch: 9, iters: 80064, time: 0.008) nll: 55.620598 \n",
      "(GPU: 0, epoch: 9, iters: 80864, time: 0.008) nll: 52.290527 \n",
      "(GPU: 0, epoch: 9, iters: 81664, time: 0.008) nll: 57.963764 \n",
      "(GPU: 0, epoch: 9, iters: 82464, time: 0.008) nll: 56.269341 \n",
      "(GPU: 0, epoch: 9, iters: 83264, time: 0.008) nll: 54.830193 \n",
      "(GPU: 0, epoch: 9, iters: 84064, time: 0.008) nll: 62.625313 \n",
      "(GPU: 0, epoch: 9, iters: 84864, time: 0.008) nll: 68.703247 \n",
      "(GPU: 0, epoch: 9, iters: 85664, time: 0.007) nll: 86.985962 \n",
      "(GPU: 0, epoch: 9, iters: 86464, time: 0.008) nll: 52.790733 \n",
      "(GPU: 0, epoch: 9, iters: 87264, time: 0.008) nll: 67.609406 \n",
      "(GPU: 0, epoch: 9, iters: 88064, time: 0.008) nll: 86.147720 \n",
      "(GPU: 0, epoch: 9, iters: 88864, time: 0.008) nll: 99.946068 \n",
      "(GPU: 0, epoch: 9, iters: 89664, time: 0.008) nll: 53.705086 \n",
      "(GPU: 0, epoch: 9, iters: 90464, time: 0.008) nll: 67.530380 \n",
      "(GPU: 0, epoch: 9, iters: 91264, time: 0.008) nll: 76.050644 \n",
      "(GPU: 0, epoch: 9, iters: 92064, time: 0.008) nll: 58.595116 \n",
      "(GPU: 0, epoch: 9, iters: 92864, time: 0.009) nll: 68.514748 \n",
      "(GPU: 0, epoch: 9, iters: 93664, time: 0.008) nll: 61.392315 \n",
      "saving the latest model (epoch 9, total_steps 1360000)\n",
      "(GPU: 0, epoch: 9, iters: 94464, time: 0.008) nll: 73.468307 \n",
      "(GPU: 0, epoch: 9, iters: 95264, time: 0.008) nll: 84.975037 \n",
      "(GPU: 0, epoch: 9, iters: 96064, time: 0.008) nll: 86.094612 \n",
      "(GPU: 0, epoch: 9, iters: 96864, time: 0.008) nll: 48.976990 \n",
      "(GPU: 0, epoch: 9, iters: 97664, time: 0.008) nll: 52.654938 \n",
      "(GPU: 0, epoch: 9, iters: 98464, time: 0.008) nll: 51.593071 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4397/4397 [20:56<00:00,  3.50it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 9, iters: 99264, time: 0.008) nll: 71.652809 \n",
      "(GPU: 0, epoch: 9, iters: 100064, time: 0.008) nll: 54.209106 \n",
      "(GPU: 0, epoch: 9, iters: 100864, time: 0.008) nll: 57.103645 \n",
      "(GPU: 0, epoch: 9, iters: 101664, time: 0.008) nll: 69.849510 \n",
      "(GPU: 0, epoch: 9, iters: 102464, time: 0.008) nll: 82.847054 \n",
      "(GPU: 0, epoch: 9, iters: 103264, time: 0.008) nll: 70.172920 \n",
      "(GPU: 0, epoch: 9, iters: 104064, time: 0.008) nll: 75.433685 \n",
      "(GPU: 0, epoch: 9, iters: 104864, time: 0.008) nll: 80.392303 \n",
      "(GPU: 0, epoch: 9, iters: 105664, time: 0.008) nll: 93.788589 \n",
      "(GPU: 0, epoch: 9, iters: 106464, time: 0.008) nll: 64.364876 \n",
      "(GPU: 0, epoch: 9, iters: 107264, time: 0.008) nll: 57.019066 \n",
      "(GPU: 0, epoch: 9, iters: 108064, time: 0.008) nll: 74.025169 \n",
      "(GPU: 0, epoch: 9, iters: 108864, time: 0.008) nll: 68.654480 \n",
      "(GPU: 0, epoch: 9, iters: 109664, time: 0.008) nll: 67.263687 \n",
      "(GPU: 0, epoch: 9, iters: 110464, time: 0.008) nll: 58.446690 \n",
      "(GPU: 0, epoch: 9, iters: 111264, time: 0.007) nll: 59.803207 \n",
      "(GPU: 0, epoch: 9, iters: 112064, time: 0.008) nll: 87.923813 \n",
      "(GPU: 0, epoch: 9, iters: 112864, time: 0.008) nll: 66.509354 \n",
      "(GPU: 0, epoch: 9, iters: 113664, time: 0.008) nll: 67.518478 \n",
      "saving the latest model (epoch 9, total_steps 1380000)\n",
      "(GPU: 0, epoch: 9, iters: 114464, time: 0.008) nll: 63.558529 \n",
      "(GPU: 0, epoch: 9, iters: 115264, time: 0.008) nll: 55.868633 \n",
      "(GPU: 0, epoch: 9, iters: 116064, time: 0.008) nll: 66.817413 \n",
      "(GPU: 0, epoch: 9, iters: 116864, time: 0.008) nll: 65.673340 \n",
      "(GPU: 0, epoch: 9, iters: 117664, time: 0.007) nll: 73.107597 \n",
      "(GPU: 0, epoch: 9, iters: 118464, time: 0.008) nll: 47.384800 \n",
      "(GPU: 0, epoch: 9, iters: 119264, time: 0.008) nll: 71.459427 \n",
      "(GPU: 0, epoch: 9, iters: 120064, time: 0.008) nll: 49.013878 \n",
      "(GPU: 0, epoch: 9, iters: 120864, time: 0.008) nll: 48.984558 \n",
      "(GPU: 0, epoch: 9, iters: 121664, time: 0.008) nll: 60.470520 \n",
      "(GPU: 0, epoch: 9, iters: 122464, time: 0.008) nll: 78.690933 \n",
      "(GPU: 0, epoch: 9, iters: 123264, time: 0.008) nll: 72.685570 \n",
      "(GPU: 0, epoch: 9, iters: 124064, time: 0.008) nll: 73.022842 \n",
      "(GPU: 0, epoch: 9, iters: 124864, time: 0.008) nll: 70.578888 \n",
      "(GPU: 0, epoch: 9, iters: 125664, time: 0.008) nll: 68.470886 \n",
      "(GPU: 0, epoch: 9, iters: 126464, time: 0.008) nll: 63.626488 \n",
      "(GPU: 0, epoch: 9, iters: 127264, time: 0.008) nll: 64.307022 \n",
      "(GPU: 0, epoch: 9, iters: 128064, time: 0.008) nll: 66.463264 \n",
      "(GPU: 0, epoch: 9, iters: 128864, time: 0.008) nll: 65.016205 \n",
      "(GPU: 0, epoch: 9, iters: 129664, time: 0.008) nll: 104.659645 \n",
      "(GPU: 0, epoch: 9, iters: 130464, time: 0.008) nll: 52.840706 \n",
      "(GPU: 0, epoch: 9, iters: 131264, time: 0.008) nll: 80.185028 \n",
      "(GPU: 0, epoch: 9, iters: 132064, time: 0.008) nll: 80.254463 \n",
      "(GPU: 0, epoch: 9, iters: 132864, time: 0.008) nll: 58.443985 \n",
      "(GPU: 0, epoch: 9, iters: 133664, time: 0.008) nll: 53.951004 \n",
      "saving the latest model (epoch 9, total_steps 1400000)\n",
      "(GPU: 0, epoch: 9, iters: 134464, time: 0.008) nll: 61.346859 \n",
      "(GPU: 0, epoch: 9, iters: 135264, time: 0.008) nll: 52.393726 \n",
      "(GPU: 0, epoch: 9, iters: 136064, time: 0.008) nll: 61.148571 \n",
      "(GPU: 0, epoch: 9, iters: 136864, time: 0.008) nll: 72.867325 \n",
      "(GPU: 0, epoch: 9, iters: 137664, time: 0.008) nll: 63.451218 \n",
      "(GPU: 0, epoch: 9, iters: 138464, time: 0.008) nll: 58.886395 \n",
      "(GPU: 0, epoch: 9, iters: 139264, time: 0.008) nll: 65.309013 \n",
      "(GPU: 0, epoch: 9, iters: 140064, time: 0.008) nll: 78.541252 \n",
      "saving the model at the end of epoch 9, iters 1407040\n",
      "([test] GPU: 0, epoch: 9) \n",
      "OrderedDict()\n",
      "[*] End of epoch 9 / 25 \t Time Taken: 1285 sec \n",
      "/cluster/54/streakfull/ADL4CV/Project/src/plz-autosdf/logs/bert2vqsc_v4-text2shape-seq-LR1e-4-network-bert-shapeset-shapeset-SmoothL1Loss\n",
      "[*] learning rate = 0.0001000\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 3029/4397 [14:24<05:57,  3.83it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 10, iters: 32, time: 0.004) nll: 77.100143 \n",
      "(GPU: 0, epoch: 10, iters: 32, time: 0.004) nll: 72.160995 \n",
      "(GPU: 0, epoch: 10, iters: 160, time: 0.007) nll: 54.562138 \n",
      "(GPU: 0, epoch: 10, iters: 960, time: 0.008) nll: 74.251755 \n",
      "(GPU: 0, epoch: 10, iters: 1760, time: 0.008) nll: 82.979004 \n",
      "(GPU: 0, epoch: 10, iters: 2560, time: 0.008) nll: 52.731350 \n",
      "(GPU: 0, epoch: 10, iters: 3360, time: 0.008) nll: 96.494827 \n",
      "(GPU: 0, epoch: 10, iters: 4160, time: 0.008) nll: 57.299366 \n",
      "(GPU: 0, epoch: 10, iters: 4960, time: 0.008) nll: 41.481247 \n",
      "(GPU: 0, epoch: 10, iters: 5760, time: 0.008) nll: 61.709465 \n",
      "(GPU: 0, epoch: 10, iters: 6560, time: 0.008) nll: 70.187363 \n",
      "(GPU: 0, epoch: 10, iters: 7360, time: 0.008) nll: 70.811569 \n",
      "(GPU: 0, epoch: 10, iters: 8160, time: 0.007) nll: 94.697701 \n",
      "(GPU: 0, epoch: 10, iters: 8960, time: 0.008) nll: 71.870575 \n",
      "(GPU: 0, epoch: 10, iters: 9760, time: 0.007) nll: 74.246712 \n",
      "(GPU: 0, epoch: 10, iters: 10560, time: 0.008) nll: 61.502068 \n",
      "(GPU: 0, epoch: 10, iters: 11360, time: 0.008) nll: 72.359512 \n",
      "(GPU: 0, epoch: 10, iters: 12160, time: 0.008) nll: 88.412514 \n",
      "(GPU: 0, epoch: 10, iters: 12960, time: 0.008) nll: 63.059700 \n",
      "saving the latest model (epoch 10, total_steps 1420000)\n",
      "(GPU: 0, epoch: 10, iters: 13760, time: 0.008) nll: 49.627464 \n",
      "(GPU: 0, epoch: 10, iters: 14560, time: 0.008) nll: 73.589615 \n",
      "(GPU: 0, epoch: 10, iters: 15360, time: 0.008) nll: 77.188019 \n",
      "(GPU: 0, epoch: 10, iters: 16160, time: 0.008) nll: 83.561493 \n",
      "(GPU: 0, epoch: 10, iters: 16960, time: 0.008) nll: 97.402451 \n",
      "(GPU: 0, epoch: 10, iters: 17760, time: 0.008) nll: 86.741959 \n",
      "(GPU: 0, epoch: 10, iters: 18560, time: 0.008) nll: 71.001549 \n",
      "(GPU: 0, epoch: 10, iters: 19360, time: 0.008) nll: 77.634972 \n",
      "(GPU: 0, epoch: 10, iters: 20160, time: 0.008) nll: 73.224670 \n",
      "(GPU: 0, epoch: 10, iters: 20960, time: 0.007) nll: 82.317322 \n",
      "(GPU: 0, epoch: 10, iters: 21760, time: 0.008) nll: 73.622635 \n",
      "(GPU: 0, epoch: 10, iters: 22560, time: 0.008) nll: 63.170292 \n",
      "(GPU: 0, epoch: 10, iters: 23360, time: 0.008) nll: 66.725174 \n",
      "(GPU: 0, epoch: 10, iters: 24160, time: 0.008) nll: 82.972168 \n",
      "(GPU: 0, epoch: 10, iters: 24960, time: 0.008) nll: 60.662735 \n",
      "(GPU: 0, epoch: 10, iters: 25760, time: 0.008) nll: 62.422985 \n",
      "(GPU: 0, epoch: 10, iters: 26560, time: 0.008) nll: 71.122322 \n",
      "(GPU: 0, epoch: 10, iters: 27360, time: 0.008) nll: 74.419548 \n",
      "(GPU: 0, epoch: 10, iters: 28160, time: 0.008) nll: 71.916718 \n",
      "(GPU: 0, epoch: 10, iters: 28960, time: 0.008) nll: 55.675301 \n",
      "(GPU: 0, epoch: 10, iters: 29760, time: 0.008) nll: 38.128544 \n",
      "(GPU: 0, epoch: 10, iters: 30560, time: 0.008) nll: 64.200035 \n",
      "(GPU: 0, epoch: 10, iters: 31360, time: 0.008) nll: 82.315811 \n",
      "(GPU: 0, epoch: 10, iters: 32160, time: 0.008) nll: 104.694595 \n",
      "(GPU: 0, epoch: 10, iters: 32960, time: 0.008) nll: 68.813309 \n",
      "(GPU: 0, epoch: 10, iters: 32960, time: 0.013) nll: 67.257294 \n",
      "(GPU: 0, epoch: 10, iters: 32960, time: 0.013) nll: 36.571098 \n",
      "saving the latest model (epoch 10, total_steps 1440000)\n",
      "(GPU: 0, epoch: 10, iters: 33760, time: 0.008) nll: 74.169228 \n",
      "(GPU: 0, epoch: 10, iters: 34560, time: 0.008) nll: 58.326172 \n",
      "(GPU: 0, epoch: 10, iters: 35360, time: 0.008) nll: 45.981735 \n",
      "(GPU: 0, epoch: 10, iters: 36160, time: 0.008) nll: 67.951935 \n",
      "(GPU: 0, epoch: 10, iters: 36960, time: 0.008) nll: 78.146362 \n",
      "(GPU: 0, epoch: 10, iters: 37760, time: 0.008) nll: 79.934319 \n",
      "(GPU: 0, epoch: 10, iters: 38560, time: 0.008) nll: 65.754608 \n",
      "(GPU: 0, epoch: 10, iters: 39360, time: 0.008) nll: 56.256641 \n",
      "(GPU: 0, epoch: 10, iters: 40160, time: 0.008) nll: 56.724354 \n",
      "(GPU: 0, epoch: 10, iters: 40960, time: 0.008) nll: 65.266426 \n",
      "(GPU: 0, epoch: 10, iters: 41760, time: 0.008) nll: 66.252945 \n",
      "(GPU: 0, epoch: 10, iters: 42560, time: 0.008) nll: 82.394577 \n",
      "(GPU: 0, epoch: 10, iters: 43360, time: 0.008) nll: 75.911804 \n",
      "(GPU: 0, epoch: 10, iters: 44160, time: 0.008) nll: 52.318279 \n",
      "(GPU: 0, epoch: 10, iters: 44960, time: 0.008) nll: 65.622063 \n",
      "(GPU: 0, epoch: 10, iters: 45760, time: 0.008) nll: 64.430984 \n",
      "(GPU: 0, epoch: 10, iters: 46560, time: 0.008) nll: 34.967930 \n",
      "(GPU: 0, epoch: 10, iters: 47360, time: 0.008) nll: 73.590439 \n",
      "(GPU: 0, epoch: 10, iters: 48160, time: 0.007) nll: 46.022903 \n",
      "(GPU: 0, epoch: 10, iters: 48960, time: 0.008) nll: 68.381500 \n",
      "(GPU: 0, epoch: 10, iters: 49760, time: 0.008) nll: 93.884995 \n",
      "(GPU: 0, epoch: 10, iters: 50560, time: 0.008) nll: 66.332657 \n",
      "(GPU: 0, epoch: 10, iters: 51360, time: 0.008) nll: 81.859367 \n",
      "(GPU: 0, epoch: 10, iters: 52160, time: 0.008) nll: 57.828346 \n",
      "(GPU: 0, epoch: 10, iters: 52960, time: 0.008) nll: 51.886715 \n",
      "saving the latest model (epoch 10, total_steps 1460000)\n",
      "(GPU: 0, epoch: 10, iters: 53760, time: 0.008) nll: 76.570740 \n",
      "(GPU: 0, epoch: 10, iters: 54560, time: 0.008) nll: 89.398621 \n",
      "(GPU: 0, epoch: 10, iters: 55360, time: 0.008) nll: 70.126549 \n",
      "(GPU: 0, epoch: 10, iters: 56160, time: 0.008) nll: 45.093788 \n",
      "(GPU: 0, epoch: 10, iters: 56960, time: 0.008) nll: 72.953506 \n",
      "(GPU: 0, epoch: 10, iters: 57760, time: 0.008) nll: 72.621780 \n",
      "(GPU: 0, epoch: 10, iters: 58560, time: 0.008) nll: 65.810158 \n",
      "(GPU: 0, epoch: 10, iters: 59360, time: 0.008) nll: 66.729004 \n",
      "(GPU: 0, epoch: 10, iters: 60160, time: 0.008) nll: 87.628235 \n",
      "(GPU: 0, epoch: 10, iters: 60960, time: 0.008) nll: 79.027222 \n",
      "(GPU: 0, epoch: 10, iters: 61760, time: 0.008) nll: 71.640793 \n",
      "(GPU: 0, epoch: 10, iters: 62560, time: 0.008) nll: 70.478394 \n",
      "(GPU: 0, epoch: 10, iters: 63360, time: 0.008) nll: 83.561188 \n",
      "(GPU: 0, epoch: 10, iters: 64160, time: 0.008) nll: 60.361229 \n",
      "(GPU: 0, epoch: 10, iters: 64960, time: 0.008) nll: 66.797623 \n",
      "(GPU: 0, epoch: 10, iters: 65760, time: 0.008) nll: 58.565094 \n",
      "(GPU: 0, epoch: 10, iters: 66560, time: 0.008) nll: 64.323090 \n",
      "(GPU: 0, epoch: 10, iters: 67360, time: 0.008) nll: 72.805107 \n",
      "(GPU: 0, epoch: 10, iters: 68160, time: 0.008) nll: 49.892990 \n",
      "(GPU: 0, epoch: 10, iters: 68960, time: 0.008) nll: 71.803482 \n",
      "(GPU: 0, epoch: 10, iters: 69760, time: 0.008) nll: 68.782021 \n",
      "(GPU: 0, epoch: 10, iters: 70560, time: 0.008) nll: 71.710960 \n",
      "(GPU: 0, epoch: 10, iters: 71360, time: 0.008) nll: 90.991364 \n",
      "(GPU: 0, epoch: 10, iters: 72160, time: 0.008) nll: 98.544983 \n",
      "(GPU: 0, epoch: 10, iters: 72960, time: 0.008) nll: 69.742096 \n",
      "saving the latest model (epoch 10, total_steps 1480000)\n",
      "(GPU: 0, epoch: 10, iters: 73760, time: 0.008) nll: 77.857712 \n",
      "(GPU: 0, epoch: 10, iters: 74560, time: 0.008) nll: 83.167236 \n",
      "(GPU: 0, epoch: 10, iters: 75360, time: 0.008) nll: 71.443726 \n",
      "(GPU: 0, epoch: 10, iters: 76160, time: 0.008) nll: 53.421852 \n",
      "(GPU: 0, epoch: 10, iters: 76960, time: 0.008) nll: 87.959740 \n",
      "(GPU: 0, epoch: 10, iters: 77760, time: 0.008) nll: 63.129974 \n",
      "(GPU: 0, epoch: 10, iters: 78560, time: 0.008) nll: 70.005753 \n",
      "(GPU: 0, epoch: 10, iters: 79360, time: 0.008) nll: 44.353874 \n",
      "(GPU: 0, epoch: 10, iters: 80160, time: 0.008) nll: 50.975891 \n",
      "(GPU: 0, epoch: 10, iters: 80960, time: 0.008) nll: 67.386131 \n",
      "(GPU: 0, epoch: 10, iters: 81760, time: 0.008) nll: 74.166588 \n",
      "(GPU: 0, epoch: 10, iters: 82560, time: 0.008) nll: 61.812626 \n",
      "(GPU: 0, epoch: 10, iters: 83360, time: 0.008) nll: 61.216686 \n",
      "(GPU: 0, epoch: 10, iters: 84160, time: 0.008) nll: 66.933777 \n",
      "(GPU: 0, epoch: 10, iters: 84960, time: 0.008) nll: 62.751568 \n",
      "(GPU: 0, epoch: 10, iters: 85760, time: 0.008) nll: 65.413208 \n",
      "(GPU: 0, epoch: 10, iters: 86560, time: 0.008) nll: 41.349495 \n",
      "(GPU: 0, epoch: 10, iters: 87360, time: 0.008) nll: 86.528831 \n",
      "(GPU: 0, epoch: 10, iters: 88160, time: 0.008) nll: 56.970566 \n",
      "(GPU: 0, epoch: 10, iters: 88960, time: 0.008) nll: 62.751339 \n",
      "(GPU: 0, epoch: 10, iters: 89760, time: 0.008) nll: 85.330368 \n",
      "(GPU: 0, epoch: 10, iters: 90560, time: 0.008) nll: 71.099525 \n",
      "(GPU: 0, epoch: 10, iters: 91360, time: 0.007) nll: 80.894989 \n",
      "(GPU: 0, epoch: 10, iters: 92160, time: 0.008) nll: 48.086407 \n",
      "(GPU: 0, epoch: 10, iters: 92960, time: 0.007) nll: 63.003403 \n",
      "saving the latest model (epoch 10, total_steps 1500000)\n",
      "(GPU: 0, epoch: 10, iters: 93760, time: 0.008) nll: 66.285950 \n",
      "(GPU: 0, epoch: 10, iters: 94560, time: 0.008) nll: 73.793762 \n",
      "(GPU: 0, epoch: 10, iters: 95360, time: 0.008) nll: 59.023220 \n",
      "(GPU: 0, epoch: 10, iters: 96160, time: 0.008) nll: 61.556484 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4397/4397 [20:52<00:00,  3.51it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 10, iters: 96960, time: 0.008) nll: 59.871140 \n",
      "(GPU: 0, epoch: 10, iters: 97760, time: 0.008) nll: 73.406677 \n",
      "(GPU: 0, epoch: 10, iters: 98560, time: 0.008) nll: 72.353683 \n",
      "(GPU: 0, epoch: 10, iters: 99360, time: 0.008) nll: 81.947189 \n",
      "(GPU: 0, epoch: 10, iters: 100160, time: 0.008) nll: 62.316624 \n",
      "(GPU: 0, epoch: 10, iters: 100960, time: 0.008) nll: 52.504986 \n",
      "(GPU: 0, epoch: 10, iters: 101760, time: 0.008) nll: 60.055786 \n",
      "(GPU: 0, epoch: 10, iters: 102560, time: 0.008) nll: 64.199158 \n",
      "(GPU: 0, epoch: 10, iters: 103360, time: 0.008) nll: 76.741333 \n",
      "(GPU: 0, epoch: 10, iters: 104160, time: 0.008) nll: 79.580795 \n",
      "(GPU: 0, epoch: 10, iters: 104960, time: 0.008) nll: 87.124275 \n",
      "(GPU: 0, epoch: 10, iters: 105760, time: 0.008) nll: 69.345428 \n",
      "(GPU: 0, epoch: 10, iters: 106560, time: 0.008) nll: 71.723473 \n",
      "(GPU: 0, epoch: 10, iters: 107360, time: 0.008) nll: 70.774841 \n",
      "(GPU: 0, epoch: 10, iters: 108160, time: 0.008) nll: 54.584572 \n",
      "(GPU: 0, epoch: 10, iters: 108960, time: 0.008) nll: 88.424103 \n",
      "(GPU: 0, epoch: 10, iters: 109760, time: 0.008) nll: 75.485222 \n",
      "(GPU: 0, epoch: 10, iters: 110560, time: 0.008) nll: 66.593231 \n",
      "(GPU: 0, epoch: 10, iters: 111360, time: 0.008) nll: 64.602280 \n",
      "(GPU: 0, epoch: 10, iters: 112160, time: 0.007) nll: 28.108067 \n",
      "(GPU: 0, epoch: 10, iters: 112960, time: 0.008) nll: 91.425186 \n",
      "saving the latest model (epoch 10, total_steps 1520000)\n",
      "(GPU: 0, epoch: 10, iters: 113760, time: 0.008) nll: 71.555756 \n",
      "(GPU: 0, epoch: 10, iters: 114560, time: 0.008) nll: 45.582870 \n",
      "(GPU: 0, epoch: 10, iters: 115360, time: 0.008) nll: 51.102974 \n",
      "(GPU: 0, epoch: 10, iters: 116160, time: 0.008) nll: 59.985088 \n",
      "(GPU: 0, epoch: 10, iters: 116960, time: 0.008) nll: 61.865219 \n",
      "(GPU: 0, epoch: 10, iters: 117760, time: 0.008) nll: 32.839703 \n",
      "(GPU: 0, epoch: 10, iters: 118560, time: 0.008) nll: 66.264191 \n",
      "(GPU: 0, epoch: 10, iters: 119360, time: 0.008) nll: 75.442490 \n",
      "(GPU: 0, epoch: 10, iters: 120160, time: 0.008) nll: 88.913879 \n",
      "(GPU: 0, epoch: 10, iters: 120960, time: 0.008) nll: 63.027283 \n",
      "(GPU: 0, epoch: 10, iters: 121760, time: 0.008) nll: 77.031052 \n",
      "(GPU: 0, epoch: 10, iters: 122560, time: 0.008) nll: 57.235352 \n",
      "(GPU: 0, epoch: 10, iters: 123360, time: 0.008) nll: 66.554825 \n",
      "(GPU: 0, epoch: 10, iters: 124160, time: 0.008) nll: 79.263939 \n",
      "(GPU: 0, epoch: 10, iters: 124960, time: 0.008) nll: 81.680000 \n",
      "(GPU: 0, epoch: 10, iters: 125760, time: 0.008) nll: 69.178612 \n",
      "(GPU: 0, epoch: 10, iters: 126560, time: 0.008) nll: 57.254311 \n",
      "(GPU: 0, epoch: 10, iters: 127360, time: 0.008) nll: 79.074356 \n",
      "(GPU: 0, epoch: 10, iters: 128160, time: 0.008) nll: 45.355385 \n",
      "(GPU: 0, epoch: 10, iters: 128960, time: 0.008) nll: 87.050705 \n",
      "(GPU: 0, epoch: 10, iters: 128960, time: 0.013) nll: 85.822289 \n",
      "(GPU: 0, epoch: 10, iters: 128960, time: 0.013) nll: 71.744179 \n",
      "(GPU: 0, epoch: 10, iters: 129760, time: 0.008) nll: 81.400085 \n",
      "(GPU: 0, epoch: 10, iters: 130560, time: 0.008) nll: 43.497108 \n",
      "(GPU: 0, epoch: 10, iters: 131360, time: 0.008) nll: 48.867325 \n",
      "(GPU: 0, epoch: 10, iters: 132160, time: 0.008) nll: 72.254395 \n",
      "(GPU: 0, epoch: 10, iters: 132960, time: 0.008) nll: 84.853668 \n",
      "saving the latest model (epoch 10, total_steps 1540000)\n",
      "(GPU: 0, epoch: 10, iters: 133760, time: 0.008) nll: 61.353527 \n",
      "(GPU: 0, epoch: 10, iters: 134560, time: 0.007) nll: 82.451508 \n",
      "(GPU: 0, epoch: 10, iters: 135360, time: 0.008) nll: 75.245804 \n",
      "(GPU: 0, epoch: 10, iters: 136160, time: 0.008) nll: 63.982327 \n",
      "(GPU: 0, epoch: 10, iters: 136960, time: 0.008) nll: 58.149574 \n",
      "(GPU: 0, epoch: 10, iters: 137760, time: 0.008) nll: 73.055847 \n",
      "(GPU: 0, epoch: 10, iters: 138560, time: 0.008) nll: 69.107208 \n",
      "(GPU: 0, epoch: 10, iters: 139360, time: 0.008) nll: 73.849121 \n",
      "(GPU: 0, epoch: 10, iters: 140160, time: 0.008) nll: 54.635262 \n",
      "[*] End of epoch 10 / 25 \t Time Taken: 1253 sec \n",
      "/cluster/54/streakfull/ADL4CV/Project/src/plz-autosdf/logs/bert2vqsc_v4-text2shape-seq-LR1e-4-network-bert-shapeset-shapeset-SmoothL1Loss\n",
      "[*] learning rate = 0.0000953\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 3032/4397 [14:25<06:00,  3.79it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 11, iters: 32, time: 0.004) nll: 64.442871 \n",
      "(GPU: 0, epoch: 11, iters: 32, time: 0.004) nll: 81.088760 \n",
      "(GPU: 0, epoch: 11, iters: 256, time: 0.008) nll: 59.591240 \n",
      "(GPU: 0, epoch: 11, iters: 1056, time: 0.008) nll: 57.681179 \n",
      "(GPU: 0, epoch: 11, iters: 1856, time: 0.008) nll: 69.412582 \n",
      "(GPU: 0, epoch: 11, iters: 2656, time: 0.008) nll: 82.311745 \n",
      "(GPU: 0, epoch: 11, iters: 3456, time: 0.008) nll: 63.814087 \n",
      "(GPU: 0, epoch: 11, iters: 4256, time: 0.008) nll: 61.583054 \n",
      "(GPU: 0, epoch: 11, iters: 5056, time: 0.008) nll: 57.471626 \n",
      "(GPU: 0, epoch: 11, iters: 5856, time: 0.008) nll: 69.867325 \n",
      "(GPU: 0, epoch: 11, iters: 6656, time: 0.008) nll: 85.587311 \n",
      "(GPU: 0, epoch: 11, iters: 7456, time: 0.008) nll: 61.177536 \n",
      "(GPU: 0, epoch: 11, iters: 8256, time: 0.008) nll: 79.624237 \n",
      "(GPU: 0, epoch: 11, iters: 9056, time: 0.008) nll: 50.212486 \n",
      "(GPU: 0, epoch: 11, iters: 9856, time: 0.008) nll: 59.635513 \n",
      "(GPU: 0, epoch: 11, iters: 10656, time: 0.008) nll: 55.155983 \n",
      "(GPU: 0, epoch: 11, iters: 11456, time: 0.008) nll: 79.915787 \n",
      "(GPU: 0, epoch: 11, iters: 12256, time: 0.008) nll: 74.271973 \n",
      "saving the latest model (epoch 11, total_steps 1560000)\n",
      "(GPU: 0, epoch: 11, iters: 13056, time: 0.008) nll: 67.843765 \n",
      "(GPU: 0, epoch: 11, iters: 13856, time: 0.008) nll: 93.978424 \n",
      "(GPU: 0, epoch: 11, iters: 14656, time: 0.008) nll: 41.869781 \n",
      "(GPU: 0, epoch: 11, iters: 15456, time: 0.008) nll: 82.242157 \n",
      "(GPU: 0, epoch: 11, iters: 16256, time: 0.008) nll: 39.309406 \n",
      "(GPU: 0, epoch: 11, iters: 17056, time: 0.008) nll: 81.504501 \n",
      "(GPU: 0, epoch: 11, iters: 17856, time: 0.008) nll: 45.555321 \n",
      "(GPU: 0, epoch: 11, iters: 18656, time: 0.008) nll: 79.564758 \n",
      "(GPU: 0, epoch: 11, iters: 19456, time: 0.008) nll: 59.850098 \n",
      "(GPU: 0, epoch: 11, iters: 20256, time: 0.008) nll: 68.308571 \n",
      "(GPU: 0, epoch: 11, iters: 21056, time: 0.008) nll: 51.801903 \n",
      "(GPU: 0, epoch: 11, iters: 21856, time: 0.008) nll: 55.407288 \n",
      "(GPU: 0, epoch: 11, iters: 22656, time: 0.008) nll: 78.970245 \n",
      "(GPU: 0, epoch: 11, iters: 23456, time: 0.008) nll: 73.499130 \n",
      "(GPU: 0, epoch: 11, iters: 24256, time: 0.008) nll: 86.812119 \n",
      "(GPU: 0, epoch: 11, iters: 25056, time: 0.008) nll: 60.813797 \n",
      "(GPU: 0, epoch: 11, iters: 25856, time: 0.008) nll: 84.856613 \n",
      "(GPU: 0, epoch: 11, iters: 26656, time: 0.008) nll: 47.641476 \n",
      "(GPU: 0, epoch: 11, iters: 27456, time: 0.008) nll: 56.314674 \n",
      "(GPU: 0, epoch: 11, iters: 28256, time: 0.008) nll: 43.599346 \n",
      "(GPU: 0, epoch: 11, iters: 29056, time: 0.008) nll: 78.481750 \n",
      "(GPU: 0, epoch: 11, iters: 29856, time: 0.008) nll: 85.105225 \n",
      "(GPU: 0, epoch: 11, iters: 30656, time: 0.008) nll: 69.867996 \n",
      "(GPU: 0, epoch: 11, iters: 31456, time: 0.008) nll: 61.469547 \n",
      "(GPU: 0, epoch: 11, iters: 32256, time: 0.008) nll: 61.371201 \n",
      "saving the latest model (epoch 11, total_steps 1580000)\n",
      "(GPU: 0, epoch: 11, iters: 33056, time: 0.008) nll: 57.537773 \n",
      "(GPU: 0, epoch: 11, iters: 33856, time: 0.008) nll: 69.838821 \n",
      "(GPU: 0, epoch: 11, iters: 34656, time: 0.008) nll: 88.591476 \n",
      "(GPU: 0, epoch: 11, iters: 35456, time: 0.008) nll: 66.915649 \n",
      "(GPU: 0, epoch: 11, iters: 36256, time: 0.008) nll: 69.232147 \n",
      "(GPU: 0, epoch: 11, iters: 37056, time: 0.008) nll: 73.527428 \n",
      "(GPU: 0, epoch: 11, iters: 37856, time: 0.008) nll: 55.332508 \n",
      "(GPU: 0, epoch: 11, iters: 38656, time: 0.008) nll: 85.967453 \n",
      "(GPU: 0, epoch: 11, iters: 39456, time: 0.008) nll: 77.509010 \n",
      "(GPU: 0, epoch: 11, iters: 40256, time: 0.008) nll: 57.540955 \n",
      "(GPU: 0, epoch: 11, iters: 41056, time: 0.008) nll: 86.684837 \n",
      "(GPU: 0, epoch: 11, iters: 41856, time: 0.008) nll: 57.186432 \n",
      "(GPU: 0, epoch: 11, iters: 42656, time: 0.008) nll: 52.711929 \n",
      "(GPU: 0, epoch: 11, iters: 43456, time: 0.008) nll: 57.660332 \n",
      "(GPU: 0, epoch: 11, iters: 44256, time: 0.008) nll: 46.716602 \n",
      "(GPU: 0, epoch: 11, iters: 45056, time: 0.008) nll: 60.497234 \n",
      "(GPU: 0, epoch: 11, iters: 45856, time: 0.008) nll: 47.296776 \n",
      "(GPU: 0, epoch: 11, iters: 46656, time: 0.008) nll: 53.015224 \n",
      "(GPU: 0, epoch: 11, iters: 47456, time: 0.008) nll: 70.287781 \n",
      "(GPU: 0, epoch: 11, iters: 48256, time: 0.008) nll: 44.157372 \n",
      "(GPU: 0, epoch: 11, iters: 49056, time: 0.008) nll: 72.763245 \n",
      "(GPU: 0, epoch: 11, iters: 49856, time: 0.008) nll: 61.691761 \n",
      "(GPU: 0, epoch: 11, iters: 50656, time: 0.008) nll: 75.834946 \n",
      "(GPU: 0, epoch: 11, iters: 51456, time: 0.008) nll: 77.030045 \n",
      "(GPU: 0, epoch: 11, iters: 52256, time: 0.008) nll: 57.006042 \n",
      "saving the latest model (epoch 11, total_steps 1600000)\n",
      "(GPU: 0, epoch: 11, iters: 53056, time: 0.008) nll: 66.508041 \n",
      "(GPU: 0, epoch: 11, iters: 53856, time: 0.008) nll: 55.200432 \n",
      "(GPU: 0, epoch: 11, iters: 54656, time: 0.008) nll: 63.176865 \n",
      "(GPU: 0, epoch: 11, iters: 55456, time: 0.008) nll: 40.520084 \n",
      "(GPU: 0, epoch: 11, iters: 56256, time: 0.008) nll: 54.652962 \n",
      "(GPU: 0, epoch: 11, iters: 57056, time: 0.008) nll: 89.744156 \n",
      "(GPU: 0, epoch: 11, iters: 57856, time: 0.008) nll: 89.712402 \n",
      "(GPU: 0, epoch: 11, iters: 58656, time: 0.008) nll: 44.850037 \n",
      "(GPU: 0, epoch: 11, iters: 59456, time: 0.008) nll: 53.002495 \n",
      "(GPU: 0, epoch: 11, iters: 60256, time: 0.008) nll: 84.523254 \n",
      "(GPU: 0, epoch: 11, iters: 61056, time: 0.008) nll: 47.806801 \n",
      "(GPU: 0, epoch: 11, iters: 61856, time: 0.008) nll: 65.931824 \n",
      "(GPU: 0, epoch: 11, iters: 62656, time: 0.008) nll: 86.977531 \n",
      "(GPU: 0, epoch: 11, iters: 63456, time: 0.008) nll: 52.515980 \n",
      "(GPU: 0, epoch: 11, iters: 64256, time: 0.008) nll: 52.393684 \n",
      "(GPU: 0, epoch: 11, iters: 65056, time: 0.008) nll: 54.989967 \n",
      "(GPU: 0, epoch: 11, iters: 65856, time: 0.008) nll: 61.014679 \n",
      "(GPU: 0, epoch: 11, iters: 66656, time: 0.008) nll: 66.020493 \n",
      "(GPU: 0, epoch: 11, iters: 67456, time: 0.008) nll: 67.666092 \n",
      "(GPU: 0, epoch: 11, iters: 68256, time: 0.008) nll: 55.324799 \n",
      "(GPU: 0, epoch: 11, iters: 69056, time: 0.008) nll: 62.407604 \n",
      "(GPU: 0, epoch: 11, iters: 69856, time: 0.008) nll: 80.999985 \n",
      "(GPU: 0, epoch: 11, iters: 70656, time: 0.008) nll: 43.950794 \n",
      "(GPU: 0, epoch: 11, iters: 71456, time: 0.008) nll: 61.453026 \n",
      "(GPU: 0, epoch: 11, iters: 72256, time: 0.008) nll: 79.293373 \n",
      "saving the latest model (epoch 11, total_steps 1620000)\n",
      "(GPU: 0, epoch: 11, iters: 73056, time: 0.008) nll: 57.851280 \n",
      "(GPU: 0, epoch: 11, iters: 73856, time: 0.008) nll: 51.431595 \n",
      "(GPU: 0, epoch: 11, iters: 74656, time: 0.008) nll: 66.299461 \n",
      "(GPU: 0, epoch: 11, iters: 75456, time: 0.008) nll: 68.909767 \n",
      "(GPU: 0, epoch: 11, iters: 76256, time: 0.008) nll: 67.789642 \n",
      "(GPU: 0, epoch: 11, iters: 77056, time: 0.008) nll: 96.058609 \n",
      "(GPU: 0, epoch: 11, iters: 77856, time: 0.008) nll: 87.092651 \n",
      "(GPU: 0, epoch: 11, iters: 78656, time: 0.008) nll: 85.957336 \n",
      "(GPU: 0, epoch: 11, iters: 79456, time: 0.008) nll: 55.755352 \n",
      "(GPU: 0, epoch: 11, iters: 80256, time: 0.008) nll: 45.214615 \n",
      "(GPU: 0, epoch: 11, iters: 81056, time: 0.008) nll: 72.858154 \n",
      "(GPU: 0, epoch: 11, iters: 81856, time: 0.008) nll: 81.339722 \n",
      "(GPU: 0, epoch: 11, iters: 82656, time: 0.008) nll: 59.042633 \n",
      "(GPU: 0, epoch: 11, iters: 83456, time: 0.008) nll: 85.936493 \n",
      "(GPU: 0, epoch: 11, iters: 84256, time: 0.008) nll: 56.432693 \n",
      "(GPU: 0, epoch: 11, iters: 84256, time: 0.013) nll: 54.679745 \n",
      "(GPU: 0, epoch: 11, iters: 84256, time: 0.013) nll: 75.399750 \n",
      "(GPU: 0, epoch: 11, iters: 85056, time: 0.008) nll: 85.187531 \n",
      "(GPU: 0, epoch: 11, iters: 85856, time: 0.008) nll: 93.888817 \n",
      "(GPU: 0, epoch: 11, iters: 86656, time: 0.008) nll: 60.794201 \n",
      "(GPU: 0, epoch: 11, iters: 87456, time: 0.008) nll: 72.522446 \n",
      "(GPU: 0, epoch: 11, iters: 88256, time: 0.009) nll: 62.544231 \n",
      "(GPU: 0, epoch: 11, iters: 89056, time: 0.008) nll: 65.024124 \n",
      "(GPU: 0, epoch: 11, iters: 89856, time: 0.008) nll: 63.349773 \n",
      "(GPU: 0, epoch: 11, iters: 90656, time: 0.008) nll: 61.442646 \n",
      "(GPU: 0, epoch: 11, iters: 91456, time: 0.008) nll: 68.665062 \n",
      "(GPU: 0, epoch: 11, iters: 92256, time: 0.008) nll: 82.749924 \n",
      "saving the latest model (epoch 11, total_steps 1640000)\n",
      "(GPU: 0, epoch: 11, iters: 93056, time: 0.008) nll: 66.767319 \n",
      "(GPU: 0, epoch: 11, iters: 93856, time: 0.008) nll: 87.106827 \n",
      "(GPU: 0, epoch: 11, iters: 94656, time: 0.008) nll: 74.147034 \n",
      "(GPU: 0, epoch: 11, iters: 95456, time: 0.008) nll: 83.224579 \n",
      "(GPU: 0, epoch: 11, iters: 96256, time: 0.008) nll: 54.159988 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4397/4397 [20:53<00:00,  3.51it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 11, iters: 97056, time: 0.008) nll: 95.795280 \n",
      "(GPU: 0, epoch: 11, iters: 97856, time: 0.008) nll: 50.946953 \n",
      "(GPU: 0, epoch: 11, iters: 98656, time: 0.008) nll: 53.534691 \n",
      "(GPU: 0, epoch: 11, iters: 99456, time: 0.008) nll: 59.835632 \n",
      "(GPU: 0, epoch: 11, iters: 100256, time: 0.007) nll: 58.404572 \n",
      "(GPU: 0, epoch: 11, iters: 101056, time: 0.008) nll: 54.273239 \n",
      "(GPU: 0, epoch: 11, iters: 101856, time: 0.008) nll: 72.428070 \n",
      "(GPU: 0, epoch: 11, iters: 102656, time: 0.008) nll: 81.771393 \n",
      "(GPU: 0, epoch: 11, iters: 103456, time: 0.008) nll: 47.741062 \n",
      "(GPU: 0, epoch: 11, iters: 104256, time: 0.008) nll: 60.191383 \n",
      "(GPU: 0, epoch: 11, iters: 105056, time: 0.008) nll: 67.439575 \n",
      "(GPU: 0, epoch: 11, iters: 105856, time: 0.008) nll: 53.931118 \n",
      "(GPU: 0, epoch: 11, iters: 106656, time: 0.008) nll: 53.999817 \n",
      "(GPU: 0, epoch: 11, iters: 107456, time: 0.008) nll: 76.203354 \n",
      "(GPU: 0, epoch: 11, iters: 108256, time: 0.008) nll: 75.984528 \n",
      "(GPU: 0, epoch: 11, iters: 109056, time: 0.008) nll: 75.121071 \n",
      "(GPU: 0, epoch: 11, iters: 109856, time: 0.008) nll: 73.939240 \n",
      "(GPU: 0, epoch: 11, iters: 110656, time: 0.008) nll: 72.393814 \n",
      "(GPU: 0, epoch: 11, iters: 111456, time: 0.008) nll: 59.365440 \n",
      "(GPU: 0, epoch: 11, iters: 112256, time: 0.008) nll: 59.181999 \n",
      "saving the latest model (epoch 11, total_steps 1660000)\n",
      "(GPU: 0, epoch: 11, iters: 113056, time: 0.008) nll: 76.202248 \n",
      "(GPU: 0, epoch: 11, iters: 113856, time: 0.008) nll: 70.428131 \n",
      "(GPU: 0, epoch: 11, iters: 114656, time: 0.008) nll: 92.235794 \n",
      "(GPU: 0, epoch: 11, iters: 115456, time: 0.008) nll: 62.418423 \n",
      "(GPU: 0, epoch: 11, iters: 116256, time: 0.008) nll: 75.351257 \n",
      "(GPU: 0, epoch: 11, iters: 117056, time: 0.008) nll: 73.454285 \n",
      "(GPU: 0, epoch: 11, iters: 117856, time: 0.008) nll: 88.842728 \n",
      "(GPU: 0, epoch: 11, iters: 118656, time: 0.008) nll: 61.105663 \n",
      "(GPU: 0, epoch: 11, iters: 119456, time: 0.008) nll: 55.293732 \n",
      "(GPU: 0, epoch: 11, iters: 120256, time: 0.008) nll: 69.609909 \n",
      "(GPU: 0, epoch: 11, iters: 121056, time: 0.008) nll: 55.967834 \n",
      "(GPU: 0, epoch: 11, iters: 121856, time: 0.008) nll: 82.134949 \n",
      "(GPU: 0, epoch: 11, iters: 122656, time: 0.008) nll: 68.968460 \n",
      "(GPU: 0, epoch: 11, iters: 123456, time: 0.008) nll: 55.797298 \n",
      "(GPU: 0, epoch: 11, iters: 124256, time: 0.008) nll: 95.572777 \n",
      "(GPU: 0, epoch: 11, iters: 125056, time: 0.008) nll: 54.832191 \n",
      "(GPU: 0, epoch: 11, iters: 125856, time: 0.007) nll: 93.678284 \n",
      "(GPU: 0, epoch: 11, iters: 126656, time: 0.008) nll: 65.690254 \n",
      "(GPU: 0, epoch: 11, iters: 127456, time: 0.008) nll: 67.794228 \n",
      "(GPU: 0, epoch: 11, iters: 128256, time: 0.008) nll: 61.439190 \n",
      "(GPU: 0, epoch: 11, iters: 129056, time: 0.008) nll: 66.852310 \n",
      "(GPU: 0, epoch: 11, iters: 129856, time: 0.008) nll: 79.798355 \n",
      "(GPU: 0, epoch: 11, iters: 130656, time: 0.008) nll: 60.640343 \n",
      "(GPU: 0, epoch: 11, iters: 131456, time: 0.008) nll: 51.965050 \n",
      "(GPU: 0, epoch: 11, iters: 132256, time: 0.008) nll: 63.580143 \n",
      "saving the latest model (epoch 11, total_steps 1680000)\n",
      "(GPU: 0, epoch: 11, iters: 133056, time: 0.008) nll: 70.729630 \n",
      "(GPU: 0, epoch: 11, iters: 133856, time: 0.008) nll: 63.795464 \n",
      "(GPU: 0, epoch: 11, iters: 134656, time: 0.008) nll: 58.400810 \n",
      "(GPU: 0, epoch: 11, iters: 135456, time: 0.008) nll: 59.794441 \n",
      "(GPU: 0, epoch: 11, iters: 136256, time: 0.008) nll: 57.529079 \n",
      "(GPU: 0, epoch: 11, iters: 137056, time: 0.007) nll: 92.655380 \n",
      "(GPU: 0, epoch: 11, iters: 137856, time: 0.008) nll: 61.333439 \n",
      "(GPU: 0, epoch: 11, iters: 138656, time: 0.008) nll: 72.817886 \n",
      "(GPU: 0, epoch: 11, iters: 139456, time: 0.008) nll: 61.386505 \n",
      "(GPU: 0, epoch: 11, iters: 140256, time: 0.008) nll: 63.454277 \n",
      "[*] End of epoch 11 / 25 \t Time Taken: 1253 sec \n",
      "/cluster/54/streakfull/ADL4CV/Project/src/plz-autosdf/logs/bert2vqsc_v4-text2shape-seq-LR1e-4-network-bert-shapeset-shapeset-SmoothL1Loss\n",
      "[*] learning rate = 0.0000913\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 3035/4397 [14:27<06:01,  3.77it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 12, iters: 32, time: 0.004) nll: 78.875137 \n",
      "(GPU: 0, epoch: 12, iters: 32, time: 0.004) nll: 62.317017 \n",
      "(GPU: 0, epoch: 12, iters: 352, time: 0.008) nll: 68.349136 \n",
      "(GPU: 0, epoch: 12, iters: 1152, time: 0.008) nll: 67.618843 \n",
      "(GPU: 0, epoch: 12, iters: 1952, time: 0.008) nll: 70.175270 \n",
      "(GPU: 0, epoch: 12, iters: 2752, time: 0.008) nll: 74.202148 \n",
      "(GPU: 0, epoch: 12, iters: 3552, time: 0.007) nll: 55.965504 \n",
      "(GPU: 0, epoch: 12, iters: 4352, time: 0.008) nll: 56.615509 \n",
      "(GPU: 0, epoch: 12, iters: 5152, time: 0.008) nll: 80.238029 \n",
      "(GPU: 0, epoch: 12, iters: 5952, time: 0.008) nll: 67.655983 \n",
      "(GPU: 0, epoch: 12, iters: 6752, time: 0.008) nll: 79.582985 \n",
      "(GPU: 0, epoch: 12, iters: 7552, time: 0.008) nll: 39.636215 \n",
      "(GPU: 0, epoch: 12, iters: 8352, time: 0.008) nll: 57.170544 \n",
      "(GPU: 0, epoch: 12, iters: 9152, time: 0.008) nll: 74.594193 \n",
      "(GPU: 0, epoch: 12, iters: 9952, time: 0.008) nll: 79.279953 \n",
      "(GPU: 0, epoch: 12, iters: 10752, time: 0.008) nll: 47.872780 \n",
      "(GPU: 0, epoch: 12, iters: 11552, time: 0.008) nll: 85.234756 \n",
      "saving the latest model (epoch 12, total_steps 1700000)\n",
      "(GPU: 0, epoch: 12, iters: 12352, time: 0.008) nll: 89.237442 \n",
      "(GPU: 0, epoch: 12, iters: 13152, time: 0.008) nll: 74.117569 \n",
      "(GPU: 0, epoch: 12, iters: 13952, time: 0.008) nll: 51.633358 \n",
      "(GPU: 0, epoch: 12, iters: 14752, time: 0.008) nll: 76.028809 \n",
      "(GPU: 0, epoch: 12, iters: 15552, time: 0.008) nll: 62.625526 \n",
      "(GPU: 0, epoch: 12, iters: 16352, time: 0.008) nll: 89.249603 \n",
      "(GPU: 0, epoch: 12, iters: 17152, time: 0.008) nll: 56.090401 \n",
      "(GPU: 0, epoch: 12, iters: 17952, time: 0.008) nll: 88.095871 \n",
      "(GPU: 0, epoch: 12, iters: 18752, time: 0.008) nll: 52.564583 \n",
      "(GPU: 0, epoch: 12, iters: 19552, time: 0.008) nll: 90.925827 \n",
      "(GPU: 0, epoch: 12, iters: 20352, time: 0.008) nll: 51.154137 \n",
      "(GPU: 0, epoch: 12, iters: 21152, time: 0.008) nll: 44.068195 \n",
      "(GPU: 0, epoch: 12, iters: 21952, time: 0.008) nll: 65.065598 \n",
      "(GPU: 0, epoch: 12, iters: 22752, time: 0.008) nll: 75.811554 \n",
      "(GPU: 0, epoch: 12, iters: 23552, time: 0.008) nll: 87.037819 \n",
      "(GPU: 0, epoch: 12, iters: 24352, time: 0.008) nll: 85.193199 \n",
      "(GPU: 0, epoch: 12, iters: 25152, time: 0.008) nll: 74.259781 \n",
      "(GPU: 0, epoch: 12, iters: 25952, time: 0.008) nll: 56.898499 \n",
      "(GPU: 0, epoch: 12, iters: 26752, time: 0.008) nll: 66.620651 \n",
      "(GPU: 0, epoch: 12, iters: 27552, time: 0.008) nll: 61.779186 \n",
      "(GPU: 0, epoch: 12, iters: 28352, time: 0.008) nll: 68.882843 \n",
      "(GPU: 0, epoch: 12, iters: 29152, time: 0.008) nll: 80.543831 \n",
      "(GPU: 0, epoch: 12, iters: 29952, time: 0.008) nll: 52.235264 \n",
      "(GPU: 0, epoch: 12, iters: 30752, time: 0.008) nll: 81.474960 \n",
      "(GPU: 0, epoch: 12, iters: 31552, time: 0.008) nll: 67.633636 \n",
      "saving the latest model (epoch 12, total_steps 1720000)\n",
      "(GPU: 0, epoch: 12, iters: 32352, time: 0.008) nll: 61.546692 \n",
      "(GPU: 0, epoch: 12, iters: 33152, time: 0.008) nll: 55.422791 \n",
      "(GPU: 0, epoch: 12, iters: 33952, time: 0.007) nll: 63.709541 \n",
      "(GPU: 0, epoch: 12, iters: 34752, time: 0.008) nll: 40.528198 \n",
      "(GPU: 0, epoch: 12, iters: 35552, time: 0.008) nll: 73.325012 \n",
      "(GPU: 0, epoch: 12, iters: 36352, time: 0.008) nll: 56.386005 \n",
      "(GPU: 0, epoch: 12, iters: 37152, time: 0.008) nll: 51.543713 \n",
      "(GPU: 0, epoch: 12, iters: 37952, time: 0.008) nll: 74.502159 \n",
      "(GPU: 0, epoch: 12, iters: 38752, time: 0.008) nll: 68.233261 \n",
      "(GPU: 0, epoch: 12, iters: 39552, time: 0.008) nll: 64.588112 \n",
      "(GPU: 0, epoch: 12, iters: 39552, time: 0.013) nll: 63.378365 \n",
      "(GPU: 0, epoch: 12, iters: 39552, time: 0.013) nll: 61.275036 \n",
      "(GPU: 0, epoch: 12, iters: 40352, time: 0.008) nll: 75.485352 \n",
      "(GPU: 0, epoch: 12, iters: 41152, time: 0.008) nll: 86.279182 \n",
      "(GPU: 0, epoch: 12, iters: 41952, time: 0.008) nll: 59.659336 \n",
      "(GPU: 0, epoch: 12, iters: 42752, time: 0.008) nll: 50.795734 \n",
      "(GPU: 0, epoch: 12, iters: 43552, time: 0.007) nll: 52.829758 \n",
      "(GPU: 0, epoch: 12, iters: 44352, time: 0.008) nll: 62.339706 \n",
      "(GPU: 0, epoch: 12, iters: 45152, time: 0.008) nll: 67.139404 \n",
      "(GPU: 0, epoch: 12, iters: 45952, time: 0.008) nll: 64.608200 \n",
      "(GPU: 0, epoch: 12, iters: 46752, time: 0.008) nll: 55.569862 \n",
      "(GPU: 0, epoch: 12, iters: 47552, time: 0.008) nll: 63.386765 \n",
      "(GPU: 0, epoch: 12, iters: 48352, time: 0.008) nll: 50.512642 \n",
      "(GPU: 0, epoch: 12, iters: 49152, time: 0.008) nll: 80.488083 \n",
      "(GPU: 0, epoch: 12, iters: 49952, time: 0.008) nll: 80.860672 \n",
      "(GPU: 0, epoch: 12, iters: 50752, time: 0.008) nll: 89.289368 \n",
      "(GPU: 0, epoch: 12, iters: 51552, time: 0.008) nll: 63.264851 \n",
      "saving the latest model (epoch 12, total_steps 1740000)\n",
      "(GPU: 0, epoch: 12, iters: 52352, time: 0.008) nll: 58.159981 \n",
      "(GPU: 0, epoch: 12, iters: 53152, time: 0.008) nll: 54.900436 \n",
      "(GPU: 0, epoch: 12, iters: 53952, time: 0.008) nll: 50.749744 \n",
      "(GPU: 0, epoch: 12, iters: 54752, time: 0.008) nll: 51.134495 \n",
      "(GPU: 0, epoch: 12, iters: 55552, time: 0.008) nll: 83.628372 \n",
      "(GPU: 0, epoch: 12, iters: 56352, time: 0.008) nll: 87.562408 \n",
      "(GPU: 0, epoch: 12, iters: 57152, time: 0.008) nll: 46.658089 \n",
      "(GPU: 0, epoch: 12, iters: 57952, time: 0.008) nll: 51.090256 \n",
      "(GPU: 0, epoch: 12, iters: 58752, time: 0.008) nll: 52.730286 \n",
      "(GPU: 0, epoch: 12, iters: 59552, time: 0.008) nll: 55.523827 \n",
      "(GPU: 0, epoch: 12, iters: 60352, time: 0.008) nll: 80.078888 \n",
      "(GPU: 0, epoch: 12, iters: 61152, time: 0.008) nll: 64.367363 \n",
      "(GPU: 0, epoch: 12, iters: 61952, time: 0.008) nll: 89.675903 \n",
      "(GPU: 0, epoch: 12, iters: 62752, time: 0.008) nll: 51.993198 \n",
      "(GPU: 0, epoch: 12, iters: 63552, time: 0.008) nll: 75.407272 \n",
      "(GPU: 0, epoch: 12, iters: 64352, time: 0.008) nll: 61.740784 \n",
      "(GPU: 0, epoch: 12, iters: 65152, time: 0.008) nll: 60.294289 \n",
      "(GPU: 0, epoch: 12, iters: 65952, time: 0.008) nll: 69.855659 \n",
      "(GPU: 0, epoch: 12, iters: 66752, time: 0.008) nll: 83.209328 \n",
      "(GPU: 0, epoch: 12, iters: 67552, time: 0.008) nll: 72.526581 \n",
      "(GPU: 0, epoch: 12, iters: 68352, time: 0.008) nll: 58.033691 \n",
      "(GPU: 0, epoch: 12, iters: 69152, time: 0.008) nll: 64.949921 \n",
      "(GPU: 0, epoch: 12, iters: 69952, time: 0.008) nll: 56.745949 \n",
      "(GPU: 0, epoch: 12, iters: 70752, time: 0.008) nll: 65.676651 \n",
      "(GPU: 0, epoch: 12, iters: 71552, time: 0.008) nll: 54.620705 \n",
      "saving the latest model (epoch 12, total_steps 1760000)\n",
      "(GPU: 0, epoch: 12, iters: 72352, time: 0.008) nll: 48.683941 \n",
      "(GPU: 0, epoch: 12, iters: 73152, time: 0.008) nll: 75.749992 \n",
      "(GPU: 0, epoch: 12, iters: 73952, time: 0.008) nll: 84.625290 \n",
      "(GPU: 0, epoch: 12, iters: 74752, time: 0.008) nll: 62.658607 \n",
      "(GPU: 0, epoch: 12, iters: 75552, time: 0.008) nll: 47.656075 \n",
      "(GPU: 0, epoch: 12, iters: 76352, time: 0.008) nll: 96.182922 \n",
      "(GPU: 0, epoch: 12, iters: 77152, time: 0.007) nll: 61.802242 \n",
      "(GPU: 0, epoch: 12, iters: 77952, time: 0.008) nll: 73.828239 \n",
      "(GPU: 0, epoch: 12, iters: 78752, time: 0.008) nll: 71.424728 \n",
      "(GPU: 0, epoch: 12, iters: 79552, time: 0.008) nll: 51.621590 \n",
      "(GPU: 0, epoch: 12, iters: 80352, time: 0.007) nll: 48.585602 \n",
      "(GPU: 0, epoch: 12, iters: 81152, time: 0.008) nll: 58.212044 \n",
      "(GPU: 0, epoch: 12, iters: 81952, time: 0.008) nll: 60.524624 \n",
      "(GPU: 0, epoch: 12, iters: 82752, time: 0.008) nll: 63.519821 \n",
      "(GPU: 0, epoch: 12, iters: 83552, time: 0.008) nll: 56.733212 \n",
      "(GPU: 0, epoch: 12, iters: 84352, time: 0.008) nll: 64.264038 \n",
      "(GPU: 0, epoch: 12, iters: 85152, time: 0.008) nll: 56.732903 \n",
      "(GPU: 0, epoch: 12, iters: 85952, time: 0.008) nll: 46.267868 \n",
      "(GPU: 0, epoch: 12, iters: 86752, time: 0.008) nll: 71.846764 \n",
      "(GPU: 0, epoch: 12, iters: 87552, time: 0.008) nll: 83.764832 \n",
      "(GPU: 0, epoch: 12, iters: 88352, time: 0.008) nll: 71.397446 \n",
      "(GPU: 0, epoch: 12, iters: 89152, time: 0.008) nll: 70.002213 \n",
      "(GPU: 0, epoch: 12, iters: 89952, time: 0.008) nll: 78.771103 \n",
      "(GPU: 0, epoch: 12, iters: 90752, time: 0.008) nll: 61.800388 \n",
      "(GPU: 0, epoch: 12, iters: 91552, time: 0.008) nll: 82.934341 \n",
      "saving the latest model (epoch 12, total_steps 1780000)\n",
      "(GPU: 0, epoch: 12, iters: 92352, time: 0.008) nll: 80.441666 \n",
      "(GPU: 0, epoch: 12, iters: 93152, time: 0.008) nll: 53.662865 \n",
      "(GPU: 0, epoch: 12, iters: 93952, time: 0.008) nll: 58.473339 \n",
      "(GPU: 0, epoch: 12, iters: 94752, time: 0.008) nll: 57.232929 \n",
      "(GPU: 0, epoch: 12, iters: 95552, time: 0.008) nll: 62.135826 \n",
      "(GPU: 0, epoch: 12, iters: 96352, time: 0.008) nll: 77.483688 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4397/4397 [20:53<00:00,  3.51it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: 0, epoch: 12, iters: 97152, time: 0.008) nll: 62.543846 \n",
      "(GPU: 0, epoch: 12, iters: 97952, time: 0.008) nll: 53.557961 \n",
      "(GPU: 0, epoch: 12, iters: 98752, time: 0.008) nll: 69.628151 \n",
      "(GPU: 0, epoch: 12, iters: 99552, time: 0.008) nll: 79.864227 \n",
      "(GPU: 0, epoch: 12, iters: 100352, time: 0.008) nll: 71.788757 \n",
      "(GPU: 0, epoch: 12, iters: 101152, time: 0.008) nll: 67.799225 \n",
      "(GPU: 0, epoch: 12, iters: 101952, time: 0.008) nll: 47.128788 \n",
      "(GPU: 0, epoch: 12, iters: 102752, time: 0.008) nll: 65.009796 \n",
      "(GPU: 0, epoch: 12, iters: 103552, time: 0.008) nll: 72.575287 \n",
      "(GPU: 0, epoch: 12, iters: 104352, time: 0.008) nll: 50.107365 \n",
      "(GPU: 0, epoch: 12, iters: 105152, time: 0.008) nll: 73.810913 \n",
      "(GPU: 0, epoch: 12, iters: 105952, time: 0.008) nll: 78.688644 \n",
      "(GPU: 0, epoch: 12, iters: 106752, time: 0.008) nll: 72.998680 \n",
      "(GPU: 0, epoch: 12, iters: 107552, time: 0.008) nll: 69.902184 \n",
      "(GPU: 0, epoch: 12, iters: 108352, time: 0.008) nll: 51.514458 \n",
      "(GPU: 0, epoch: 12, iters: 109152, time: 0.008) nll: 66.007217 \n",
      "(GPU: 0, epoch: 12, iters: 109952, time: 0.008) nll: 60.129215 \n",
      "(GPU: 0, epoch: 12, iters: 110752, time: 0.008) nll: 83.182053 \n",
      "(GPU: 0, epoch: 12, iters: 111552, time: 0.008) nll: 54.814049 \n",
      "saving the latest model (epoch 12, total_steps 1800000)\n",
      "(GPU: 0, epoch: 12, iters: 112352, time: 0.008) nll: 67.108482 \n",
      "(GPU: 0, epoch: 12, iters: 113152, time: 0.008) nll: 85.442787 \n",
      "(GPU: 0, epoch: 12, iters: 113952, time: 0.008) nll: 65.611160 \n",
      "(GPU: 0, epoch: 12, iters: 114752, time: 0.008) nll: 67.896881 \n",
      "(GPU: 0, epoch: 12, iters: 115552, time: 0.007) nll: 78.710175 \n",
      "(GPU: 0, epoch: 12, iters: 116352, time: 0.008) nll: 50.761013 \n",
      "(GPU: 0, epoch: 12, iters: 117152, time: 0.008) nll: 62.870674 \n",
      "(GPU: 0, epoch: 12, iters: 117952, time: 0.008) nll: 51.436905 \n",
      "(GPU: 0, epoch: 12, iters: 118752, time: 0.008) nll: 52.563553 \n",
      "(GPU: 0, epoch: 12, iters: 119552, time: 0.008) nll: 87.181274 \n",
      "(GPU: 0, epoch: 12, iters: 120352, time: 0.008) nll: 67.627182 \n",
      "(GPU: 0, epoch: 12, iters: 121152, time: 0.008) nll: 63.032101 \n",
      "(GPU: 0, epoch: 12, iters: 121952, time: 0.008) nll: 69.227760 \n",
      "(GPU: 0, epoch: 12, iters: 122752, time: 0.008) nll: 81.327202 \n",
      "(GPU: 0, epoch: 12, iters: 123552, time: 0.008) nll: 66.168350 \n",
      "(GPU: 0, epoch: 12, iters: 124352, time: 0.008) nll: 67.212860 \n",
      "(GPU: 0, epoch: 12, iters: 125152, time: 0.008) nll: 76.659882 \n",
      "(GPU: 0, epoch: 12, iters: 125952, time: 0.008) nll: 81.869247 \n",
      "(GPU: 0, epoch: 12, iters: 126752, time: 0.008) nll: 104.957336 \n",
      "(GPU: 0, epoch: 12, iters: 127552, time: 0.008) nll: 78.205009 \n",
      "(GPU: 0, epoch: 12, iters: 128352, time: 0.008) nll: 70.422394 \n",
      "(GPU: 0, epoch: 12, iters: 129152, time: 0.008) nll: 71.112328 \n",
      "(GPU: 0, epoch: 12, iters: 129952, time: 0.008) nll: 70.414917 \n",
      "(GPU: 0, epoch: 12, iters: 130752, time: 0.008) nll: 67.979088 \n",
      "(GPU: 0, epoch: 12, iters: 131552, time: 0.008) nll: 66.581871 \n",
      "saving the latest model (epoch 12, total_steps 1820000)\n",
      "(GPU: 0, epoch: 12, iters: 132352, time: 0.008) nll: 71.360367 \n",
      "(GPU: 0, epoch: 12, iters: 133152, time: 0.008) nll: 61.878685 \n",
      "(GPU: 0, epoch: 12, iters: 133952, time: 0.008) nll: 59.272842 \n",
      "(GPU: 0, epoch: 12, iters: 134752, time: 0.008) nll: 57.728432 \n",
      "(GPU: 0, epoch: 12, iters: 135552, time: 0.008) nll: 68.934685 \n",
      "(GPU: 0, epoch: 12, iters: 135552, time: 0.013) nll: 68.283829 \n",
      "(GPU: 0, epoch: 12, iters: 135552, time: 0.013) nll: 70.019623 \n",
      "(GPU: 0, epoch: 12, iters: 136352, time: 0.008) nll: 79.515945 \n",
      "(GPU: 0, epoch: 12, iters: 137152, time: 0.008) nll: 79.763931 \n",
      "(GPU: 0, epoch: 12, iters: 137952, time: 0.007) nll: 62.872070 \n",
      "(GPU: 0, epoch: 12, iters: 138752, time: 0.008) nll: 67.456985 \n",
      "(GPU: 0, epoch: 12, iters: 139552, time: 0.008) nll: 54.265266 \n",
      "(GPU: 0, epoch: 12, iters: 140352, time: 0.008) nll: 74.491379 \n",
      "saving the model at the end of epoch 12, iters 1829152\n",
      "([test] GPU: 0, epoch: 12) \n",
      "OrderedDict()\n",
      "[*] End of epoch 12 / 25 \t Time Taken: 1279 sec \n",
      "/cluster/54/streakfull/ADL4CV/Project/src/plz-autosdf/logs/bert2vqsc_v4-text2shape-seq-LR1e-4-network-bert-shapeset-shapeset-SmoothL1Loss\n",
      "[*] learning rate = 0.0000877\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 2838/4397 [13:20<06:48,  3.81it/s]  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3903948/1349841514.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./launchers/train_rand_tf_snet_code.sh\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Including KeyboardInterrupt, wait handled that.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1081\u001b[0m             \u001b[0mendtime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1083\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1084\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m             \u001b[0;31m# https://bugs.python.org/issue25942\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36m_wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1804\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1805\u001b[0m                             \u001b[0;32mbreak\u001b[0m  \u001b[0;31m# Another thread waited.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1806\u001b[0;31m                         \u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1807\u001b[0m                         \u001b[0;31m# Check the pid and loop as waitpid has been known to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1808\u001b[0m                         \u001b[0;31m# return 0 even without WNOHANG in odd situations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36m_try_wait\u001b[0;34m(self, wait_flags)\u001b[0m\n\u001b[1;32m   1762\u001b[0m             \u001b[0;34m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1763\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1764\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait_flags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1765\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mChildProcessError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1766\u001b[0m                 \u001b[0;31m# This happens if SIGCLD is set to be ignored or waiting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rc = subprocess.call(\"./launchers/train_rand_tf_snet_code.sh\", shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981f8cb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
