{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbec1c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import Image as ipy_image\n",
    "from IPython.display import display\n",
    "\n",
    "#from utils.demo_util import get_shape_comp_model, get_shape_comp_opt, make_dummy_batch\n",
    "#from utils.qual_util import load_bert2vqsc_model, get_lang_prob, save_mesh_as_gif\n",
    "from utils.qual_util import get_lang_prob, save_mesh_as_gif\n",
    "from utils.util import seed_everything\n",
    "from utils.util_3d import init_mesh_renderer, sdf_to_mesh\n",
    "import os\n",
    "from termcolor import colored, cprint\n",
    "import torch\n",
    "import utils.util as util\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from datasets.ys_shapeset import  ShapeNetZSets\n",
    "\n",
    "from datasets.text2shape import  Text2Shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import imageio\n",
    "from PIL import Image\n",
    "from einops import rearrange\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.utils as vutils\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from pytorch3d import structures\n",
    "\n",
    "from utils.util_3d import sdf_to_mesh, render_mesh, rotate_mesh_360\n",
    "\n",
    "\n",
    "from datasets.text2shape import  Text2Shape\n",
    "\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7572d808",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(opt):\n",
    "    model = None\n",
    "\n",
    "    if opt.model == 'pvqvae':\n",
    "        # vqvae\n",
    "        from models.pvqvae_model import PVQVAEModel\n",
    "        model = PVQVAEModel()\n",
    "    elif opt.model == 'rand_tf':\n",
    "        # transformer\n",
    "        from models.rand_tf_model import RandTransformerModel\n",
    "        model = RandTransformerModel()\n",
    "    elif opt.model == 'rand_tf_old':\n",
    "        from models.rand_tf_model_old import RandTransformerModelOld\n",
    "        model = RandTransformerModelOld()\n",
    "    elif opt.model == 'seq_tf':\n",
    "        # seq-transformer\n",
    "        from models.seq_tf_model import SeqTransformerModel\n",
    "        model = SeqTransformerModel()\n",
    "    elif opt.model == 'bert2vq':\n",
    "        from models.bert2vq_model import BERT2VQModel\n",
    "        model = BERT2VQModel()\n",
    "    elif opt.model == \"bert2vqsc\":\n",
    "        from models.bert2vq_scmodel import BERT2VQSCModel\n",
    "        model = BERT2VQSCModel()\n",
    "    elif opt.model == 'resnet2vq':\n",
    "        from models.resnet2vq_model import ResNet2VQModel\n",
    "        model = ResNet2VQModel()\n",
    "    elif opt.model == 'resnet2vox':\n",
    "        from models.resnet2vox_model import ResNet2VoxModel\n",
    "        model = ResNet2VoxModel()\n",
    "    elif opt.model == 'resnet2sdf':\n",
    "        from models.resnet2sdf_model import ResNet2SDFModel\n",
    "        model = ResNet2SDFModel()\n",
    "    elif opt.model == 'baseline_je':\n",
    "        from models.baseline_je_model import LangJEModel\n",
    "        model = LangJEModel()\n",
    "    elif opt.model == 'img_je':\n",
    "        from models.img_je_model import ImgJEModel\n",
    "        model = ImgJEModel()\n",
    "    else:\n",
    "        raise ValueError(\"Model [%s] not recognized.\" % opt.model)\n",
    "\n",
    "    model.initialize(opt)\n",
    "    cprint(\"[*] Model has been created: %s\" % model.name(), 'blue')\n",
    "    if(opt.ckpt is not None):\n",
    "        model.load_ckpt(opt.ckpt)\n",
    "        print(\"MODEL LOADED\")\n",
    "    return model\n",
    "\n",
    "def get_shape_comp_opt(gpu_id=0):\n",
    "    opt = Opt()\n",
    "\n",
    "    # args\n",
    "    gpuid=[gpu_id]\n",
    "    batch_size=1\n",
    "    max_dataset_size=10000000\n",
    "\n",
    "    name='test_transformer'\n",
    "\n",
    "    # default args\n",
    "    opt.serial_batches = False\n",
    "    opt.nThreads = 4\n",
    "\n",
    "    # important args\n",
    "    opt.dataset_mode = 'shapenet_code'\n",
    "    opt.seed = 111\n",
    "    opt.isTrain = False\n",
    "    opt.gpu_ids = gpuid\n",
    "    opt.device = 'cuda:%s' % gpuid[0]\n",
    "    opt.batch_size = batch_size\n",
    "    opt.max_dataset_size = max_dataset_size\n",
    "\n",
    "    opt.name = name\n",
    "\n",
    "    #utils.util.seed_everything(opt.seed)\n",
    "    opt.phase = 'test'\n",
    "    return opt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_shape_comp_model(opt, model, ckpt):\n",
    "    \n",
    "    # load tf stuff\n",
    "    opt.model=model\n",
    "    opt.tf_cfg='configs/rand_tf_snet_code.yaml'\n",
    "    opt.ckpt = ckpt\n",
    "    \n",
    "    # load vq stuff\n",
    "    opt.vq_model='pvqvae'\n",
    "    opt.vq_cfg='configs/pvqvae_snet.yaml'\n",
    "    opt.vq_ckpt='../raw_dataset/checkpoints/vqvae.pth'\n",
    "    \n",
    "    ### opt.vq_dset='sdf_code' # original\n",
    "    opt.vq_dset='snet'\n",
    "\n",
    "    model = create_model(opt)\n",
    "    print(f'[*] \"{opt.model}\" initialized.')\n",
    "    model.load_ckpt(opt.ckpt)\n",
    "        \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60a9da2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Enc has Attn at i_level, i_block: 3, 0\n",
      "Working with z of shape (1, 256, 8, 8, 8) = 131072 dimensions.\n",
      "[*] Dec has Attn at i_level, i_block: 3, 0\n",
      "[*] VQVAE: weight successfully load from: ../raw_dataset/checkpoints/vqvae.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rhome/streakfull/.cache/pypoetry/virtualenvs/adl4cv-I-Koul65-py3.8/lib/python3.8/site-packages/torch/functional.py:568: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2228.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Model has been created: Rand-Transformer-Model\n",
      "[*] weight successfully load from: ../raw_dataset/checkpoints_new/rand_tf_epoch-latest_v5.pth\n",
      "MODEL LOADED\n",
      "[*] \"rand_tf\" initialized.\n",
      "[*] weight successfully load from: ../raw_dataset/checkpoints_new/rand_tf_epoch-latest_v5.pth\n"
     ]
    }
   ],
   "source": [
    "opt = get_shape_comp_opt(gpu_id=gpu_id)\n",
    "opt.dataset_mode = \"text2shape\"\n",
    "\n",
    "#new_checkpoint_path = \"logs/valid-valid-with-fusion-rand_tf-owndataset-chair-LR1e-4-clean-with-ckpt/ckpt/rand_tf_epoch-latest.pth\"\n",
    "new_checkpoint_path_V1 = \"../raw_dataset/checkpoints_new/rand_tf_epoch_latest_fusion_transfer_v1.pth\"\n",
    "new_checkpoint_path_V2=\"../raw_dataset/checkpoints_new/rand_tf_epoch_latest_fusion_v2.pth\"\n",
    "new_checkpoint_path_V3 = \"../raw_dataset/checkpoints_new/rand_tf_epoch_0_fusion_v3.pth\"\n",
    "new_checkpoint_path_V5 = \"../raw_dataset/checkpoints_new/rand_tf_epoch-latest_v5.pth\"\n",
    "\n",
    "\n",
    "# modelV0 = get_shape_comp_model(opt,\"rand_tf_old\",\"../raw_dataset/checkpoints/rand_tf_singles_best.pth\")    \n",
    "# modelV0.eval()\n",
    "\n",
    "# modelV4 =  get_shape_comp_model(opt,\"rand_tf_old\",\"../raw_dataset/checkpoints_new/rand_tf_250_epochs_v4.pth\")\n",
    "# modelV4.eval()\n",
    "\n",
    "#modelV1 = get_shape_comp_model(opt,\"rand_tf\",new_checkpoint_path_V1) \n",
    "#modelV1.eval()\n",
    "\n",
    "# modelV2 = get_shape_comp_model(opt,\"rand_tf\",new_checkpoint_path_V2)\n",
    "# modelV2.eval()\n",
    "\n",
    "# modelV3 = get_shape_comp_model(opt,\"rand_tf\",new_checkpoint_path_V3)\n",
    "# modelV3.eval()\n",
    "\n",
    "modelV5 = get_shape_comp_model(opt,\"rand_tf\",new_checkpoint_path_V5)\n",
    "modelV5.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15276bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\"\"\" setup renderer \"\"\"\n",
    "dist, elev, azim = 1.7, 20, 20\n",
    "mesh_renderer = init_mesh_renderer(image_size=256, dist=dist, elev=elev, azim=azim, device=opt.device)\n",
    "\n",
    "\n",
    "def load_bert2vqsc_model_v3(opt):\n",
    "    from models.networks.bert2vq_sc import BERT2VQ\n",
    "    \n",
    "    net = BERT2VQ(opt)\n",
    "    # bert2vq_ckpt = '/home/paritosh/Desktop/Capstone/clean-code/generative_transformers/logs/bert2vq-shapenet_lang-all-LR1e-4-cleanCode-langMode-/ckpt/bert2vq_epoch-145.pth'\n",
    "    #bert2vq_ckpt = '../raw_dataset/checkpoints/bert2vq_epoch-latest.pth'\n",
    "    #bert2vq_ckpt = \"../raw_dataset/checkpoints_plz/bert2vq_v2.pth\"\n",
    "    #bert2vq_ckpt = \"./logs/bert2vqsc-text2shape-seq-LR1e-4-new-bert-1/ckpt/bert2vq_epoch-latest.pth\"\n",
    "    bert2vq_ckpt = \"./logs/bert2vqsc-text2shape-seq-LR1e-4-new-bert-2-prev_z_shape_input-try-2/ckpt/bert2vq_epoch-latest.pth\"\n",
    "    state_dict = torch.load(bert2vq_ckpt)\n",
    "    net.load_state_dict(state_dict['bert2vq'])\n",
    "    net.eval()\n",
    "    net.to(opt.device)\n",
    "    \n",
    "    return net\n",
    "\n",
    "def load_bert2vqsc_model_v5(opt):\n",
    "    from models.networks.bert2vq_sc_v5 import BERT2VQ\n",
    "    \n",
    "    net = BERT2VQ(opt)\n",
    "    # bert2vq_ckpt = '/home/paritosh/Desktop/Capstone/clean-code/generative_transformers/logs/bert2vq-shapenet_lang-all-LR1e-4-cleanCode-langMode-/ckpt/bert2vq_epoch-145.pth'\n",
    "    #bert2vq_ckpt = '../raw_dataset/checkpoints/bert2vq_epoch-latest.pth'\n",
    "    #bert2vq_ckpt = \"../raw_dataset/checkpoints_plz/bert2vq_v2.pth\"\n",
    "    bert2vq_ckpt = \"../raw_dataset/checkpoints_bert/bert2vq_eopch-latest_v5_2.pth\"\n",
    "    state_dict = torch.load(bert2vq_ckpt)\n",
    "    net.load_state_dict(state_dict['bert2vq'])\n",
    "    net.eval()\n",
    "    net.to(opt.device)\n",
    "    \n",
    "    return net\n",
    "\n",
    "def load_bert2vqsc_model_v4(opt):\n",
    "    from models.networks.bert2vq_sc_v4 import BERT2VQ\n",
    "    \n",
    "    net = BERT2VQ(opt)\n",
    "    # bert2vq_ckpt = '/home/paritosh/Desktop/Capstone/clean-code/generative_transformers/logs/bert2vq-shapenet_lang-all-LR1e-4-cleanCode-langMode-/ckpt/bert2vq_epoch-145.pth'\n",
    "    #bert2vq_ckpt = '../raw_dataset/checkpoints/bert2vq_epoch-latest.pth'\n",
    "    #bert2vq_ckpt = \"../raw_dataset/checkpoints_plz/bert2vq_v2.pth\"\n",
    "    bert2vq_ckpt = \"../raw_dataset/checkpoints_bert/bert2vq_epoch-latest_smoothL1Loss_v4_3.pth\"\n",
    "    state_dict = torch.load(bert2vq_ckpt)\n",
    "    net.load_state_dict(state_dict['bert2vq'])\n",
    "    net.eval()\n",
    "    net.to(opt.device)\n",
    "    \n",
    "    return net\n",
    "\n",
    "opt.mlp_layers = 3\n",
    "opt.mlp_hidden = 1024\n",
    "bert2vqV4 = load_bert2vqsc_model_v4(opt)\n",
    "bert2vqV5 = load_bert2vqsc_model_v5(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "debde967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['an office chair', 'with long back', 'without arms']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"./test_samples_paper.txt\") as file:\n",
    "    sequences = [line.rstrip() for line in file]\n",
    "sequences_clean = [0] * len(sequences)\n",
    "for i in range(len(sequences)):\n",
    "    seq = sequences[i]\n",
    "    sequence = seq.split(\",\")\n",
    "    sequences_clean[i] = sequence\n",
    "sequences_clean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301ee8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from copy import deepcopy\n",
    "# import random\n",
    "# import string\n",
    "# sequences_random = deepcopy(sequences_clean)\n",
    "# for i in range(len(sequence_random)):\n",
    "#     sequence = sequences_random[i]\n",
    "#     for j in range(len(sequence)):\n",
    "#         current_text = sequence[j]\n",
    "#         letters = string.ascii_lowercase\n",
    "#         text = ''.join(random.choice(letters) for k in range(len(current_text)))\n",
    "#         sequence[j] = text\n",
    "\n",
    "# sequences_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "33772804",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(111)\n",
    "\n",
    "description = \"bertV5/transformerV5 passing output of transformer to bert model\"\n",
    "res_dir = 'logs/new-bert/bertV5-transformerV5/passingOutput'\n",
    "if not os.path.exists(res_dir): os.makedirs(res_dir)\n",
    "\n",
    "with open(f\"{res_dir}/description.txt\",\"w\") as f:\n",
    "    f.write(description)\n",
    "\n",
    "gpu_id = 0\n",
    "nimgs=6\n",
    "\n",
    "class Opt:\n",
    "    def __init__(self):\n",
    "        self.name = 'opt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b7d008d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                          | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================================================\n",
      "an office chair ALL TEXt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fcad5a6274c4ae8aa220900d03bc427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[*] autoregressively inferencing...:   0%|          | 0/512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                          | 0/14 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 2. Expected size 36 but got size 6 for tensor number 3 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3912503/3653162896.py\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"idx\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;31m#import pdb;pdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0;31m#import pdb;pdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mz1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutp_concat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnimgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cluster/54/streakfull/ADL4CV/Project/src/plz-autosdf/models/rand_tf_model.py\u001b[0m in \u001b[0;36minference\u001b[0;34m(self, data, seq_len, gen_order, topk, prob, alpha, should_render, verbose)\u001b[0m\n\u001b[1;32m    272\u001b[0m                 \u001b[0mz_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m                 \u001b[0;31m# inp_mask = self.generate_square_subsequent_mask(transformer_inp.shape[0], self.opt.device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m                 \u001b[0moutp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m                 \u001b[0moutp_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/adl4cv-I-Koul65-py3.8/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cluster/54/streakfull/ADL4CV/Project/src/plz-autosdf/models/networks/transformer_networks/rand_transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inp, inp_posn, tgt_posn, z_q)\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0;31m#import pdb;pdb.set_trace();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;31m#inp = torch.cat([inp_val, inp_posn, tgt_posn, z_q_concat], dim=-1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minp_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp_posn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_posn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mz_q_concat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minp_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp_posn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_posn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 2. Expected size 36 but got size 6 for tensor number 3 in the list."
     ]
    }
   ],
   "source": [
    "def get_lang_prob_recursive(bert_model,test_data, z1, opt=None):\n",
    "    lang_logits = bert_model(test_data, z1)\n",
    "    lang_logprob = torch.Softmax(lang_logits, dim=1) # compute the prob. of next ele\n",
    "    # img_logprob = torch.sum(img_logprob, dim=1) # multiply the image priors\n",
    "    lang_logprob = rearrange(lang_logprob, 'bs c d h w -> (d h w) bs c')\n",
    "    return lang_logprob\n",
    "\n",
    "\n",
    "model = modelV5\n",
    "model_name = \"modelV5\"\n",
    "bert2vq = bert2vqV5\n",
    "import torch\n",
    "from utils.qual_util import get_lang_prob_recursive, save_mesh_as_pics\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "nimgs=6\n",
    "for j in tqdm(range(len(sequences_clean))):\n",
    "#for j in range(10):\n",
    "    #text = sequence[j]\n",
    "        sequence = sequences_clean[j]\n",
    "        last_text = \"\"\n",
    "        z1 = torch.full((1,8,8,8,512), 1/512).cuda()\n",
    "        test_data = { }\n",
    "        sampler = torch.distributions.categorical.Categorical(z1)\n",
    "        codeix = sampler.sample()\n",
    "    \n",
    "        #z1  = z1.repeat_interleave(nimgs, dim=0)\n",
    "        test_data[\"z_set\"] = z1\n",
    "        #test_data[\"z_set\"] = shapeset[4][\"z_set\"].unsqueeze(0).repeat_interleave(nimgs, dim=0)\n",
    "   \n",
    "        test_data[\"idx\"] = sampler.sample().repeat_interleave(nimgs, axis=0).cpu()\n",
    "        test_data[\"z_q\"] = z1\n",
    "        z1 =  z1.repeat_interleave(nimgs, dim=0)\n",
    "        print(\"======================================================================================================\")\n",
    "        for i in range(len(sequence)):\n",
    "    #for i in range(10):\n",
    "        #text = input(\"Enter text\")\n",
    "            text = sequence[i]\n",
    "            all_text = last_text + text\n",
    "            text_conditional = text\n",
    "            lang_conditional_prob = get_lang_prob_recursive(bert2vq, [all_text]*nimgs, z1)\n",
    "            z1_old = rearrange(z1, 'bs d h w c -> (d h w bs) c')\n",
    "            z1 = rearrange(lang_conditional_prob, '(d h w) bs c -> bs d h w c', d=8, h=8, w=8)\n",
    "            #import pdb;pdb.set_trace()\n",
    "            print(all_text,\"ALL TEXt\")\n",
    "            #import pdb;pdb.set_trace()\n",
    "            #z1 = bert2vq([text] * nimgs, z1)\n",
    "            #z1 = torch.clamp(z1, 1e-3)\n",
    "            topk = 50\n",
    "            #import pdb;pdb.set_trace()\n",
    "            #test_data[\"z_set\"] = z1.repeat_interleave(nimgs, dim=0)\n",
    "            test_data[\"z_set\"] = z1\n",
    "            sampler = torch.distributions.categorical.Categorical(z1)\n",
    "            #test_data[\"idx\"] = torch.cat( [sampler.sample().cpu() for i in range(6)], dim=0)\n",
    "            test_data[\"idx\"] = torch.cat( [sampler.sample().cpu()], dim=0)\n",
    "            #import pdb;pdb.set_trace()\n",
    "            model.inference(test_data, topk=topk, prob=None, gen_order=None)\n",
    "            #import pdb;pdb.set_trace()\n",
    "            z1 = model.outp_concat.reshape((nimgs,8,8,8,512))\n",
    "            #z1 = z1[0]\n",
    "            gen_mesh = sdf_to_mesh(model.x_recon_tf)\n",
    "    \n",
    "            gen_gif_name = f'{res_dir}/{j}-{i}-{model_name}-{all_text}.gif'\n",
    "            try:\n",
    "                save_mesh_as_pics(mesh_renderer, gen_mesh, nrow=3, out_name=gen_gif_name)\n",
    "            except:\n",
    "                import pdb;pdb.set_trace()\n",
    "            last_text = all_text + \" \"\n",
    "        \n",
    "            print(\"TEXT:\", text_conditional)\n",
    "            print(\"Full Text:\", all_text)\n",
    "            print(\"Saved:\", gen_gif_name)\n",
    "            for name in [gen_gif_name]:\n",
    "                display(ipy_image(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b79f73a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
